ID,Title,Authors,Published Date,Abstract,URL,Categories
http://arxiv.org/abs/2510.20258v1,Using Large Language Models for Abstraction of Planning Domains - Extended Version,"Bita Banihashemi, Megh Patel, Yves Lesp√©rance",2025-10-23,"Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.",http://arxiv.org/abs/2510.20258v1,"cs.AI, I.2"
http://arxiv.org/abs/2510.00182v1,A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream,Jorge Mendez-Mendez,2025-09-30,"Using large language models (LLMs) to solve complex robotics problems
requires understanding their planning capabilities. Yet while we know that LLMs
can plan on some problems, the extent to which these planning capabilities
cover the space of robotics tasks is unclear. One promising direction is to
integrate the semantic knowledge of LLMs with the formal reasoning of task and
motion planning (TAMP). However, the myriad of choices for how to integrate
LLMs within TAMP complicates the design of such systems. We develop 16
algorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our
zero-shot experiments across 4,950 problems and three domains reveal that the
Gemini-based planners exhibit lower success rates and higher planning times
than their engineered counterparts. We show that providing geometric details
increases the number of task-planning errors compared to pure PDDL
descriptions, and that (faster) non-reasoning LLM variants outperform (slower)
reasoning variants in most cases, since the TAMP system can direct the LLM to
correct its mistakes.",http://arxiv.org/abs/2510.00182v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2509.18083v1,Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning,"Valentin Lacombe, Valentin Quesnel, Damien Sileo",2025-09-22,"We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.",http://arxiv.org/abs/2509.18083v1,"cs.AI, cs.CL"
http://arxiv.org/abs/2509.12987v1,Toward PDDL Planning Copilot,"Yarin Benyamin, Argaman Mordoch, Shahaf S. Shperberg, Roni Stern",2025-09-16,"Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.",http://arxiv.org/abs/2509.12987v1,cs.AI
http://arxiv.org/abs/2509.13351v1,Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning,"Pulkit Verma, Ngoc La, Anthony Favier, Swaroop Mishra, Julie A. Shah",2025-09-14,"Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.",http://arxiv.org/abs/2509.13351v1,"cs.AI, cs.CL"
http://arxiv.org/abs/2509.10054v1,XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph,"Hailong Yang, Mingxian Gu, Jianqi Wang, Guanjin Wang, Zhaohong Deng",2025-09-12,"The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.",http://arxiv.org/abs/2509.10054v1,cs.AI
http://arxiv.org/abs/2509.08222v1,Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following,"Minjong Yoo, Jinwoo Jang, Wei-jin Park, Honguk Woo",2025-09-10,"This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.",http://arxiv.org/abs/2509.08222v1,cs.AI
http://arxiv.org/abs/2508.13876v1,Improved Generalized Planning with LLMs through Strategy Refinement and Reflection,"Katharina Stein, Nils Hodel, Daniel Fi≈°er, J√∂rg Hoffmann, Michael Katz, Alexander Koller",2025-08-19,"LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.",http://arxiv.org/abs/2508.13876v1,"cs.AI, cs.CL"
http://arxiv.org/abs/2508.08997v1,Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory,"Sizhe Yuen, Francisco Gomez Medina, Ting Su, Yali Du, Adam J. Sobey",2025-08-12,"Multi-agent systems built on Large Language Models (LLMs) show exceptional
promise for complex collaborative problem-solving, yet they face fundamental
challenges stemming from context window limitations that impair memory
consistency, role adherence, and procedural integrity. This paper introduces
Intrinsic Memory Agents, a novel framework that addresses these limitations
through structured agent-specific memories that evolve intrinsically with agent
outputs. Specifically, our method maintains role-aligned memory templates that
preserve specialized perspectives while focusing on task-relevant information.
We benchmark our approach on the PDDL dataset, comparing its performance to
existing state-of-the-art multi-agentic memory approaches and showing an
improvement of 38.6\% with the highest token efficiency. An additional
evaluation is performed on a complex data pipeline design task, we demonstrate
that our approach produces higher quality designs when comparing 5 metrics:
scalability, reliability, usability, cost-effectiveness and documentation with
additional qualitative evidence of the improvements. Our findings suggest that
addressing memory limitations through structured, intrinsic approaches can
improve the capabilities of multi-agent LLM systems on structured planning
tasks.",http://arxiv.org/abs/2508.08997v1,cs.AI
http://arxiv.org/abs/2508.03345v1,Adaptive AI Agent Placement and Migration in Edge Intelligence Systems,"Xingdan Wang, Jiayi He, Zhiqing Tang, Jianxiong Guo, Jiong Lou, Liping Qian, Tian Wang, Weijia Jia",2025-08-05,"The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents
capable of real-time task handling. However, migrating data-intensive,
multi-modal edge workloads to cloud data centers, traditionally used for agent
deployment, introduces significant latency. Deploying AI agents at the edge
improves efficiency and reduces latency. However, edge environments present
challenges due to limited and heterogeneous resources. Maintaining QoS for
mobile users necessitates agent migration, which is complicated by the
complexity of AI agents coordinating LLMs, task planning, memory, and external
tools. This paper presents the first systematic deployment and management
solution for LLM-based AI agents in dynamic edge environments. We propose a
novel adaptive framework for AI agent placement and migration in edge
intelligence systems. Our approach models resource constraints and
latency/cost, leveraging ant colony algorithms and LLM-based optimization for
efficient decision-making. It autonomously places agents to optimize resource
utilization and QoS and enables lightweight agent migration by transferring
only essential state. Implemented on a distributed system using AgentScope and
validated across globally distributed edge servers, our solution significantly
reduces deployment latency and migration costs.",http://arxiv.org/abs/2508.03345v1,cs.AI
http://arxiv.org/abs/2508.01300v1,How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective,"Ma'ayan Armony, Albert Mero√±o-Pe√±uela, Gerard Canal",2025-08-02,"The reasoning and planning abilities of Large Language Models (LLMs) have
been a frequent topic of discussion in recent years. Their ability to take
unstructured planning problems as input has made LLMs' integration into AI
planning an area of interest. Nevertheless, LLMs are still not reliable as
planners, with the generated plans often containing mistaken or hallucinated
actions. Existing benchmarking and evaluation methods investigate planning with
LLMs, focusing primarily on success rate as a quality indicator in various
planning tasks, such as validating plans or planning in relaxed conditions. In
this paper, we approach planning with LLMs as a natural language processing
(NLP) task, given that LLMs are NLP models themselves. We propose a recovery
pipeline consisting of an NLP-based evaluation of the generated plans, along
with three stages to recover the plans through NLP manipulation of the
LLM-generated plans, and eventually complete the plan using a symbolic planner.
This pipeline provides a holistic analysis of LLM capabilities in the context
of AI task planning, enabling a broader understanding of the quality of invalid
plans. Our findings reveal no clear evidence of underlying reasoning during
plan generation, and that a pipeline comprising an NLP-based analysis of the
plans, followed by a recovery mechanism, still falls short of the quality and
reliability of classical planners. On average, only the first 2.65 actions of
the plan are executable, with the average length of symbolically generated
plans being 8.4 actions. The pipeline still improves action quality and
increases the overall success rate from 21.9% to 27.5%.",http://arxiv.org/abs/2508.01300v1,cs.AI
http://arxiv.org/abs/2507.02253v6,Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation,Jungkoo Kang,2025-07-03,"Robust workflow composition is critical for effective agent performance, yet
progress in Large Language Model (LLM) planning and reasoning is hindered by a
scarcity of scalable evaluation data. This work introduces NL2Flow, a fully
automated pipeline for generating and evaluating workflow planning problems.
NL2Flow generates problems parametrically in a structured intermediate
representation, translating them into both natural language and formal PDDL. I
evaluate several open-source, instruct-tuned LLMs on a dataset of 2296
low-difficulty problems generated by NL2Flow. Results demonstrate that the
best-performing model achieved 86% success in generating valid plans and 69% in
generating optimal plans (for solvable problems). Regression analysis shows
that the influence of problem characteristics on plan generation is contingent
on both model and prompt design. Importantly, translating natural language
problems into a structured JSON representation prior to symbolic planning
significantly improved success rates, suggesting a benefit from neuro-symbolic
integration. These findings underscore the importance of understanding error
sources within LLM reasoning as systems scale to more complex tasks. As LLM
reasoning scales to increasingly complex problems, understanding the shifting
bottlenecks and sources of error within these systems will be crucial.",http://arxiv.org/abs/2507.02253v6,cs.AI
http://arxiv.org/abs/2506.15828v2,Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning,"Emanuele Musumeci, Michele Brienza, Francesco Argenziano, Abdel Hakim Drid, Vincenzo Suriani, Daniele Nardi, Domenico D. Bloisi",2025-06-18,"Embodied agents need to plan and act reliably in real and complex 3D
environments. Classical planning (e.g., PDDL) offers structure and guarantees,
but in practice it fails under noisy perception and incorrect predicate
grounding. On the other hand, Large Language Models (LLMs)-based planners
leverage commonsense reasoning, yet frequently propose actions that are
unfeasible or unsafe. Following recent works that combine the two approaches,
we introduce ContextMatters, a framework that fuses LLMs and classical planning
to perform hierarchical goal relaxation: the LLM helps ground symbols to the
scene and, when the target is unreachable, it proposes functionally equivalent
goals that progressively relax constraints, adapting the goal to the context of
the agent's environment. Operating on 3D Scene Graphs, this mechanism turns
many nominally unfeasible tasks into tractable plans and enables context-aware
partial achievement when full completion is not achievable. Our experimental
results show a +52.45% Success Rate improvement over state-of-the-art LLMs+PDDL
baseline, demonstrating the effectiveness of our approach. Moreover, we
validate the execution of ContextMatter in a real world scenario by deploying
it on a TIAGo robot. Code, dataset, and supplementary materials are available
to the community at https://lab-rococo-sapienza.github.io/context-matters/.",http://arxiv.org/abs/2506.15828v2,"cs.RO, cs.AI"
http://arxiv.org/abs/2506.11380v1,Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation,"Xiaoxin Lu, Ranran Haoran Zhang, Yusen Zhang, Rui Zhang",2025-06-13,"People get informed of a daily task plan through diverse media involving both
texts and images. However, most prior research only focuses on LLM's capability
of textual plan generation. The potential of large-scale models in providing
text-image plans remains understudied. Generating high-quality text-image plans
faces two main challenges: ensuring consistent alignment between two modalities
and keeping coherence among visual steps. To address these challenges, we
propose a novel framework that generates and refines text-image plans
step-by-step. At each iteration, our framework (1) drafts the next textual step
based on the prediction history; (2) edits the last visual step to obtain the
next one; (3) extracts PDDL-like visual information; and (4) refines the draft
with the extracted visual information. The textual and visual step produced in
stage (4) and (2) will then serve as inputs for the next iteration. Our
approach offers a plug-and-play improvement to various backbone models, such as
Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our
approach, we collect a new benchmark consisting of 1,100 tasks and their
text-image pair solutions covering 11 daily topics. We also design and validate
a new set of metrics to evaluate the multimodal consistency and coherence in
text-image plans. Extensive experiment results show the effectiveness of our
approach on a range of backbone models against competitive baselines. Our code
and data are available at https://github.com/psunlpgroup/MPlanner.",http://arxiv.org/abs/2506.11380v1,"cs.CV, cs.AI"
http://arxiv.org/abs/2506.10897v1,GenPlanX. Generation of Plans and Execution,"Daniel Borrajo, Giuseppe Canonaco, Tom√°s de la Rosa, Alfredo Garrach√≥n, Sriram Gopalakrishnan, Simerjot Kaur, Marianela Morales, Sunandita Patra, Alberto Pozanco, Keshav Ramani, Charese Smiley, Pietro Totis, Manuela Veloso",2025-06-12,"Classical AI Planning techniques generate sequences of actions for complex
tasks. However, they lack the ability to understand planning tasks when
provided using natural language. The advent of Large Language Models (LLMs) has
introduced novel capabilities in human-computer interaction. In the context of
planning tasks, LLMs have shown to be particularly good in interpreting human
intents among other uses. This paper introduces GenPlanX that integrates LLMs
for natural language-based description of planning tasks, with a classical AI
planning engine, alongside an execution and monitoring framework. We
demonstrate the efficacy of GenPlanX in assisting users with office-related
tasks, highlighting its potential to streamline workflows and enhance
productivity through seamless human-AI collaboration.",http://arxiv.org/abs/2506.10897v1,cs.AI
http://arxiv.org/abs/2506.10387v1,Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills,"Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, Liqiang Nie",2025-06-12,"Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI
agents have yielded promising outcomes. However, these agents still struggle
with long-horizon tasks in online environments, primarily due to insufficient
knowledge and the inherent gap between offline and online domains. In this
paper, inspired by how humans generalize knowledge in open-ended environments,
we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of
insufficient knowledge. It progressively abstracts trajectories into execution
skills, core skills, and ultimately meta-skills, providing a hierarchical
knowledge structure for long-horizon task planning. To bridge the domain gap,
we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,
which efficiently leverages skills acquired in offline environments to reduce
the action search space during online tree exploration. Building on HMS, we
propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To
validate the performance of Mirage-1 in real-world long-horizon scenarios, we
constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1
outperforms previous agents by 32\%, 19\%, 15\%, and 79\% on AndroidWorld,
MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:
https://cybertronagent.github.io/Mirage-1.github.io/",http://arxiv.org/abs/2506.10387v1,cs.AI
http://arxiv.org/abs/2505.19933v1,"Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making","Yejin Son, Minseo Kim, Sungwoong Kim, Seungju Han, Jian Kim, Dongju Jang, Youngjae Yu, Chanyoung Park",2025-05-26,"Large Language Models (LLMs) are increasingly used for decision making in
embodied agents, yet existing safety evaluations often rely on coarse success
rates and domain-specific setups, making it difficult to diagnose why and where
these models fail. This obscures our understanding of embodied safety and
limits the selective deployment of LLMs in high-risk physical environments. We
introduce SAFEL, the framework for systematically evaluating the physical
safety of LLMs in embodied decision making. SAFEL assesses two key
competencies: (1) rejecting unsafe commands via the Command Refusal Test, and
(2) generating safe and executable plans via the Plan Safety Test. Critically,
the latter is decomposed into functional modules, goal interpretation,
transition modeling, action sequencing, enabling fine-grained diagnosis of
safety failures. To support this framework, we introduce EMBODYGUARD, a
PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both
overtly malicious and contextually hazardous instructions. Evaluation across 13
state-of-the-art LLMs reveals that while models often reject clearly unsafe
commands, they struggle to anticipate and mitigate subtle, situational risks.
Our results highlight critical limitations in current LLMs and provide a
foundation for more targeted, modular improvements in safe embodied reasoning.",http://arxiv.org/abs/2505.19933v1,cs.AI
http://arxiv.org/abs/2505.13180v1,ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models,"Matteo Merler, Nicola Dainese, Minttu Alakuijala, Giovanni Bonetta, Pietro Ferrazzi, Yu Tian, Bernardo Magnini, Pekka Marttinen",2025-05-19,"Integrating Large Language Models with symbolic planners is a promising
direction for obtaining verifiable and grounded plans compared to planning in
natural language, with recent works extending this idea to visual domains using
Vision-Language Models (VLMs). However, rigorous comparison between
VLM-grounded symbolic approaches and methods that plan directly with a VLM has
been hindered by a lack of common environments, evaluation protocols and model
coverage. We introduce ViPlan, the first open-source benchmark for Visual
Planning with symbolic predicates and VLMs. ViPlan features a series of
increasingly challenging tasks in two domains: a visual variant of the classic
Blocksworld planning problem and a simulated household robotics environment. We
benchmark nine open-source VLM families across multiple sizes, along with
selected closed models, evaluating both VLM-grounded symbolic planning and
using the models directly to propose actions. We find symbolic planning to
outperform direct VLM planning in Blocksworld, where accurate image grounding
is crucial, whereas the opposite is true in the household robotics tasks, where
commonsense knowledge and the ability to recover from errors are beneficial.
Finally, we show that across most models and methods, there is no significant
benefit to using Chain-of-Thought prompting, suggesting that current VLMs still
struggle with visual reasoning.",http://arxiv.org/abs/2505.13180v1,cs.AI
http://arxiv.org/abs/2505.13126v2,Zero-Shot Iterative Formalization and Planning in Partially Observable Environments,"Liancheng Gong, Wang Zhu, Jesse Thomason, Li Zhang",2025-05-19,"Using LLMs not to predict plans but to formalize an environment into the
Planning Domain Definition Language (PDDL) has been shown to improve
performance and control. Existing work focuses on fully observable
environments; we tackle the more realistic and challenging partially observable
environments that lack of complete, reliable information. We propose PDDLego+,
a framework to iteratively formalize, plan, grow, and refine PDDL
representations in a zero-shot manner, without needing access to any existing
trajectories. On two textual simulated environments, we show that PDDLego+
improves goal reaching success and exhibits robustness against problem
complexity. We also show that the domain knowledge captured after a successful
trial can benefit future tasks.",http://arxiv.org/abs/2505.13126v2,"cs.AI, cs.CL"
http://arxiv.org/abs/2505.08492v2,Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM,"Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni",2025-05-13,"PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.",http://arxiv.org/abs/2505.08492v2,"cs.AI, cs.LG, cs.RO, I.2.6; I.2.8; I.2.9"
http://arxiv.org/abs/2504.21596v1,Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning,"Huihui Guo, Huilong Pi, Yunchuan Qin, Zhuo Tang, Kenli Li",2025-04-30,"With the rapid advancement of artificial intelligence, there is an increasing
demand for intelligent robots capable of assisting humans in daily tasks and
performing complex operations. Such robots not only require task planning
capabilities but must also execute tasks with stability and robustness. In this
paper, we present a closed-loop task planning and acting system, LLM-PAS, which
is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans
long-horizon tasks in a manner similar to traditional task and motion planners,
it also emphasizes the execution phase of the task. By transferring part of the
constraint-checking process from the planning phase to the execution phase,
LLM-PAS enables exploration of the constraint space and delivers more accurate
feedback on environmental anomalies during execution. The reasoning
capabilities of the LLM allow it to handle anomalies that cannot be addressed
by the robust executor. To further enhance the system's ability to assist the
planner during replanning, we propose the First Look Prompting (FLP) method,
which induces LLM to generate effective PDDL goals. Through comparative
prompting experiments and systematic experiments, we demonstrate the
effectiveness and robustness of LLM-PAS in handling anomalous conditions during
task execution.",http://arxiv.org/abs/2504.21596v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2504.00029v1,Generating Structured Plan Representation of Procedures with LLMs,"Deepeka Garg, Sihan Zeng, Sumitra Ganesh, Leo Ardon",2025-03-28,"In this paper, we address the challenges of managing Standard Operating
Procedures (SOPs), which often suffer from inconsistencies in language, format,
and execution, leading to operational inefficiencies. Traditional process
modeling demands significant manual effort, domain expertise, and familiarity
with complex languages like Business Process Modeling Notation (BPMN), creating
barriers for non-techincal users. We introduce SOP Structuring (SOPStruct), a
novel approach that leverages Large Language Models (LLMs) to transform SOPs
into decision-tree-based structured representations. SOPStruct produces a
standardized representation of SOPs across different domains, reduces cognitive
load, and improves user comprehension by effectively capturing task
dependencies and ensuring sequential integrity. Our approach enables leveraging
the structured information to automate workflows as well as empower the human
users. By organizing procedures into logical graphs, SOPStruct facilitates
backtracking and error correction, offering a scalable solution for process
optimization. We employ a novel evaluation framework, combining deterministic
methods with the Planning Domain Definition Language (PDDL) to verify graph
soundness, and non-deterministic assessment by an LLM to ensure completeness.
We empirically validate the robustness of our LLM-based structured SOP
representation methodology across SOPs from different domains and varying
levels of complexity. Despite the current lack of automation readiness in many
organizations, our research highlights the transformative potential of LLMs to
streamline process modeling, paving the way for future advancements in
automated procedure optimization.",http://arxiv.org/abs/2504.00029v1,"cs.SE, cs.AI"
http://arxiv.org/abs/2503.22674v1,QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?,"Belinda Z. Li, Been Kim, Zi Wang",2025-03-28,"Recently, a large amount of work has focused on improving large language
models' (LLMs') performance on reasoning benchmarks such as math and logic.
However, past work has largely assumed that tasks are well-defined. In the real
world, queries to LLMs are often underspecified, only solvable through
acquiring missing information. We formalize this as a constraint satisfaction
problem (CSP) with missing variable assignments. Using a special case of this
formalism where only one necessary variable assignment is missing, we can
rigorously evaluate an LLM's ability to identify the minimal necessary question
to ask and quantify axes of difficulty levels for each problem. We present
QuestBench, a set of underspecified reasoning tasks solvable by asking at most
one question, which includes: (1) Logic-Q: Logical reasoning tasks with one
missing proposition, (2) Planning-Q: PDDL planning problems with initial states
that are partially-observed, (3) GSM-Q: Human-annotated grade school math
problems with one missing variable assignment, and (4) GSME-Q: a version of
GSM-Q where word problems are translated into equations by human annotators.
The LLM is tasked with selecting the correct clarification question(s) from a
list of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their
accuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that
the ability to solve well-specified reasoning problems may not be sufficient
for success on our benchmark: models have difficulty identifying the right
question to ask, even when they can solve the fully specified version of the
problem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even
when explicitly presented with the option to predict ``not sure.'' This
highlights the need for deeper investigation into models' information
acquisition capabilities.",http://arxiv.org/abs/2503.22674v1,"cs.AI, cs.CL, cs.LG"
http://arxiv.org/abs/2503.18971v1,LLMs as Planning Modelers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models,"Marcus Tantakoun, Xiaodan Zhu, Christian Muise",2025-03-22,"Large Language Models (LLMs) excel in various natural language tasks but
often struggle with long-horizon planning problems requiring structured
reasoning. This limitation has drawn interest in integrating neuro-symbolic
approaches within the Automated Planning (AP) and Natural Language Processing
(NLP) communities. However, identifying optimal AP deployment frameworks can be
daunting. This paper aims to provide a timely survey of the current research
with an in-depth analysis, positioning LLMs as tools for extracting and
refining planning models to support reliable AP planners. By systematically
reviewing the current state of research, we highlight methodologies, and
identify critical challenges and future directions, hoping to contribute to the
joint research on NLP and Automated Planning.",http://arxiv.org/abs/2503.18971v1,cs.AI
http://arxiv.org/abs/2503.12349v5,SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?,"Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath",2025-03-16,"Reasoning and strategic behavior in social interactions is a hallmark of
intelligence. This form of reasoning is significantly more sophisticated than
isolated planning or reasoning tasks in static settings (e.g., math problem
solving). In this paper, we present Strategic Planning, Interaction, and
Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the
intelligence of strategic planning and social reasoning. While many existing
benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench
combines classical PDDL tasks, competitive board games, cooperative card games,
and multi-agent negotiation scenarios in one unified framework. The framework
includes both a benchmark as well as an arena to simulate and evaluate the
variety of social settings to test reasoning and strategic behavior of AI
agents. We formulate the benchmark SPIN-Bench by systematically varying action
spaces, state complexity, and the number of interacting agents to simulate a
variety of social settings where success depends on not only methodical and
step-wise decision making, but also conceptual inference of other (adversarial
or cooperative) participants. Our experiments reveal that while contemporary
LLMs handle basic fact retrieval and short-range planning reasonably well, they
encounter significant performance bottlenecks in tasks requiring deep multi-hop
reasoning over large state spaces and socially adept coordination under
uncertainty. We envision SPIN-Bench as a catalyst for future research on robust
multi-agent planning, social reasoning, and human--AI teaming. Project Website:
https://spinbench.github.io/",http://arxiv.org/abs/2503.12349v5,cs.AI
http://arxiv.org/abs/2503.11790v3,Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs,"Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, Leonid Karlinsky",2025-03-14,"Human reasoning relies on constructing and manipulating mental models --
simplified internal representations of situations used to understand and solve
problems. Conceptual diagrams (e.g., a sketch drawn to aid reasoning)
externalize these mental models, abstracting irrelevant details to efficiently
capture how entities interact. In contrast, Large Language Models (LLMs) and
Large MultiModal Models (LMMs) predominantly reason through text, limiting
their effectiveness on complex multi-step tasks. In this paper, we propose
Visual Thinking, a generalizable framework that enables LMMs to reason through
multiple chains of self-generated conceptual diagrams, significantly enhancing
their combinatorial planning capabilities. Our approach requires no human input
beyond the natural language description of the task. It integrates textual and
diagrammatic reasoning within an optimized Graph-of-Thought inference
framework, enhanced by beam search and depth-wise backtracking. Evaluated on
multiple challenging PDDL planning domains, our method substantially improves
LMM performance (e.g., GPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently
outperforms text-only search-based inference methods. On more difficult domains
with solution depths up to 40, it also surpasses the o1-preview reasoning model
(e.g., 16 percentage points improvement in Floor Tiles). These results
demonstrate the power of conceptual diagrams as a reasoning medium in LMMs.",http://arxiv.org/abs/2503.11790v3,cs.AI
http://arxiv.org/abs/2503.08084v1,Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation,"Fangyuan Wang, Shipeng Lyu, Peng Zhou, Anqing Duan, Guodong Guo, David Navarro-Alarcon",2025-03-11,"Enabling humanoid robots to perform long-horizon mobile manipulation planning
in real-world environments based on embodied perception and comprehension
abilities has been a longstanding challenge. With the recent rise of large
language models (LLMs), there has been a notable increase in the development of
LLM-based planners. These approaches either utilize human-provided textual
representations of the real world or heavily depend on prompt engineering to
extract such representations, lacking the capability to quantitatively
understand the environment, such as determining the feasibility of manipulating
objects. To address these limitations, we present the Instruction-Augmented
Long-Horizon Planning (IALP) system, a novel framework that employs LLMs to
generate feasible and optimal actions based on real-time sensor feedback,
including grounded knowledge of the environment, in a closed-loop interaction.
Distinct from prior works, our approach augments user instructions into PDDL
problems by leveraging both the abstract reasoning capabilities of LLMs and
grounding mechanisms. By conducting various real-world long-horizon tasks, each
consisting of seven distinct manipulatory skills, our results demonstrate that
the IALP system can efficiently solve these tasks with an average success rate
exceeding 80%. Our proposed method can operate as a high-level planner,
equipping robots with substantial autonomy in unstructured environments through
the utilization of multi-modal sensor inputs.",http://arxiv.org/abs/2503.08084v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2502.20175v1,An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs,"Kaustubh Vyas, Damien Graux, S√©bastien Montella, Pavlos Vougiouklis, Ruofei Lai, Keshuang Li, Yang Ren, Jeff Z. Pan",2025-02-27,"In recent advancements, large language models (LLMs) have exhibited
proficiency in code generation and chain-of-thought reasoning, laying the
groundwork for tackling automatic formal planning tasks. This study evaluates
the potential of LLMs to understand and generate Planning Domain Definition
Language (PDDL), an essential representation in artificial intelligence
planning. We conduct an extensive analysis across 20 distinct models spanning 7
major LLM families, both commercial and open-source. Our comprehensive
evaluation sheds light on the zero-shot LLM capabilities of parsing,
generating, and reasoning with PDDL. Our findings indicate that while some
models demonstrate notable effectiveness in handling PDDL, others pose
limitations in more complex scenarios requiring nuanced planning knowledge.
These results highlight the promise and current limitations of LLMs in formal
planning tasks, offering insights into their application and guiding future
efforts in AI-driven planning paradigms.",http://arxiv.org/abs/2502.20175v1,"cs.AI, cs.CL"
http://arxiv.org/abs/2502.20284v1,Evaluating Human Trust in LLM-Based Planners: A Preliminary Study,"Shenghui Chen, Yunhao Yang, Kayla Boggess, Seongkook Heo, Lu Feng, Ufuk Topcu",2025-02-27,"Large Language Models (LLMs) are increasingly used for planning tasks,
offering unique capabilities not found in classical planners such as generating
explanations and iterative refinement. However, trust--a critical factor in the
adoption of planning systems--remains underexplored in the context of LLM-based
planning tasks. This study bridges this gap by comparing human trust in
LLM-based planners with classical planners through a user study in a Planning
Domain Definition Language (PDDL) domain. Combining subjective measures, such
as trust questionnaires, with objective metrics like evaluation accuracy, our
findings reveal that correctness is the primary driver of trust and
performance. Explanations provided by the LLM improved evaluation accuracy but
had limited impact on trust, while plan refinement showed potential for
increasing trust without significantly enhancing evaluation accuracy.",http://arxiv.org/abs/2502.20284v1,"cs.AI, cs.HC"
http://arxiv.org/abs/2502.18836v2,"REALM-Bench: A Benchmark for Evaluating Multi-Agent Systems on Real-world, Dynamic Planning and Scheduling Tasks","Longling Geng, Edward Y. Chang",2025-02-26,"This benchmark suite provides a comprehensive evaluation framework for
assessing both individual LLMs and multi-agent systems in Real-world planning
and scheduling scenarios. The suite encompasses 14 designed planning and
scheduling problems that progress from basic to highly complex, incorporating
key aspects such as multi-agent coordination, inter-agent dependencies, and
dynamic environmental disruptions. Each problem can be scaled along three
dimensions: the number of parallel planning threads, the complexity of
inter-dependencies, and the frequency of unexpected disruptions requiring
Real-time adaptation. The benchmark includes 14 detailed problem
specifications, 15 comparison methods including Random, LPT, SPT, STPT, MPSR,
DRL-Liu, GP, GEP, LSO, SPT/TWKR, DRL-Chen, DRL-Zhang, 2+ evaluation metrics,
and baseline implementations using 3+ LLMs including GPT-4o, Claude-3.7,
DeepSeek-R1, and 4 contemporary frameworks including LangGraph, AutoGen,
CrewAI, and Swarm, enabling rigorous testing of both single-agent and
multi-agent planning capabilities. Through standardized evaluation criteria and
scalable complexity, this benchmark aims to be opened to public, and drive
progress in developing more adaptable, robust, and scalable AI planning systems
for Real-world applications.",http://arxiv.org/abs/2502.18836v2,"cs.AI, I.2.11"
http://arxiv.org/abs/2502.14563v1,Plan-over-Graph: Towards Parallelable LLM Agent Schedule,"Shiqi Zhang, Xinbei Ma, Zouying Cao, Zhuosheng Zhang, Hai Zhao",2025-02-20,"Large Language Models (LLMs) have demonstrated exceptional abilities in
reasoning for task planning. However, challenges remain under-explored for
parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in
which the model first decomposes a real-life textual task into executable
subtasks and constructs an abstract task graph. The model then understands this
task graph as input and generates a plan for parallel execution. To enhance the
planning capability of complex, scalable graphs, we design an automated and
controllable pipeline to generate synthetic graphs and propose a two-stage
training scheme. Experimental results show that our plan-over-graph method
significantly improves task performance on both API-based LLMs and trainable
open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally
supports parallel execution, demonstrating global efficiency. The code and data
are available at https://github.com/zsq259/Plan-over-Graph.",http://arxiv.org/abs/2502.14563v1,cs.AI
http://arxiv.org/abs/2502.13092v2,Text2World: Benchmarking Large Language Models for Symbolic World Model Generation,"Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo",2025-02-18,"Recently, there has been growing interest in leveraging large language models
(LLMs) to generate symbolic world models from textual descriptions. Although
LLMs have been extensively explored in the context of world modeling, prior
studies encountered several challenges, including evaluation randomness,
dependence on indirect metrics, and a limited domain scope. To address these
limitations, we introduce a novel benchmark, Text2World, based on planning
domain definition language (PDDL), featuring hundreds of diverse domains and
employing multi-criteria, execution-based metrics for a more robust evaluation.
We benchmark current LLMs using Text2World and find that reasoning models
trained with large-scale reinforcement learning outperform others. However,
even the best-performing model still demonstrates limited capabilities in world
modeling. Building on these insights, we examine several promising strategies
to enhance the world modeling capabilities of LLMs, including test-time
scaling, agent training, and more. We hope that Text2World can serve as a
crucial resource, laying the groundwork for future research in leveraging LLMs
as world models. The project page is available at
https://text-to-world.github.io/.",http://arxiv.org/abs/2502.13092v2,"cs.CL, cs.AI"
http://arxiv.org/abs/2502.12130v1,Scaling Autonomous Agents via Automatic Reward Modeling And Planning,"Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan",2025-02-17,"Large language models (LLMs) have demonstrated remarkable capabilities across
a range of text-generation tasks. However, LLMs still struggle with problems
requiring multi-step decision-making and environmental feedback, such as online
shopping, scientific reasoning, and mathematical problem-solving. Unlike pure
text data, collecting large-scale decision-making data is challenging.
Moreover, many powerful LLMs are only accessible through APIs, which hinders
their fine-tuning for agent tasks due to cost and complexity. To address LLM
agents' limitations, we propose a framework that can automatically learn a
reward model from the environment without human annotations. This model can be
used to evaluate the action trajectories of LLM agents and provide heuristics
for task planning. Specifically, our approach involves employing one LLM-based
agent to navigate an environment randomly, generating diverse action
trajectories. Subsequently, a separate LLM is leveraged to assign a task intent
and synthesize a negative response alongside the correct response for each
trajectory. These triplets (task intent, positive response, and negative
response) are then utilized as training data to optimize a reward model capable
of scoring action trajectories. The effectiveness and generalizability of our
framework are demonstrated through evaluations conducted on different agent
benchmarks. In conclusion, our proposed framework represents a significant
advancement in enhancing LLM agents' decision-making capabilities. By
automating the learning of reward models, we overcome the challenges of data
scarcity and API limitations, potentially revolutionizing the application of
LLMs in complex and interactive environments. This research paves the way for
more sophisticated AI agents capable of tackling a wide range of real-world
problems requiring multi-step decision-making.",http://arxiv.org/abs/2502.12130v1,cs.AI
http://arxiv.org/abs/2502.04728v2,Generating Symbolic World Models via Test-time Scaling of Large Language Models,"Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu",2025-02-07,"Solving complex planning problems requires Large Language Models (LLMs) to
explicitly model the state transition to avoid rule violations, comply with
constraints, and ensure optimality-a task hindered by the inherent ambiguity of
natural language. To overcome such ambiguity, Planning Domain Definition
Language (PDDL) is leveraged as a planning abstraction that enables precise and
formal state descriptions. With PDDL, we can generate a symbolic world model
where classic searching algorithms, such as A*, can be seamlessly applied to
find optimal plans. However, directly generating PDDL domains with current LLMs
remains an open challenge due to the lack of PDDL training data. To address
this challenge, we propose to scale up the test-time computation of LLMs to
enhance their PDDL reasoning capabilities, thereby enabling the generation of
high-quality PDDL domains. Specifically, we introduce a simple yet effective
algorithm, which first employs a Best-of-N sampling approach to improve the
quality of the initial solution and then refines the solution in a fine-grained
manner with verbalized machine learning. Our method outperforms o1-mini by a
considerable margin in the generation of PDDL domains, achieving over 50\%
success rate on two tasks (i.e., generating PDDL domains from natural language
description or PDDL problems). This is done without requiring additional
training. By taking advantage of PDDL as state abstraction, our method is able
to outperform current state-of-the-art methods on almost all competition-level
planning tasks.",http://arxiv.org/abs/2502.04728v2,cs.AI
http://arxiv.org/abs/2501.18784v3,LLM-Generated Heuristics for AI Planning: Do We Even Need Domain-Independence Anymore?,"Alexander Tuisov, Yonatan Vernik, Alexander Shleyfman",2025-01-30,"Domain-independent heuristics have long been a cornerstone of AI planning,
offering general solutions applicable across a wide range of tasks without
requiring domain-specific engineering. However, the advent of large language
models (LLMs) presents an opportunity to generate heuristics tailored to
specific planning problems, potentially challenging the necessity of domain
independence as a strict design principle. In this paper, we explore the use of
LLMs to automatically derive planning heuristics from task descriptions
represented as successor generators and goal tests written in general purpose
programming language. We investigate the trade-offs between domain-specific
LLM-generated heuristics and traditional domain-independent methods in terms of
computational efficiency and explainability. Our experiments demonstrate that
LLMs can create heuristics that achieve state-of-the-art performance on some
standard IPC domains, as well as their ability to solve problems that lack an
adequate Planning Domain Definition Language ({\sc pddl}) representation. We
discuss whether these results signify a paradigm shift and how they can
complement existing approaches.",http://arxiv.org/abs/2501.18784v3,cs.AI
http://arxiv.org/abs/2501.08068v1,A Roadmap to Guide the Integration of LLMs in Hierarchical Planning,"Israel Puerta-Merino, Carlos N√∫√±ez-Molina, Pablo Mesejo, Juan Fern√°ndez-Olivares",2025-01-14,"Recent advances in Large Language Models (LLMs) are fostering their
integration into several reasoning-related fields, including Automated Planning
(AP). However, their integration into Hierarchical Planning (HP), a subfield of
AP that leverages hierarchical knowledge to enhance planning performance,
remains largely unexplored. In this preliminary work, we propose a roadmap to
address this gap and harness the potential of LLMs for HP. To this end, we
present a taxonomy of integration methods, exploring how LLMs can be utilized
within the HP life cycle. Additionally, we provide a benchmark with a
standardized dataset for evaluating the performance of future LLM-based HP
approaches, and present initial results for a state-of-the-art HP planner and
LLM planner. As expected, the latter exhibits limited performance (3\% correct
plans, and none with a correct hierarchical decomposition) but serves as a
valuable baseline for future approaches.",http://arxiv.org/abs/2501.08068v1,cs.AI
http://arxiv.org/abs/2412.07493v3,LLM-guided Task and Motion Planning using Knowledge-based Reasoning,"Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Irfan Hussain",2024-12-10,"Performing complex manipulation tasks in dynamic environments requires
efficient Task and Motion Planning (TAMP) approaches that combine high-level
symbolic plans with low-level motion control. Advances in Large Language Models
(LLMs), such as GPT-4, are transforming task planning by offering natural
language as an intuitive and flexible way to describe tasks, generate symbolic
plans, and reason. However, the effectiveness of LLM-based TAMP approaches is
limited due to static and template-based prompting, which limits adaptability
to dynamic environments and complex task contexts. To address these
limitations, this work proposes a novel Onto-LLM-TAMP framework that employs
knowledge-based reasoning to refine and expand user prompts with
task-contextual reasoning and knowledge-based environment state descriptions.
Integrating domain-specific knowledge into the prompt ensures semantically
accurate and context-aware task plans. The proposed framework demonstrates its
effectiveness by resolving semantic errors in symbolic plan generation, such as
maintaining logical temporal goal ordering in scenarios involving hierarchical
object placement. The proposed framework is validated through both simulation
and real-world scenarios, demonstrating significant improvements over the
baseline approach in terms of adaptability to dynamic environments and the
generation of semantically correct task plans.",http://arxiv.org/abs/2412.07493v3,"cs.RO, cs.AI"
http://arxiv.org/abs/2412.06162v1,Query-Efficient Planning with Language Models,"Gonzalo Gonzalez-Pumariega, Wayne Chen, Kushal Kedia, Sanjiban Choudhury",2024-12-09,"Planning in complex environments requires an agent to efficiently query a
world model to find a feasible sequence of actions from start to goal. Recent
work has shown that Large Language Models (LLMs), with their rich prior
knowledge and reasoning capabilities, can potentially help with planning by
searching over promising states and adapting to feedback from the world. In
this paper, we propose and study two fundamentally competing frameworks that
leverage LLMs for query-efficient planning. The first uses LLMs as a heuristic
within a search-based planner to select promising nodes to expand and propose
promising actions. The second uses LLMs as a generative planner to propose an
entire sequence of actions from start to goal, query a world model, and adapt
based on feedback. We show that while both approaches improve upon comparable
baselines, using an LLM as a generative planner results in significantly fewer
interactions. Our key finding is that the LLM as a planner can more rapidly
adapt its planning strategies based on immediate feedback than LLM as a
heuristic. We present evaluations and ablations on Robotouille and PDDL
planning benchmarks and discuss connections to existing theory on
query-efficient planning algorithms. Code is available at
https://github.com/portal-cornell/llms-for-planning",http://arxiv.org/abs/2412.06162v1,"cs.AI, cs.CL"
http://arxiv.org/abs/2411.19886v1,PDDLFuse: A Tool for Generating Diverse Planning Domains,"Vedant Khandelwal, Amit Sheth, Forest Agostinelli",2024-11-29,"Various real-world challenges require planning algorithms that can adapt to a
broad range of domains. Traditionally, the creation of planning domains has
relied heavily on human implementation, which limits the scale and diversity of
available domains. While recent advancements have leveraged generative AI
technologies such as large language models (LLMs) for domain creation, these
efforts have predominantly focused on translating existing domains from natural
language descriptions rather than generating novel ones. In contrast, the
concept of domain randomization, which has been highly effective in
reinforcement learning, enhances performance and generalizability by training
on a diverse array of randomized new domains. Inspired by this success, our
tool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language
(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can
be used to validate new planners or test foundational planning models. We have
developed methods to adjust the domain generators parameters to modulate the
difficulty of the domains it generates. This adaptability is crucial as
existing domain-independent planners often struggle with more complex problems.
Initial tests indicate that PDDLFuse efficiently creates intricate and varied
domains, representing a significant advancement over traditional domain
generation methods and making a contribution towards planning research.",http://arxiv.org/abs/2411.19886v1,cs.AI
http://arxiv.org/abs/2410.07245v1,AAAI Workshop on AI Planning for Cyber-Physical Systems -- CAIPI24,"Oliver Niggemann, Gautam Biswas, Alexander Diedrich, Jonas Ehrhardt, Ren√© Heesch, Niklas Widulle",2024-10-08,"The workshop 'AI-based Planning for Cyber-Physical Systems', which took place
on February 26, 2024, as part of the 38th Annual AAAI Conference on Artificial
Intelligence in Vancouver, Canada, brought together researchers to discuss
recent advances in AI planning methods for Cyber-Physical Systems (CPS). CPS
pose a major challenge due to their complexity and data-intensive nature, which
often exceeds the capabilities of traditional planning algorithms. The workshop
highlighted new approaches such as neuro-symbolic architectures, large language
models (LLMs), deep reinforcement learning and advances in symbolic planning.
These techniques are promising when it comes to managing the complexity of CPS
and have potential for real-world applications.",http://arxiv.org/abs/2410.07245v1,cs.AI
http://arxiv.org/abs/2410.06108v1,ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution,"Corban Rivera, Grayson Byrd, William Paul, Tyler Feldman, Meghan Booker, Emma Holmes, David Handelman, Bethany Kemp, Andrew Badger, Aurora Schmidt, Krishna Murthy Jatavallabhula, Celso M de Melo, Lalithkumar Seenivasan, Mathias Unberath, Rama Chellappa",2024-10-08,"Robotic planning and execution in open-world environments is a complex
problem due to the vast state spaces and high variability of task embodiment.
Recent advances in perception algorithms, combined with Large Language Models
(LLMs) for planning, offer promising solutions to these challenges, as the
common sense reasoning capabilities of LLMs provide a strong heuristic for
efficiently searching the action space. However, prior work fails to address
the possibility of hallucinations from LLMs, which results in failures to
execute the planned actions largely due to logical fallacies at high- or
low-levels. To contend with automation failure due to such hallucinations, we
introduce ConceptAgent, a natural language-driven robotic platform designed for
task execution in unstructured environments. With a focus on scalability and
reliability of LLM-based planning in complex state and action spaces, we
present innovations designed to limit these shortcomings, including 1)
Predicate Grounding to prevent and recover from infeasible actions, and 2) an
embodied version of LLM-guided Monte Carlo Tree Search with self reflection. In
simulation experiments, ConceptAgent achieved a 19% task completion rate across
three room layouts and 30 easy level embodied tasks outperforming other
state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on
the same benchmark. Additionally, ablation studies on moderate to hard embodied
tasks revealed a 20% increase in task completion from the baseline agent to the
fully enhanced ConceptAgent, highlighting the individual and combined
contributions of Predicate Grounding and LLM-guided Tree Search to enable more
robust automation in complex state and action spaces.",http://arxiv.org/abs/2410.06108v1,cs.AI
http://arxiv.org/abs/2409.15915v1,Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts,"Sukai Huang, Nir Lipovetzky, Trevor Cohn",2024-09-24,"Large Language Models (LLMs) have shown promise in solving natural
language-described planning tasks, but their direct use often leads to
inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning
pipelines have emerged as a more robust alternative, they typically require
extensive expert intervention to refine and validate generated action schemas.
It not only limits scalability but also introduces a potential for biased
interpretation, as a single expert's interpretation of ambiguous natural
language descriptions might not align with the user's actual intent. To address
this, we propose a novel approach that constructs an action schema library to
generate multiple candidates, accounting for the diverse possible
interpretations of natural language descriptions. We further introduce a
semantic validation and ranking module that automatically filter and rank the
generated schemas and plans without expert-in-the-loop. The experiments showed
our pipeline maintains superiority in planning over the direct LLM planning
approach. These findings demonstrate the feasibility of a fully automated
end-to-end LLM-symbolic planner that requires no expert intervention, opening
up the possibility for a broader audience to engage with AI planning with less
prerequisite of domain expertise.",http://arxiv.org/abs/2409.15915v1,cs.AI
http://arxiv.org/abs/2409.14277v1,Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models,"Yew Ken Chia, Qi Sun, Lidong Bing, Soujanya Poria",2024-09-22,"Large multimodal models have demonstrated impressive problem-solving
abilities in vision and language tasks, and have the potential to encode
extensive world knowledge. However, it remains an open challenge for these
models to perceive, reason, plan, and act in realistic environments. In this
work, we introduce Can-Do, a benchmark dataset designed to evaluate embodied
planning abilities through more diverse and complex scenarios than previous
datasets. Our dataset includes 400 multimodal samples, each consisting of
natural language user instructions, visual images depicting the environment,
state changes, and corresponding action plans. The data encompasses diverse
aspects of commonsense knowledge, physical understanding, and safety awareness.
Our fine-grained analysis reveals that state-of-the-art models, including
GPT-4V, face bottlenecks in visual perception, comprehension, and reasoning
abilities. To address these challenges, we propose NeuroGround, a neurosymbolic
framework that first grounds the plan generation in the perceived environment
states and then leverages symbolic planning engines to augment the
model-generated plans. Experimental results demonstrate the effectiveness of
our framework compared to strong baselines. Our code and dataset are available
at https://embodied-planning.github.io.",http://arxiv.org/abs/2409.14277v1,"cs.AI, cs.CL, cs.CV, cs.RO"
http://arxiv.org/abs/2409.01806v1,LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning,"Haoming Li, Zhaoliang Chen, Jonathan Zhang, Fei Liu",2024-09-03,"Effective planning is essential for the success of any task, from organizing
a vacation to routing autonomous vehicles and developing corporate strategies.
It involves setting goals, formulating plans, and allocating resources to
achieve them. LLMs are particularly well-suited for automated planning due to
their strong capabilities in commonsense reasoning. They can deduce a sequence
of actions needed to achieve a goal from a given state and identify an
effective course of action. However, it is frequently observed that plans
generated through direct prompting often fail upon execution. Our survey aims
to highlight the existing challenges in planning with language models, focusing
on key areas such as embodied environments, optimal scheduling, competitive and
cooperative games, task decomposition, reasoning, and planning. Through this
study, we explore how LLMs transform AI planning and provide unique insights
into the future of LM-assisted planning.",http://arxiv.org/abs/2409.01806v1,"cs.AI, cs.CL, cs.LG"
http://arxiv.org/abs/2407.03321v1,Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages,"Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, Stephen H. Bach",2024-07-03,"Many recent works have explored using language models for planning problems.
One line of research focuses on translating natural language descriptions of
planning tasks into structured planning languages, such as the planning domain
definition language (PDDL). While this approach is promising, accurately
measuring the quality of generated PDDL code continues to pose significant
challenges. First, generated PDDL code is typically evaluated using planning
validators that check whether the problem can be solved with a planner. This
method is insufficient because a language model might generate valid PDDL code
that does not align with the natural language description of the task. Second,
existing evaluation sets often have natural language descriptions of the
planning task that closely resemble the ground truth PDDL, reducing the
challenge of the task. To bridge this gap, we introduce \benchmarkName, a
benchmark designed to evaluate language models' ability to generate PDDL code
from natural language descriptions of planning tasks. We begin by creating a
PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL
code generated by language models by flexibly comparing it against a ground
truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across
13 different tasks, with varying levels of difficulty. Finally, we evaluate
several API-access and open-weight language models that reveal this task's
complexity. For example, $87.6\%$ of the PDDL problem descriptions generated by
GPT-4o are syntactically parseable, $82.2\%$ are valid, solve-able problems,
but only $35.1\%$ are semantically correct, highlighting the need for a more
rigorous benchmark for this problem.",http://arxiv.org/abs/2407.03321v1,"cs.CL, cs.AI, cs.LG"
http://arxiv.org/abs/2407.03321v2,Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages,"Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, Stephen H. Bach",2024-07-03,"Recent works have explored using language models for planning problems. One
approach examines translating natural language descriptions of planning tasks
into structured planning languages, such as the planning domain definition
language (PDDL). Existing evaluation methods struggle to ensure semantic
correctness and rely on simple or unrealistic datasets. To bridge this gap, we
introduce \textit{Planetarium}, a benchmark designed to evaluate language
models' ability to generate PDDL code from natural language descriptions of
planning tasks. \textit{Planetarium} features a novel PDDL equivalence
algorithm that flexibly evaluates the correctness of generated PDDL, along with
a dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations
with varying levels of difficulty. Finally, we evaluate several API-access and
open-weight language models that reveal this task's complexity. For example,
96.1\% of the PDDL problem descriptions generated by GPT-4o are syntactically
parseable, 94.4\% are solvable, but only 24.8\% are semantically correct,
highlighting the need for a more rigorous benchmark for this problem.",http://arxiv.org/abs/2407.03321v2,"cs.CL, cs.AI, cs.LG"
http://arxiv.org/abs/2406.15609v2,Automated radiotherapy treatment planning guided by GPT-4Vision,"Sheng Liu, Oscar Pastor-Serrano, Yizheng Chen, Matthew Gopaulchan, Weixing Liang, Mark Buyyounouski, Erqi Pollom, Quynh-Thu Le, Michael Gensheimer, Peng Dong, Yong Yang, James Zou, Lei Xing",2024-06-21,"Radiotherapy treatment planning is a time-consuming and potentially
subjective process that requires the iterative adjustment of model parameters
to balance multiple conflicting objectives. Recent advancements in large
foundation models offer promising avenues for addressing the challenges in
planning and clinical decision-making. This study introduces GPT-RadPlan, a
fully automated treatment planning framework that harnesses prior radiation
oncology knowledge encoded in multi-modal large language models, such as
GPT-4Vision (GPT-4V) from OpenAI. GPT-RadPlan is made aware of planning
protocols as context and acts as an expert human planner, capable of guiding a
treatment planning process. Via in-context learning, we incorporate clinical
protocols for various disease sites as prompts to enable GPT-4V to acquire
treatment planning domain knowledge. The resulting GPT-RadPlan agent is
integrated into our in-house inverse treatment planning system through an API.
The efficacy of the automated planning system is showcased using multiple
prostate and head & neck cancer cases, where we compared GPT-RadPlan results to
clinical plans. In all cases, GPT-RadPlan either outperformed or matched the
clinical plans, demonstrating superior target coverage and organ-at-risk
sparing. Consistently satisfying the dosimetric objectives in the clinical
protocol, GPT-RadPlan represents the first multimodal large language model
agent that mimics the behaviors of human planners in radiation oncology
clinics, achieving remarkable results in automating the treatment planning
process without the need for additional training.",http://arxiv.org/abs/2406.15609v2,"physics.med-ph, cs.AI"
http://arxiv.org/abs/2406.11132v1,RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents,"Weizhe Chen, Sven Koenig, Bistra Dilkina",2024-06-17,"In this past year, large language models (LLMs) have had remarkable success
in domains outside the traditional natural language processing, and people are
starting to explore the usage of LLMs in more general and close to application
domains like code generation, travel planning, and robot controls. Connecting
these LLMs with great capacity and external tools, people are building the
so-called LLM agents, which are supposed to help people do all kinds of work in
everyday life. In all these domains, the prompt to the LLMs has been shown to
make a big difference in what the LLM would generate and thus affect the
performance of the LLM agents. Therefore, automatic prompt engineering has
become an important question for many researchers and users of LLMs. In this
paper, we propose a novel method, \textsc{RePrompt}, which does ""gradient
descent"" to optimize the step-by-step instructions in the prompt of the LLM
agents based on the chat history obtained from interactions with LLM agents. By
optimizing the prompt, the LLM will learn how to plan in specific domains. We
have used experiments in PDDL generation and travel planning to show that our
method could generally improve the performance for different reasoning tasks
when using the updated prompt as the initial prompt.",http://arxiv.org/abs/2406.11132v1,"cs.CL, cs.AI, cs.LG"
http://arxiv.org/abs/2406.11132v2,RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents,"Weizhe Chen, Sven Koenig, Bistra Dilkina",2024-06-17,"In the past year, large language models (LLMs) have had remarkable success in
domains outside the traditional natural language processing, and their capacity
is further expanded into the so-called LLM agents when connected with external
tools. In all domains, the prompt to the LLMs has been shown to make a big
difference in what the LLM would generate and thus affect the performance of
the LLM agents. Therefore, automatic prompt engineering (APE) has become an
important question for many researchers and users of LLMs. However, previous
works in APE rely on a final checker to evaluate the performance of the given
prompt -- a requirement that is hard to meet in the case of LLM agents, where
intermediate feedback is easier to obtain, and the final evaluation could be
expensive, inaccurate, or even missing. In this paper, we propose a novel
method, \textsc{RePrompt}, which does a ``gradient descent""-like approach to
optimize the step-by-step instructions in the prompts given to LLM agents,
based on the chat history obtained from interactions and reflections with LLM
agents. By leveraging intermediate feedback, \textsc{RePrompt} can optimize the
prompt without the need for a final solution checker. We evaluate our approach
on PDDL generation, TravelPlanner, and Meeting Planning to show that our method
could generally improve performance for different reasoning tasks.",http://arxiv.org/abs/2406.11132v2,"cs.CL, cs.AI, cs.LG"
http://arxiv.org/abs/2406.09953v2,DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning,"Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu",2024-06-14,"Dual-arm robots offer enhanced versatility and efficiency over single-arm
counterparts by enabling concurrent manipulation of multiple objects or
cooperative execution of tasks using both arms. However, effectively
coordinating the two arms for complex long-horizon tasks remains a significant
challenge. Existing task planning methods predominantly focus on single-arm
robots or rely on predefined bimanual operations, failing to fully leverage the
capabilities of dual-arm systems. To address this limitation, we introduce
DAG-Plan, a structured task planning framework tailored for dual-arm robots.
DAG-Plan harnesses large language models (LLMs) to decompose intricate tasks
into actionable sub-tasks represented as nodes within a directed acyclic graph
(DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the
appropriate arm based on real-time environmental observations, enabling
parallel and adaptive execution. We evaluate DAG-Plan on the novel Dual-Arm
Kitchen Benchmark, comprising 9 sequential tasks with 78 sub-tasks and 26
objects. Extensive experiments demonstrate the superiority of DAG-Plan over
directly using LLM to generate plans, achieving nearly 50% higher efficiency
compared to the single-arm task planning baseline and nearly double the success
rate of the dual-arm task planning baseline.",http://arxiv.org/abs/2406.09953v2,"cs.RO, cs.AI"
http://arxiv.org/abs/2406.03367v1,CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning,"Xinrui Lin, Yangfan Wu, Huanyu Yang, Yu Zhang, Yanyong Zhang, Jianmin Ji",2024-06-05,"Large Language Models (LLMs) possess extensive foundational knowledge and
moderate reasoning abilities, making them suitable for general task planning in
open-world scenarios. However, it is challenging to ground a LLM-generated plan
to be executable for the specified robot with certain restrictions. This paper
introduces CLMASP, an approach that couples LLMs with Answer Set Programming
(ASP) to overcome the limitations, where ASP is a non-monotonic logic
programming formalism renowned for its capacity to represent and reason about a
robot's action knowledge. CLMASP initiates with a LLM generating a basic
skeleton plan, which is subsequently tailored to the specific scenario using a
vector database. This plan is then refined by an ASP program with a robot's
action knowledge, which integrates implementation details into the skeleton,
grounding the LLM's abstract outputs in practical robot contexts. Our
experiments conducted on the VirtualHome platform demonstrate CLMASP's
efficacy. Compared to the baseline executable rate of under 2% with LLM
approaches, CLMASP significantly improves this to over 90%.",http://arxiv.org/abs/2406.03367v1,cs.AI
http://arxiv.org/abs/2405.13751v1,GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games,"Aoran Mei, Jianhua Wang, Guo-Niu Zhu, Zhongxue Gan",2024-05-22,"With their prominent scene understanding and reasoning capabilities,
pre-trained visual-language models (VLMs) such as GPT-4V have attracted
increasing attention in robotic task planning. Compared with traditional task
planning strategies, VLMs are strong in multimodal information parsing and code
generation and show remarkable efficiency. Although VLMs demonstrate great
potential in robotic task planning, they suffer from challenges like
hallucination, semantic complexity, and limited context. To handle such issues,
this paper proposes a multi-agent framework, i.e., GameVLM, to enhance the
decision-making process in robotic task planning. In this study, VLM-based
decision and expert agents are presented to conduct the task planning.
Specifically, decision agents are used to plan the task, and the expert agent
is employed to evaluate these task plans. Zero-sum game theory is introduced to
resolve inconsistencies among different agents and determine the optimal
solution. Experimental results on real robots demonstrate the efficacy of the
proposed framework, with an average success rate of 83.3%.",http://arxiv.org/abs/2405.13751v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2405.12433v1,LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs,"Sudhir Agarwal, Anu Sreepathy, David H. Alonso, Prarit Lamba",2024-05-21,"Recent availability of Large Language Models (LLMs) has led to the
development of numerous LLM-based approaches aimed at providing natural
language interfaces for various end-user tasks. These end-user tasks in turn
can typically be accomplished by orchestrating a given set of APIs. In
practice, natural language task requests (user queries) are often incomplete,
i.e., they may not contain all the information required by the APIs. While LLMs
excel at natural language processing (NLP) tasks, they frequently hallucinate
on missing information or struggle with orchestrating the APIs. The key idea
behind our proposed approach is to leverage logical reasoning and classical AI
planning along with an LLM for accurately answering user queries including
identification and gathering of any missing information in these queries. Our
approach uses an LLM and ASP (Answer Set Programming) solver to translate a
user query to a representation in Planning Domain Definition Language (PDDL)
via an intermediate representation in ASP. We introduce a special API
""get_info_api"" for gathering missing information. We model all the APIs as PDDL
actions in a way that supports dataflow between the APIs. Our approach then
uses a classical AI planner to generate an orchestration of API calls
(including calls to get_info_api) to answer the user query. Our evaluation
results show that our approach significantly outperforms a pure LLM based
approach by achieving over 95\% success rate in most cases on a dataset
containing complete and incomplete single goal and multi-goal queries where the
multi-goal queries may or may not require dataflow among the APIs.",http://arxiv.org/abs/2405.12433v1,cs.AI
http://arxiv.org/abs/2405.12433v3,LLM+Reasoning+Planning for Supporting Incomplete User Queries in Presence of APIs,"Sudhir Agarwal, Anu Sreepathy, David H. Alonso, Prarit Lamba",2024-05-21,"Recent availability of Large Language Models (LLMs) has led to the
development of numerous LLM-based approaches aimed at providing natural
language interfaces for various end-user tasks. These end-user tasks in turn
can typically be accomplished by orchestrating a given set of APIs. In
practice, natural language task requests (user queries) are often incomplete,
i.e., they may not contain all the information required by the APIs. While LLMs
excel at natural language processing (NLP) tasks, they frequently hallucinate
on missing information or struggle with orchestrating the APIs. The key idea
behind our proposed approach is to leverage logical reasoning and classical AI
planning along with an LLM for accurately answering user queries including
identification and gathering of any missing information in these queries. Our
approach uses an LLM and ASP (Answer Set Programming) solver to translate a
user query to a representation in Planning Domain Definition Language (PDDL)
via an intermediate representation in ASP. We introduce a special API
""get_info_api"" for gathering missing information. We model all the APIs as PDDL
actions in a way that supports dataflow between the APIs. Our approach then
uses a classical AI planner to generate an orchestration of API calls
(including calls to get_info_api) to answer the user query. Our evaluation
results show that our approach significantly outperforms a pure LLM based
approach by achieving over 95% success rate in most cases on a dataset
containing complete and incomplete single goal and multi-goal queries where the
multi-goal queries may or may not require dataflow among the APIs.",http://arxiv.org/abs/2405.12433v3,cs.AI
http://arxiv.org/abs/2405.04215v1,NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions,"Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp",2024-05-07,"Today's classical planners are powerful, but modeling input tasks in formats
such as PDDL is tedious and error-prone. In contrast, planning with Large
Language Models (LLMs) allows for almost any input text, but offers no
guarantees on plan quality or even soundness. In an attempt to merge the best
of these two approaches, some work has begun to use LLMs to automate parts of
the PDDL creation process. However, these methods still require various degrees
of expert input. We present NL2Plan, the first domain-agnostic offline
LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the
necessary information from a short text prompt before creating a complete PDDL
description of both the domain and the problem, which is finally solved by a
classical planner. We evaluate NL2Plan on four planning domains and find that
it solves 10 out of 15 tasks - a clear improvement over a plain
chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover,
in two out of the five failure cases, instead of returning an invalid plan,
NL2Plan reports that it failed to solve the task. In addition to using NL2Plan
in end-to-end mode, users can inspect and correct all of its intermediate
results, such as the PDDL representation, increasing explainability and making
it an assistive tool for PDDL creation.",http://arxiv.org/abs/2405.04215v1,cs.AI
http://arxiv.org/abs/2405.04215v2,NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions,"Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp",2024-05-07,"Classical planners are powerful systems, but modeling tasks in input formats
such as PDDL is tedious and error-prone. In contrast, planning with Large
Language Models (LLMs) allows for almost any input text, but offers no
guarantees on plan quality or even soundness. In an attempt to merge the best
of these two approaches, some work has begun to use LLMs to automate parts of
the PDDL creation process. However, these methods still require various degrees
of expert input or domain-specific adaptations. We present NL2Plan, the first
fully automatic system for generating complete PDDL tasks from minimal natural
language descriptions. NL2Plan uses an LLM to incrementally extract the
necessary information from the short text input before creating a complete PDDL
description of both the domain and the problem which is finally solved by a
classical planner. We evaluate NL2Plan on seven planning domains, five of which
are novel and thus not in the LLM training data, and find that NL2Plan
outperforms directly generating the files with an LLM+validator combination. As
such, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards
solving natural language planning task with interpretability and guarantees.",http://arxiv.org/abs/2405.04215v2,cs.AI
http://arxiv.org/abs/2404.07751v1,Generating consistent PDDL domains with Large Language Models,"Pavel Smirnov, Frank Joublin, Antonello Ceravola, Michael Gienger",2024-04-11,"Large Language Models (LLMs) are capable of transforming natural language
domain descriptions into plausibly looking PDDL markup. However, ensuring that
actions are consistent within domains still remains a challenging task. In this
paper we present a novel concept to significantly improve the quality of
LLM-generated PDDL models by performing automated consistency checking during
the generation process. Although the proposed consistency checking strategies
still can't guarantee absolute correctness of generated models, they can serve
as valuable source of feedback reducing the amount of correction efforts
expected from a human in the loop. We demonstrate the capabilities of our error
detection approach on a number of classical and custom planning domains
(logistics, gripper, tyreworld, household, pizza).",http://arxiv.org/abs/2404.07751v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2404.04540v1,The Case for Developing a Foundation Model for Planning-like Tasks from Scratch,"Biplav Srivastava, Vishal Pallagani",2024-04-06,"Foundation Models (FMs) have revolutionized many areas of computing,
including Automated Planning and Scheduling (APS). For example, a recent study
found them useful for planning problems: plan generation, language translation,
model construction, multi-agent planning, interactive planning, heuristics
optimization, tool integration, and brain-inspired planning. Besides APS, there
are many seemingly related tasks involving the generation of a series of
actions with varying guarantees of their executability to achieve intended
goals, which we collectively call planning-like (PL) tasks like business
processes, programs, workflows, and guidelines, where researchers have
considered using FMs. However, previous works have primarily focused on
pre-trained, off-the-shelf FMs and optionally fine-tuned them. This paper
discusses the need for a comprehensive FM for PL tasks from scratch and
explores its design considerations. We argue that such an FM will open new and
efficient avenues for PL problem-solving, just like LLMs are creating for APS.",http://arxiv.org/abs/2404.04540v1,cs.AI
http://arxiv.org/abs/2404.03275v1,DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models,"Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello",2024-04-04,"Recent advancements in Large Language Models (LLMs) have sparked a revolution
across various research fields. In particular, the integration of common-sense
knowledge from LLMs into robot task and motion planning has been proven to be a
game-changer, elevating performance in terms of explainability and downstream
task efficiency to unprecedented heights. However, managing the vast knowledge
encapsulated within these large models has posed challenges, often resulting in
infeasible plans generated by LLM-based planning systems due to hallucinations
or missing domain information. To overcome these challenges and obtain even
greater planning feasibility and computational efficiency, we propose a novel
LLM-driven task planning approach called DELTA. For achieving better grounding
from environmental topology into actionable knowledge, DELTA leverages the
power of scene graphs as environment representations within LLMs, enabling the
fast generation of precise planning problem descriptions. For obtaining higher
planning performance, we use LLMs to decompose the long-term task goals into an
autoregressive sequence of sub-goals for an automated task planner to solve.
Our contribution enables a more efficient and fully automatic task planning
pipeline, achieving higher planning success rates and significantly shorter
planning times compared to the state of the art.",http://arxiv.org/abs/2404.03275v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2404.03275v2,DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models,"Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello",2024-04-04,"Recent advancements in Large Language Models (LLMs) have sparked a revolution
across many research fields. In robotics, the integration of common-sense
knowledge from LLMs into task and motion planning has drastically advanced the
field by unlocking unprecedented levels of context awareness. Despite their
vast collection of knowledge, large language models may generate infeasible
plans due to hallucinations or missing domain information. To address these
challenges and improve plan feasibility and computational efficiency, we
introduce DELTA, a novel LLM-informed task planning approach. By using scene
graphs as environment representations within LLMs, DELTA achieves rapid
generation of precise planning problem descriptions. To enhance planning
performance, DELTA decomposes long-term task goals with LLMs into an
autoregressive sequence of sub-goals, enabling automated task planners to
efficiently solve complex problems. In our extensive evaluation, we show that
DELTA enables an efficient and fully automatic task planning pipeline,
achieving higher planning success rates and significantly shorter planning
times compared to the state of the art.",http://arxiv.org/abs/2404.03275v2,"cs.RO, cs.AI"
http://arxiv.org/abs/2404.02817v4,A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches,"Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao",2024-04-03,"Task and Motion Planning (TAMP) integrates high-level task planning and
low-level motion planning to equip robots with the autonomy to effectively
reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on
hybrid optimization approaches that define goal conditions via objective
functions and are capable of handling open-ended goals, robotic dynamics, and
physical interaction between the robot and the environment. Therefore,
optimization-based TAMP is particularly suited to solve highly complex,
contact-rich locomotion and manipulation problems. This survey provides a
comprehensive review on optimization-based TAMP, covering (i) planning domain
representations, including action description languages and temporal logic,
(ii) individual solution strategies for components of TAMP, including AI
planning and trajectory optimization (TO), and (iii) the dynamic interplay
between logic-based task planning and model-based TO. A particular focus of
this survey is to highlight the algorithm structures to efficiently solve TAMP,
especially hierarchical and distributed approaches. Additionally, the survey
emphasizes the synergy between the classical methods and contemporary
learning-based innovations such as large language models. Furthermore, the
future research directions for TAMP is discussed in this survey, highlighting
both algorithmic and application-specific challenges.",http://arxiv.org/abs/2404.02817v4,"cs.RO, cs.AI"
http://arxiv.org/abs/2405.06650v1,Large Language Models as Planning Domain Generators,"James Oswald, Kavitha Srinivas, Harsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi",2024-04-02,"Developing domain models is one of the few remaining places that require
manual human labor in AI planning. Thus, in order to make planning more
accessible, it is desirable to automate the process of domain model generation.
To this end, we investigate if large language models (LLMs) can be used to
generate planning domain models from simple textual descriptions. Specifically,
we introduce a framework for automated evaluation of LLM-generated domains by
comparing the sets of plans for domain instances. Finally, we perform an
empirical analysis of 7 large language models, including coding and chat models
across 9 different planning domains, and under three classes of natural
language domain descriptions. Our results indicate that LLMs, particularly
those with high parameter counts, exhibit a moderate level of proficiency in
generating correct planning domains from natural language descriptions. Our
code is available at https://github.com/IBM/NL2PDDL.",http://arxiv.org/abs/2405.06650v1,"cs.CL, cs.AI"
http://arxiv.org/abs/2403.17246v1,TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,"Ishika Singh, David Traum, Jesse Thomason",2024-03-25,"Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep",http://arxiv.org/abs/2403.17246v1,"cs.AI, cs.CL, cs.MA, cs.RO"
http://arxiv.org/abs/2403.17246v2,TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,"David Bai, Ishika Singh, David Traum, Jesse Thomason",2024-03-25,"Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, such as concurrent actions
between two agents when there are no conflicting conditions, without
significant modification and definition to existing PDDL domains. A human
expert aware of such constraints can decompose a goal into subgoals, each
reachable through single agent planning, to take advantage of simultaneous
actions. In contrast to classical planning, large language models (LLMs)
directly used for inferring plan steps rarely guarantee execution success, but
are capable of leveraging commonsense reasoning to assemble action sequences.
We combine the strengths of both classical planning and LLMs by approximating
human intuitions for multi-agent planning goal decomposition. We demonstrate
that LLM-based goal decomposition leads to faster planning times than solving
multi-agent PDDL problems directly while simultaneously achieving fewer plan
execution steps than a single agent plan alone, as well as most multiagent
plans, while guaranteeing execution success. Additionally, we find that
LLM-based approximations of subgoals result in similar multi-agent execution
lengths to those specified by human experts. Website and resources at
https://glamor-usc.github.io/twostep",http://arxiv.org/abs/2403.17246v2,"cs.AI, cs.CL, cs.MA, cs.RO"
http://arxiv.org/abs/2403.11552v3,LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,"Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu",2024-03-18,"Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.",http://arxiv.org/abs/2403.11552v3,"cs.RO, cs.AI"
http://arxiv.org/abs/2402.14083v2,Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping,"Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, Yuandong Tian",2024-02-21,"While Transformers have enabled tremendous progress in various application
settings, such architectures still trail behind traditional symbolic planners
for solving complex decision making tasks. In this work, we demonstrate how to
train Transformers to solve complex planning tasks. This is accomplished by
training an encoder-decoder Transformer model to predict the search dynamics of
the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a
Transformer model that optimally solves previously unseen Sokoban puzzles 93.7%
of the time, while using up to 26.8% fewer search steps than the $A^*$
implementation that was used for training initially. In our training method,
$A^*$'s search dynamics are expressed as a token sequence outlining when task
states are added and removed into the search tree during symbolic planning.
Searchformer significantly outperforms baselines that predict the optimal plan
directly with a 5-10$\times$ smaller model size and a 10$\times$ smaller
training dataset. Lastly, we demonstrate how Searchformer scales to larger and
more complex decision making tasks with improved percentage of solved tasks and
shortened search dynamics.",http://arxiv.org/abs/2402.14083v2,cs.AI
http://arxiv.org/abs/2402.10778v2,AutoGPT+P: Affordance-based Task Planning with Large Language Models,"Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour",2024-02-16,"Recent advances in task planning leverage Large Language Models (LLMs) to
improve generalizability by combining such models with classical planning
algorithms to address their inherent limitations in reasoning capabilities.
However, these approaches face the challenge of dynamically capturing the
initial state of the task planning problem. To alleviate this issue, we propose
AutoGPT+P, a system that combines an affordance-based scene representation with
a planning system. Affordances encompass the action possibilities of an agent
on the environment and objects present in it. Thus, deriving the planning
domain from an affordance-based scene representation allows symbolic planning
with arbitrary objects. AutoGPT+P leverages this representation to derive and
execute a plan for a task specified by the user in natural language. In
addition to solving planning tasks under a closed-world assumption, AutoGPT+P
can also handle planning with incomplete information, e. g., tasks with missing
objects by exploring the scene, suggesting alternatives, or providing a partial
plan. The affordance-based scene representation combines object detection with
an automatically generated object-affordance-mapping using ChatGPT. The core
planning tool extends existing work by automatically correcting semantic and
syntactic errors. Our approach achieves a success rate of 98%, surpassing the
current 81% success rate of the current state-of-the-art LLM-based planning
method SayCan on the SayCan instruction set. Furthermore, we evaluated our
approach on our newly created dataset with 150 scenarios covering a wide range
of complex tasks with missing objects, achieving a success rate of 79% on our
dataset. The dataset and the code are publicly available at
https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.",http://arxiv.org/abs/2402.10778v2,"cs.RO, cs.AI, I.2"
http://arxiv.org/abs/2402.08178v1,LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents,"Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang",2024-02-13,"Large language models (LLMs) have recently received considerable attention as
alternative solutions for task planning. However, comparing the performance of
language-oriented task planners becomes difficult, and there exists a dearth of
detailed exploration regarding the effects of various factors such as
pre-trained model selection and prompt construction. To address this, we
propose a benchmark system for automatically quantifying performance of task
planning for home-service embodied agents. Task planners are tested on two
pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of
Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform
extensive experiments with LLMs and prompts, and explore several enhancements
of the baseline planner. We expect that the proposed benchmark tool would
accelerate the development of language-oriented task planners.",http://arxiv.org/abs/2402.08178v1,cs.AI
http://arxiv.org/abs/2402.06608v2,"TIC: Translate-Infer-Compile for accurate ""text to plan"" using LLMs and Logical Representations","Sudhir Agarwal, Anu Sreepathy",2024-02-09,"We study the problem of generating plans for given natural language planning
task requests. On one hand, LLMs excel at natural language processing but do
not perform well on planning. On the other hand, classical planning tools excel
at planning tasks but require input in a structured language such as the
Planning Domain Definition Language (PDDL). We leverage the strengths of both
the techniques by using an LLM for generating the PDDL representation (task
PDDL) of planning task requests followed by using a classical planner for
computing a plan. Unlike previous approaches that use LLMs for generating task
PDDLs directly, our approach comprises of (a) translate: using an LLM only for
generating a logically interpretable intermediate representation of natural
language task description, (b) infer: deriving additional logically dependent
information from the intermediate representation using a logic reasoner
(currently, Answer Set Programming solver), and (c) compile: generating the
target task PDDL from the base and inferred information. We observe that using
an LLM to only output the intermediate representation significantly reduces LLM
errors. Consequently, TIC approach achieves, for at least one LLM, high
accuracy on task PDDL generation for all seven domains of our evaluation
dataset.",http://arxiv.org/abs/2402.06608v2,"cs.CL, cs.AI"
http://arxiv.org/abs/2401.07868v1,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,"Md Sadman Sakib, Yu Sun",2024-01-15,"The inherent probabilistic nature of Large Language Models (LLMs) introduces
an element of unpredictability, raising concerns about potential discrepancies
in their output. This paper introduces an innovative approach aims to generate
correct and optimal robotic task plans for diverse real-world demands and
scenarios. LLMs have been used to generate task plans, but they are unreliable
and may contain wrong, questionable, or high-cost steps. The proposed approach
uses LLM to generate a number of task plans as trees and amalgamates them into
a graph by removing questionable paths. Then an optimal task tree can be
retrieved to circumvent questionable and high-cost nodes, thereby improving
planning accuracy and execution efficiency. The approach is further improved by
incorporating a large knowledge network. Leveraging GPT-4 further, the
high-level task plan is converted into a low-level Planning Domain Definition
Language (PDDL) plan executable by a robot. Evaluation results highlight the
superior accuracy and efficiency of our approach compared to previous
methodologies in the field of task planning.",http://arxiv.org/abs/2401.07868v1,"cs.RO, cs.AI, cs.CL"
http://arxiv.org/abs/2401.04334v1,"Large Language Models for Robotics: Opportunities, Challenges, and Perspectives","Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Huaqin Zhao, Zhengliang Liu, Haixing Dai, Lin Zhao, Bao Ge, Xiang Li, Tianming Liu, Shu Zhang",2024-01-09,"Large language models (LLMs) have undergone significant expansion and have
been increasingly integrated across various domains. Notably, in the realm of
robot task planning, LLMs harness their advanced reasoning and language
comprehension capabilities to formulate precise and efficient action plans
based on natural language instructions. However, for embodied tasks, where
robots interact with complex environments, text-only LLMs often face challenges
due to a lack of compatibility with robotic visual perception. This study
provides a comprehensive overview of the emerging integration of LLMs and
multimodal LLMs into various robotic tasks. Additionally, we propose a
framework that utilizes multimodal GPT-4V to enhance embodied task planning
through the combination of natural language instructions and robot visual
perceptions. Our results, based on diverse datasets, indicate that GPT-4V
effectively enhances robot performance in embodied tasks. This extensive survey
and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks
enriches the understanding of LLM-centric embodied intelligence and provides
forward-looking insights toward bridging the gap in Human-Robot-Environment
interaction.",http://arxiv.org/abs/2401.04334v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2401.02500v2,On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS),"Vishal Pallagani, Kaushik Roy, Bharath Muppasani, Francesco Fabiano, Andrea Loreggia, Keerthiram Murugesan, Biplav Srivastava, Francesca Rossi, Lior Horesh, Amit Sheth",2024-01-04,"Automated Planning and Scheduling is among the growing areas in Artificial
Intelligence (AI) where mention of LLMs has gained popularity. Based on a
comprehensive review of 126 papers, this paper investigates eight categories
based on the unique applications of LLMs in addressing various aspects of
planning problems: language translation, plan generation, model construction,
multi-agent planning, interactive planning, heuristics optimization, tool
integration, and brain-inspired planning. For each category, we articulate the
issues considered and existing gaps. A critical insight resulting from our
review is that the true potential of LLMs unfolds when they are integrated with
traditional symbolic planners, pointing towards a promising neuro-symbolic
approach. This approach effectively combines the generative aspects of LLMs
with the precision of classical planning methods. By synthesizing insights from
existing literature, we underline the potential of this integration to address
complex planning challenges. Our goal is to encourage the ICAPS community to
recognize the complementary strengths of LLMs and symbolic planners, advocating
for a direction in automated planning that leverages these synergistic
capabilities to develop more advanced and intelligent planning systems.",http://arxiv.org/abs/2401.02500v2,cs.AI
http://arxiv.org/abs/2311.17842v2,Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning,"Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao",2023-11-29,"In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa's superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.",http://arxiv.org/abs/2311.17842v2,"cs.RO, cs.AI, cs.CL, cs.CV, cs.LG"
http://arxiv.org/abs/2311.13720v2,Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,"Turgay Caglar, Sirine Belhaj, Tathagata Chakraborti, Michael Katz, Sarath Sreedharan",2023-11-22,"This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this union, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) -- an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.",http://arxiv.org/abs/2311.13720v2,cs.AI
http://arxiv.org/abs/2311.13577v1,Physical Reasoning and Object Planning for Household Embodied Agents,"Ayush Agrawal, Raghav Prabhakar, Anirudh Goyal, Dianbo Liu",2023-11-22,"In this study, we explore the sophisticated domain of task planning for
robust household embodied agents, with a particular emphasis on the intricate
task of selecting substitute objects. We introduce the CommonSense Object
Affordance Task (COAT), a novel framework designed to analyze reasoning
capabilities in commonsense scenarios. This approach is centered on
understanding how these agents can effectively identify and utilize alternative
objects when executing household tasks, thereby offering insights into the
complexities of practical decision-making in real-world environments.Drawing
inspiration from human decision-making, we explore how large language models
tackle this challenge through three meticulously crafted commonsense
question-and-answer datasets, featuring refined rules and human annotations.
Our evaluation of state-of-the-art language models on these datasets sheds
light on three pivotal considerations: 1) aligning an object's inherent utility
with the task at hand, 2) navigating contextual dependencies (societal norms,
safety, appropriateness, and efficiency), and 3) accounting for the current
physical state of the object. To maintain accessibility, we introduce five
abstract variables reflecting an object's physical condition, modulated by
human insights to simulate diverse household scenarios. Our contributions
include insightful Object-Utility mappings addressing the first consideration
and two extensive QA datasets (15k and 130k questions) probing the intricacies
of contextual dependencies and object states. The datasets, along with our
findings, are accessible at: \url{https://github.com/com-phy-affordance/COAT}.
This research not only advances our understanding of physical commonsense
reasoning in language models but also paves the way for future improvements in
household agent intelligence.",http://arxiv.org/abs/2311.13577v1,cs.AI
http://arxiv.org/abs/2311.11315v1,TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems,"Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao",2023-11-19,"Large Language Models (LLMs) have demonstrated proficiency in addressing
tasks that necessitate a combination of task planning and the usage of external
tools that require a blend of task planning and the utilization of external
tools, such as APIs. However, real-world complex systems present three
prevalent challenges concerning task planning and tool usage: (1) The real
system usually has a vast array of APIs, so it is impossible to feed the
descriptions of all APIs to the prompt of LLMs as the token length is limited;
(2) the real system is designed for handling complex tasks, and the base LLMs
can hardly plan a correct sub-task order and API-calling order for such tasks;
(3) Similar semantics and functionalities among APIs in real systems create
challenges for both LLMs and even humans in distinguishing between them. In
response, this paper introduces a comprehensive framework aimed at enhancing
the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating
within real-world systems. Our framework comprises three key components
designed to address these challenges: (1) the API Retriever selects the most
pertinent APIs for the user task among the extensive array available; (2) LLM
Finetuner tunes a base LLM so that the finetuned LLM can be more capable for
task planning and API calling; (3) the Demo Selector adaptively retrieves
different demonstrations related to hard-to-distinguish APIs, which is further
used for in-context learning to boost the final performance. We validate our
methods using a real-world commercial system as well as an open-sourced
academic dataset, and the outcomes clearly showcase the efficacy of each
individual component as well as the integrated framework.",http://arxiv.org/abs/2311.11315v1,cs.AI
http://arxiv.org/abs/2311.09830v2,AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,"Katharina Stein, Daniel Fi≈°er, J√∂rg Hoffmann, Alexander Koller",2023-11-16,"LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present
AutoPlanBench, a novel method for automatically converting planning benchmarks
written in PDDL into textual descriptions and offer a benchmark dataset created
with our method. We show that while the best LLM planners do well on some
planning tasks, others remain out of reach of current methods.",http://arxiv.org/abs/2311.09830v2,"cs.AI, cs.CL"
http://arxiv.org/abs/2311.09830v4,Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning,"Katharina Stein, Daniel Fi≈°er, J√∂rg Hoffmann, Alexander Koller",2023-11-16,"Large language models (LLMs) have revolutionized a large variety of NLP
tasks. An active debate is to what extent they can do reasoning and planning.
Prior work has assessed the latter in the specific context of PDDL planning,
based on manually converting three PDDL domains into natural language (NL)
prompts. Here we automate this conversion step, showing how to leverage an LLM
to automatically generate NL prompts from PDDL input. Our automatically
generated NL prompts result in similar LLM-planning performance as the previous
manually generated ones. Beyond this, the automation enables us to run much
larger experiments, providing for the first time a broad evaluation of LLM
planning performance in PDDL. Our NL prompts yield better performance than PDDL
prompts and simple template-based NL prompts. Compared to symbolic planners,
LLM planning lags far behind; but in some domains, our best LLM configuration
scales up further than A$^\star$ using LM-cut.",http://arxiv.org/abs/2311.09830v4,"cs.AI, cs.CL"
http://arxiv.org/abs/2310.17140v1,Symbolic Planning and Code Generation for Grounded Dialogue,"Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried",2023-10-26,"Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.",http://arxiv.org/abs/2310.17140v1,"cs.CL, cs.AI"
http://arxiv.org/abs/2309.09181v1,From Cooking Recipes to Robot Task Trees -- Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,"Md Sadman Sakib, Yu Sun",2023-09-17,"Task planning for robotic cooking involves generating a sequence of actions
for a robot to prepare a meal successfully. This paper introduces a novel task
tree generation pipeline producing correct planning and efficient execution for
cooking tasks. Our method first uses a large language model (LLM) to retrieve
recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a
task tree, capturing sequential and parallel dependencies among subtasks. The
pipeline then mitigates the uncertainty and unreliable features of LLM outputs
using task tree retrieval. We combine multiple LLM task tree outputs into a
graph and perform a task tree retrieval to avoid questionable nodes and
high-cost nodes to improve planning correctness and improve execution
efficiency. Our evaluation results show its superior performance compared to
previous works in task planning accuracy and efficiency.",http://arxiv.org/abs/2309.09181v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2308.16505v3,Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations,"Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie",2023-08-31,"Recommender models excel at providing domain-specific item recommendations by
leveraging extensive user behavior data. Despite their ability to act as
lightweight domain experts, they struggle to perform versatile tasks such as
providing explanations and engaging in conversations. On the other hand, large
language models (LLMs) represent a significant step towards artificial general
intelligence, showcasing remarkable capabilities in instruction comprehension,
commonsense reasoning, and human interaction. However, LLMs lack the knowledge
of domain-specific item catalogs and behavioral patterns, particularly in areas
that diverge from general world knowledge, such as online e-commerce.
Finetuning LLMs for each domain is neither economic nor efficient.
  In this paper, we bridge the gap between recommender models and LLMs,
combining their respective strengths to create a versatile and interactive
recommender system. We introduce an efficient framework called
\textbf{InteRecAgent}, which employs LLMs as the brain and recommender models
as tools. We first outline a minimal set of essential tools required to
transform LLMs into InteRecAgent. We then propose an efficient workflow within
InteRecAgent for task execution, incorporating key components such as memory
components, dynamic demonstration-augmented task planning, and reflection.
InteRecAgent enables traditional recommender systems, such as those ID-based
matrix factorization models, to become interactive systems with a natural
language interface through the integration of LLMs. Experimental results on
several public datasets show that InteRecAgent achieves satisfying performance
as a conversational recommender system, outperforming general-purpose LLMs. The
source code of InteRecAgent is released at https://aka.ms/recagent.",http://arxiv.org/abs/2308.16505v3,"cs.IR, cs.AI"
http://arxiv.org/abs/2308.13724v1,ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,"Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, Lei Ma",2023-08-26,"Motivated by the substantial achievements observed in Large Language Models
(LLMs) in the field of natural language processing, recent research has
commenced investigations into the application of LLMs for complex, long-horizon
sequential task planning challenges in robotics. LLMs are advantageous in
offering the potential to enhance the generalizability as task-agnostic
planners and facilitate flexible interaction between human instructors and
planning systems. However, task plans generated by LLMs often lack feasibility
and correctness. To address this challenge, we introduce ISR-LLM, a novel
framework that improves LLM-based planning through an iterative self-refinement
process. The framework operates through three sequential steps: preprocessing,
planning, and iterative self-refinement. During preprocessing, an LLM
translator is employed to convert natural language input into a Planning Domain
Definition Language (PDDL) formulation. In the planning phase, an LLM planner
formulates an initial plan, which is then assessed and refined in the iterative
self-refinement step by using a validator. We examine the performance of
ISR-LLM across three distinct planning domains. The results show that ISR-LLM
is able to achieve markedly higher success rates in task accomplishments
compared to state-of-the-art LLM-based planners. Moreover, it also preserves
the broad applicability and generalizability of working with natural language
instructions.",http://arxiv.org/abs/2308.13724v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2308.12682v2,SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,"Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt",2023-08-24,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",http://arxiv.org/abs/2308.12682v2,cs.AI
http://arxiv.org/abs/2308.03427v3,TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,"Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao",2023-08-07,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",http://arxiv.org/abs/2308.03427v3,cs.AI
http://arxiv.org/abs/2307.14740v1,New Interaction Paradigm for Complex EDA Software Leveraging GPT,"Boyu Han, Xinyu Wang, Yifan Wang, Junyu Yan, Yidong Tian",2023-07-27,"In the rapidly growing field of electronic design automation (EDA),
professional software such as KiCad, Cadence , and Altium Designer provide
increasingly extensive design functionalities. However, the intricate command
structure and high learning curve create a barrier, particularly for novice
printed circuit board (PCB) designers. This results in difficulties in
selecting appropriate functions or plugins for varying design purposes,
compounded by the lack of intuitive learning methods beyond traditional
documentation, videos, and online forums. To address this challenge, an
artificial intelligence (AI) interaction assist plugin for EDA software named
SmartonAl is developed here, also KiCad is taken as the first example.
SmartonAI is inspired by the HuggingGPT framework and employs large language
models, such as GPT and BERT, to facilitate task planning and execution. On
receiving a designer request, SmartonAI conducts a task breakdown and
efficiently executes relevant subtasks, such as analysis of help documentation
paragraphs and execution of different plugins, along with leveraging the
built-in schematic and PCB manipulation functions in both SmartonAl itself and
software. Our preliminary results demonstrate that SmartonAI can significantly
streamline the PCB design process by simplifying complex commands into
intuitive language-based interactions. By harnessing the powerful language
capabilities of ChatGPT and the rich design functions of KiCad, the plugin
effectively bridges the gap between complex EDA software and user-friendly
interaction. Meanwhile, the new paradigm behind SmartonAI can also extend to
other complex software systems, illustrating the immense potential of
AI-assisted user interfaces in advancing digital interactions across various
domains.",http://arxiv.org/abs/2307.14740v1,"cs.SE, cs.AI"
http://arxiv.org/abs/2307.06135v2,SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf",2023-07-12,"Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.",http://arxiv.org/abs/2307.06135v2,"cs.RO, cs.AI"
http://arxiv.org/abs/2305.19234v3,Grammar Prompting for Domain-Specific Language Generation with Large Language Models,"Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, Yoon Kim",2023-05-30,"Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.",http://arxiv.org/abs/2305.19234v3,"cs.CL, cs.AI"
http://arxiv.org/abs/2305.17590v2,Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds,"Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang",2023-05-27,"Task planning systems have been developed to help robots use human knowledge
(about actions) to complete long-horizon tasks. Most of them have been
developed for ""closed worlds"" while assuming the robot is provided with
complete world knowledge. However, the real world is generally open, and the
robots frequently encounter unforeseen situations that can potentially break
the planner's completeness. Could we leverage the recent advances on
pre-trained Large Language Models (LLMs) to enable classical planning systems
to deal with novel situations?
  This paper introduces a novel framework, called COWP, for open-world task
planning and situation handling. COWP dynamically augments the robot's action
knowledge, including the preconditions and effects of actions, with
task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and
is grounded to specific domains via action knowledge. For systematic
evaluations, we collected a dataset that includes 1,085 execution-time
situations. Each situation corresponds to a state instance wherein a robot is
potentially unable to complete a task using a solution that normally works.
Experimental results show that our approach outperforms competitive baselines
from the literature in the success rate of service tasks. Additionally, we have
demonstrated COWP using a mobile manipulator. Supplementary materials are
available at: https://cowplanning.github.io/",http://arxiv.org/abs/2305.17590v2,"cs.RO, cs.AI"
http://arxiv.org/abs/2305.16151v1,Understanding the Capabilities of Large Language Models for Automated Planning,"Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, Andrea Loreggia",2023-05-25,"Automated planning is concerned with developing efficient algorithms to
generate plans or sequences of actions to achieve a specific goal in a given
environment. Emerging Large Language Models (LLMs) can answer questions, write
high-quality programming code, and predict protein folding, showcasing their
versatility in solving various tasks beyond language-based problems. In this
paper, we aim to explore how LLMs can also be used for automated planning. To
do so, we seek to answer four key questions. Firstly, we want to understand the
extent to which LLMs can be used for plan generation. Secondly, we aim to
identify which pre-training data is most effective in facilitating plan
generation. Thirdly, we investigate whether fine-tuning or prompting is a more
effective approach for plan generation. Finally, we explore whether LLMs are
capable of plan generalization. By answering these questions, the study seeks
to shed light on the capabilities of LLMs in solving complex planning problems
and provide insights into the most effective approaches for using LLMs in this
context.",http://arxiv.org/abs/2305.16151v1,cs.AI
http://arxiv.org/abs/2305.14909v2,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,"Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati",2023-05-24,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",http://arxiv.org/abs/2305.14909v2,cs.AI
http://arxiv.org/abs/2305.11014v2,Generalized Planning in PDDL Domains with Pretrained Large Language Models,"Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz",2023-05-18,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",http://arxiv.org/abs/2305.11014v2,cs.AI
http://arxiv.org/abs/2305.07716v1,Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning,"Georgia Chalvatzaki, Ali Younes, Daljeet Nandha, An Le, Leonardo F. R. Ribeiro, Iryna Gurevych",2023-05-12,"Long-horizon task planning is essential for the development of intelligent
assistive and service robots. In this work, we investigate the applicability of
a smaller class of large language models (LLMs), specifically GPT-2, in robotic
task planning by learning to decompose tasks into subgoal specifications for a
planner to execute sequentially. Our method grounds the input of the LLM on the
domain that is represented as a scene graph, enabling it to translate human
requests into executable robot plans, thereby learning to reason over
long-horizon tasks, as encountered in the ALFRED benchmark. We compare our
approach with classical planning and baseline methods to examine the
applicability and generalizability of LLM-based planners. Our findings suggest
that the knowledge stored in an LLM can be effectively grounded to perform
long-horizon task planning, demonstrating the promising potential for the
future application of neuro-symbolic planning methods in robotics.",http://arxiv.org/abs/2305.07716v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2304.11477v3,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,"Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone",2023-04-22,"Large language models (LLMs) have demonstrated remarkable zero-shot
generalization abilities: state-of-the-art chatbots can provide plausible
answers to many common questions that arise in daily life. However, so far,
LLMs cannot reliably solve long-horizon planning problems. By contrast,
classical planners, once a problem is given in a formatted way, can use
efficient search algorithms to quickly identify correct, or even optimal,
plans. In an effort to get the best of both worlds, this paper introduces
LLM+P, the first framework that incorporates the strengths of classical
planners into LLMs. LLM+P takes in a natural language description of a planning
problem, then returns a correct (or optimal) plan for solving that problem in
natural language. LLM+P does so by first converting the language description
into a file written in the planning domain definition language (PDDL), then
leveraging classical planners to quickly find a solution, and then translating
the found solution back into natural language. Along with LLM+P, we define a
diverse set of different benchmark problems taken from common planning
scenarios. Via a comprehensive set of experiments on these benchmark problems,
we find that LLM+P is able to provide optimal solutions for most problems,
while LLMs fail to provide even feasible plans for most problems.\footnote{The
code and results are publicly available at
https://github.com/Cranial-XIX/llm-pddl.git.",http://arxiv.org/abs/2304.11477v3,"cs.AI, cs.RO"
http://arxiv.org/abs/2303.00438v3,A Framework for Neurosymbolic Robot Action Planning using Large Language Models,"Alessio Capitanelli, Fulvio Mastrogiovanni",2023-03-01,"Symbolic task planning is a widely used approach to enforce robot autonomy
due to its ease of understanding and deployment in robot architectures.
However, techniques for symbolic task planning are difficult to scale in
real-world, human-robot collaboration scenarios because of the poor performance
in complex planning domains or when frequent re-planning is needed. We present
a framework, Teriyaki, specifically aimed at bridging the gap between symbolic
task planning and machine learning approaches. The rationale is training Large
Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner
compatible with the Planning Domain Definition Language (PDDL), and then
leveraging its generative capabilities to overcome a number of limitations
inherent to symbolic task planners. Potential benefits include (i) a better
scalability in so far as the planning domain complexity increases, since LLMs'
response time linearly scales with the combined length of the input and the
output, and (ii) the ability to synthesize a plan action-by-action instead of
end-to-end, making each action available for execution as soon as it is
generated instead of waiting for the whole plan to be available, which in turn
enables concurrent planning and execution. Recently, significant efforts have
been devoted by the research community to evaluate the cognitive capabilities
of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an
overall planning performance comparable to traditional planners in specific
planning domains, while leveraging LLMs capabilities to build a look-ahead
predictive planning model. Preliminary results in selected domains show that
our method can: (i) solve 95.5% of problems in a test data set of 1,000
samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic
planner; (iii) reduce average overall waiting times for a plan availability by
up to 61.4%",http://arxiv.org/abs/2303.00438v3,"cs.AI, cs.LG, cs.RO, I.2.6; I.2.8; I.2.9"
http://arxiv.org/abs/2302.01560v3,"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang",2023-02-03,"We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
""$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect"" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",http://arxiv.org/abs/2302.01560v3,cs.AI
http://arxiv.org/abs/2212.08681v1,Plansformer: Generating Symbolic Plans using Transformers,"Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, Andrea Loreggia",2022-12-16,"Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain.",http://arxiv.org/abs/2212.08681v1,cs.AI
http://arxiv.org/abs/2210.01287v1,Robot Task Planning and Situation Handling in Open Worlds,"Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Chad Esselink, Shiqi Zhang",2022-10-04,"Automated task planning algorithms have been developed to help robots
complete complex tasks that require multiple actions. Most of those algorithms
have been developed for ""closed worlds"" assuming complete world knowledge is
provided. However, the real world is generally open, and the robots frequently
encounter unforeseen situations that can potentially break the planner's
completeness. This paper introduces a novel algorithm (COWP) for open-world
task planning and situation handling that dynamically augments the robot's
action knowledge with task-oriented common sense. In particular, common sense
is extracted from Large Language Models based on the current task at hand and
robot skills. For systematic evaluations, we collected a dataset that includes
561 execution-time situations in a dining domain, where each situation
corresponds to a state instance of a robot being potentially unable to complete
a task using a solution that normally works. Experimental results show that our
approach significantly outperforms competitive baselines from the literature in
the success rate of service tasks. Additionally, we have demonstrated COWP
using a mobile manipulator. Supplementary materials are available at:
https://cowplanning.github.io/",http://arxiv.org/abs/2210.01287v1,"cs.RO, cs.AI"
http://arxiv.org/abs/2206.10498v4,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati",2022-06-21,"Generating plans of action, and reasoning about change have long been
considered a core competence of intelligent agents. It is thus no surprise that
evaluating the planning and reasoning capabilities of large language models
(LLMs) has become a hot topic of research. Most claims about LLM planning
capabilities are however based on common sense tasks-where it becomes hard to
tell whether LLMs are planning or merely retrieving from their vast world
knowledge. There is a strong need for systematic and extensible planning
benchmarks with sufficient diversity to evaluate whether LLMs have innate
planning capabilities. Motivated by this, we propose PlanBench, an extensible
benchmark suite based on the kinds of domains used in the automated planning
community, especially in the International Planning Competition, to test the
capabilities of LLMs in planning or reasoning about actions and change.
PlanBench provides sufficient diversity in both the task domains and the
specific planning capabilities. Our studies also show that on many critical
capabilities-including plan generation-LLM performance falls quite short, even
with the SOTA models. PlanBench can thus function as a useful marker of
progress of LLMs in planning and reasoning.",http://arxiv.org/abs/2206.10498v4,"cs.CL, cs.AI"
http://arxiv.org/abs/1810.09245v1,A Review on Learning Planning Action Models for Socio-Communicative HRI,"Ankuj Arora, Humbert Fiorino, Damien Pellier, Sylvie Pesty",2018-10-22,"For social robots to be brought more into widespread use in the fields of
companionship, care taking and domestic help, they must be capable of
demonstrating social intelligence. In order to be acceptable, they must exhibit
socio-communicative skills. Classic approaches to program HRI from observed
human-human interactions fails to capture the subtlety of multimodal
interactions as well as the key structural differences between robots and
humans. The former arises due to a difficulty in quantifying and coding
multimodal behaviours, while the latter due to a difference of the degrees of
liberty between a robot and a human. However, the notion of reverse engineering
from multimodal HRI traces to learn the underlying behavioral blueprint of the
robot given multimodal traces seems an option worth exploring. With this
spirit, the entire HRI can be seen as a sequence of exchanges of speech acts
between the robot and human, each act treated as an action, bearing in mind
that the entire sequence is goal-driven. Thus, this entire interaction can be
treated as a sequence of actions propelling the interaction from its initial to
goal state, also known as a plan in the domain of AI planning. In the same
domain, this action sequence that stems from plan execution can be represented
as a trace. AI techniques, such as machine learning, can be used to learn
behavioral models (also known as symbolic action models in AI), intended to be
reusable for AI planning, from the aforementioned multimodal traces. This
article reviews recent machine learning techniques for learning planning action
models which can be applied to the field of HRI with the intent of rendering
robots as socio-communicative.",http://arxiv.org/abs/1810.09245v1,cs.AI
http://arxiv.org/abs/1109.2153v1,mGPT: A Probabilistic Planner Based on Heuristic Search,"B. Bonet, H. Geffner",2011-09-09,"We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version, called mGPT,
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different,
independent, deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy.",http://arxiv.org/abs/1109.2153v1,cs.AI
