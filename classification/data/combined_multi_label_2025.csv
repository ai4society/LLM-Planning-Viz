title,abstract,link,authors,category
A Framework to Generate Neurosymbolic PDDL-compliant Planners,"Symbolic task planning is a widely used approach to enforce robot autonomy
due to its ease of understanding and deployment in robot architectures.
However, techniques for symbolic task planning are difficult to scale in
real-world, human-robot collaboration scenarios because of the poor performance
in complex planning domains or when frequent re-planning is needed. We present
a framework, Teriyaki, specifically aimed at bridging the gap between symbolic
task planning and machine learning approaches. The rationale is training Large
Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner
compatible with the Planning Domain Definition Language (PDDL), and then
leveraging its generative capabilities to overcome a number of limitations
inherent to symbolic task planners. Potential benefits include (i) a better
scalability in so far as the planning domain complexity increases, since LLMs'
response time linearly scales with the combined length of the input and the
output, and (ii) the ability to synthesize a plan action-by-action instead of
end-to-end, making each action available for execution as soon as it is
generated instead of waiting for the whole plan to be available, which in turn
enables concurrent planning and execution. Recently, significant efforts have
been devoted by the research community to evaluate the cognitive capabilities
of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an
overall planning performance comparable to traditional planners in specific
planning domains, while leveraging LLMs capabilities to build a look-ahead
predictive planning model. Preliminary results in selected domains show that
our method can: (i) solve 95.5% of problems in a test data set of 1,000
samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic
planner; (iii) reduce average overall waiting times for a plan availability by
up to 61.4%",https://arxiv.org/abs/2303.00438,"Capitanelli, Alessio and Mastrogiovanni, Fulvio",['plan-generation']
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models,"Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on three
challenging planning tasks -- graph traversal, Tower of Hanoi, and logistics --
finding that it yields significant improvements over standard LLM methods
(e.g., zero-shot prompting, in-context learning, and chain-of-thought). These
results demonstrate the benefit of utilizing knowledge from cognitive
neuroscience to improve planning in LLMs.",https://arxiv.org/abs/2310.00194,"Webb, Taylor and Mondal, Shanka Subhra and Wang, Chi and Krabach, Brian and Momennejad, Ida",['brain-inspired-planning']
AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/b5c8c1c117618267944b2617add0a766-Abstract-Conference.html,"Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao",['interactive-planning']
Alphazero-like tree-search can guide large language model decoding and training,"Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim
to augment the reasoning capabilities of LLMs by using tree-search algorithms
to guide multi-step reasoning. These methods rely on prompting a pre-trained
model to serve as a value function and focus on problems with low search depth.
As a result, these methods will not work in domains where the pre-trained LLM
does not have enough knowledge to serve as an effective value function or in
domains that require long-horizon planning. To address these limitations, we
present an AlphaZero-like tree-search learning framework for LLMs (termed
TS-LLM), systematically illustrating how tree-search with a learned value
function can guide LLM decoding. TS-LLM distinguishes itself in two key ways.
(1) Leveraging a learned value function and AlphaZero-like algorithms, our
approach can be generally adaptable to a wide range of tasks, language models
of any size, and tasks of varying search depths. (2) Our approach can guide
LLMs during both inference and training, iteratively improving the LLM.
Empirical results across reasoning, planning, alignment, and decision-making
tasks show that TS-LLM outperforms existing approaches and can handle trees
with a depth of 64.",https://arxiv.org/abs/2309.17179,"Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun",['heuristics-optimization']
Api-bank: A benchmark for tool-augmented llms,"Recent research has demonstrated that Large Language Models (LLMs) can
enhance their capabilities by utilizing external tools. However, three pivotal
questions remain unanswered: (1) How effective are current LLMs in utilizing
tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What
obstacles need to be overcome to leverage tools? To address these questions, we
introduce API-Bank, a groundbreaking benchmark, specifically designed for
tool-augmented LLMs. For the first question, we develop a runnable evaluation
system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753
API calls to assess the existing LLMs' capabilities in planning, retrieving,
and calling APIs. For the second question, we construct a comprehensive
training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000
distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM
initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits
improved tool utilization compared to GPT-3, while GPT-4 excels in planning.
However, there is still significant potential for further improvement.
Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26
pts and approaches the effectiveness of GPT-3.5. Through error analysis, we
highlight the key challenges for future research in this field to answer the
third question.",https://arxiv.org/abs/2304.08244,"Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin",['tool-integration']
Asking Before Action: Gather Information in Embodied Decision Making with Language Models,"With strong capabilities of reasoning and a broad understanding of the world,
Large Language Models (LLMs) have demonstrated immense potential in building
versatile embodied decision-making agents capable of executing a wide array of
tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM
agents encounter challenges in efficiently gathering essential information,
leading to suboptimal performance. Conversely, human individuals often seek
additional information from their peers prior to taking action, harnessing
external knowledge to avoid unnecessary trial and error. Drawing inspiration
from this behavior, we propose \textit{Asking Before Acting} (ABA), a method
that empowers the agent to proactively inquire with external sources for
pertinent information using natural language during their interactions within
the environment. In this way, the agent is able to enhance its efficiency and
performance by circumventing potentially laborious steps and combating the
difficulties associated with exploration in unfamiliar environments and
vagueness of the instructions. We conduct extensive experiments involving a
spectrum of environments including text-based household everyday tasks, robot
arm manipulation tasks, and real world open domain image based embodied tasks.
The experiments involve various models from Vicuna to GPT-4. The results
demonstrate that, even with modest prompts modifications, ABA exhibits
substantial advantages on both performance and efficiency over baseline LLM
agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates
learning the rationale for asking and allows for additional enhancements
especially in tasks that baselines struggle to solve.",https://arxiv.org/abs/2305.15695,"Chen, Xiaoyu and Zhang, Shenao and Zhang, Pushi and Zhao, Li and Chen, Jianyu",['interactive-planning']
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers,"For effective human-robot interaction, robots need to understand, plan, and
execute complex, long-horizon tasks described by natural language. Recent
advances in large language models (LLMs) have shown promise for translating
natural language into robot action sequences for complex tasks. However,
existing approaches either translate the natural language directly into robot
trajectories or factor the inference process by decomposing language into task
sub-goals and relying on a motion planner to execute each sub-goal. When
complex environmental and temporal constraints are involved, inference over
planning tasks must be performed jointly with motion plans using traditional
task-and-motion planning (TAMP) algorithms, making factorization into subgoals
untenable. Rather than using LLMs to directly plan task sub-goals, we instead
perform few-shot translation from natural language task descriptions to an
intermediate task representation that can then be consumed by a TAMP algorithm
to jointly solve the task and motion plan. To improve translation, we
automatically detect and correct both syntactic and semantic errors via
autoregressive re-prompting, resulting in significant improvements in task
completion. We show that our approach outperforms several methods using LLMs as
planners in complex task domains. See our project website
https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.",https://arxiv.org/abs/2306.06531,"Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu",['plan-generation']
Bootstrap your own skills: Learning to solve new tasks with large language model guidance,"We propose BOSS, an approach that automatically learns to solve new
long-horizon, complex, and meaningful tasks by growing a learned skill library
with minimal supervision. Prior work in reinforcement learning require expert
supervision, in the form of demonstrations or rich reward functions, to learn
long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)
learns to accomplish new tasks by performing ""skill bootstrapping,"" where an
agent with a set of primitive skills interacts with the environment to practice
new skills without receiving reward feedback for tasks outside of the initial
skill set. This bootstrapping phase is guided by large language models (LLMs)
that inform the agent of meaningful skills to chain together. Through this
process, BOSS builds a wide range of complex and useful behaviors from a basic
set of primitive skills. We demonstrate through experiments in realistic
household environments that agents trained with our LLM-guided bootstrapping
procedure outperform those trained with naive bootstrapping as well as prior
unsupervised skill acquisition methods on zero-shot execution of unseen,
long-horizon tasks in new environments. Website at clvrai.com/boss.",https://arxiv.org/abs/2310.10021,"Zhang, Jesse and Zhang, Jiahui and Pertsch, Karl and Liu, Ziyi and Ren, Xiang and Chang, Minsuk and Sun, Shao-Hua and Lim, Joseph J",['plan-generation']
Building cooperative embodied agents modularly with large language models,"In this work, we address challenging multi-agent cooperation problems with
decentralized control, raw sensory observations, costly communication, and
multi-objective tasks instantiated in various embodied environments. While
previous research either presupposes a cost-free communication channel or
relies on a centralized controller with shared observations, we harness the
commonsense knowledge, reasoning ability, language comprehension, and text
generation prowess of LLMs and seamlessly incorporate them into a
cognitive-inspired modular framework that integrates with perception, memory,
and execution. Thus building a Cooperative Embodied Language Agent CoELA, who
can plan, communicate, and cooperate with others to accomplish long-horizon
tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA
driven by GPT-4 can surpass strong planning-based methods and exhibit emergent
effective communication. Though current Open LMs like LLAMA-2 still
underperform, we fine-tune a CoELA with data collected with our agents and show
how they can achieve promising performance. We also conducted a user study for
human-agent interaction and discovered that CoELA communicating in natural
language can earn more trust and cooperate more effectively with humans. Our
research underscores the potential of LLMs for future research in multi-agent
cooperation. Videos can be found on the project website
https://vis-www.cs.umass.edu/Co-LLM-Agents/.",https://arxiv.org/abs/2307.02485,"Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B and Shu, Tianmin and Gan, Chuang",['multiagent-planning']
Can Large Language Models Really Improve by Self-critiquing Their Own Plans?,"There have been widespread claims about Large Language Models (LLMs) being
able to successfully verify or self-critique their candidate solutions in
reasoning problems in an iterative mode. Intrigued by those claims, in this
paper we set out to investigate the verification/self-critiquing abilities of
large language models in the context of planning. We evaluate a planning system
that employs LLMs for both plan generation and verification. We assess the
verifier LLM's performance against ground-truth verification, the impact of
self-critiquing on plan generation, and the influence of varying feedback
levels on system performance. Using GPT-4, a state-of-the-art LLM, for both
generation and verification, our findings reveal that self-critiquing appears
to diminish plan generation performance, especially when compared to systems
with external, sound verifiers and the LLM verifiers in that system produce a
notable number of false positives, compromising the system's reliability.
Additionally, the nature of feedback, whether binary or detailed, showed
minimal impact on plan generation. Collectively, our results cast doubt on the
effectiveness of LLMs in a self-critiquing, iterative framework for planning
tasks.",https://arxiv.org/abs/2310.08118,"Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao",['plan-generation']
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models,"In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning",https://arxiv.org/abs/2305.10276,"Hu, Hanxu and Lu, Hongyuan and Zhang, Huajian and Lam, Wai and Zhang, Yue",['plan-generation']
Chameleon: Plug-and-play compositional reasoning with large language models,"Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html,"Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng",['tool-integration']
CoPAL: Corrective Planning of Robot Actions with Large Language Models,"In the pursuit of fully autonomous robotic systems capable of taking over
tasks traditionally performed by humans, the complexity of open-world
environments poses a considerable challenge. Addressing this imperative, this
study contributes to the field of Large Language Models (LLMs) applied to task
and motion planning for robots. We propose a system architecture that
orchestrates a seamless interplay between multiple cognitive levels,
encompassing reasoning, planning, and motion generation. At its core lies a
novel replanning strategy that handles physically grounded, logical, and
semantic errors in the generated plans. We demonstrate the efficacy of the
proposed feedback architecture, particularly its impact on executability,
correctness, and time complexity via empirical evaluation in the context of a
simulation and two intricate real-world scenarios: blocks world, barman and
pizza preparation.",https://arxiv.org/abs/2310.07263,"Joublin, Frank and Ceravola, Antonello and Smirnov, Pavel and Ocker, Felix and Deigmoeller, Joerg and Belardinelli, Anna and Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Gienger, Michael",['plan-generation']
Cognitive architectures for language agents,"Recent efforts have augmented large language models (LLMs) with external
resources (e.g., the Internet) or internal control flows (e.g., prompt
chaining) for tasks requiring grounding or reasoning, leading to a new class of
language agents. While these agents have achieved substantial empirical
success, we lack a systematic framework to organize existing agents and plan
future developments. In this paper, we draw on the rich history of cognitive
science and symbolic artificial intelligence to propose Cognitive Architectures
for Language Agents (CoALA). CoALA describes a language agent with modular
memory components, a structured action space to interact with internal memory
and external environments, and a generalized decision-making process to choose
actions. We use CoALA to retrospectively survey and organize a large body of
recent work, and prospectively identify actionable directions towards more
capable agents. Taken together, CoALA contextualizes today's language agents
within the broader history of AI and outlines a path towards language-based
general intelligence.",https://arxiv.org/abs/2309.02427,"Sumers, Theodore and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L",['brain-inspired-planning']
Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning,"For robots to perform a wide variety of tasks, they require a 3D
representation of the world that is semantically rich, yet compact and
efficient for task-driven perception and planning. Recent approaches have
attempted to leverage features from large vision-language models to encode
semantics in 3D representations. However, these approaches tend to produce maps
with per-point feature vectors, which do not scale well in larger environments,
nor do they contain semantic spatial relationships between entities in the
environment, which are useful for downstream planning. In this work, we propose
ConceptGraphs, an open-vocabulary graph-structured representation for 3D
scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing
their output to 3D by multi-view association. The resulting representations
generalize to novel semantic classes, without the need to collect large 3D
datasets or finetune models. We demonstrate the utility of this representation
through a number of downstream planning tasks that are specified through
abstract (language) prompts and require complex reasoning over spatial and
semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer
video: https://youtu.be/mRhNkQwRYnc )",https://arxiv.org/abs/2309.16650,"Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others",['plan-generation']
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help,"This paper addresses planning problems for mobile robots. We consider
missions that require accomplishing multiple high-level sub-tasks, expressed in
natural language (NL), in a temporal and logical order. To formally define the
mission, we treat these sub-tasks as atomic predicates in a Linear Temporal
Logic (LTL) formula. We refer to this task specification framework as LTL-NL.
Our goal is to design plans, defined as sequences of robot actions,
accomplishing LTL-NL tasks. This action planning problem cannot be solved
directly by existing LTL planners because of the NL nature of atomic
predicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolic
planner that relies on a novel integration of (i) existing symbolic planners
generating high-level task plans determining the order at which the NL
sub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)
to design sequences of robot actions based on these task plans; and (iii)
conformal prediction acting as a formal interface between (i) and (ii) and
managing uncertainties due to LLM imperfections. We show, both theoretically
and empirically, that HERACLEs can achieve user-defined mission success rates.
Finally, we provide comparative experiments demonstrating that HERACLEs
outperforms LLM-based planners that require the mission to be defined solely
using NL. Additionally, we present examples demonstrating that our approach
enhances user-friendliness compared to conventional symbolic approaches.",https://arxiv.org/abs/2309.10092,"Wang, Jun and Tong, Jiaming and Tan, Kaiyuan and Vorobeychik, Yevgeniy and Kantaros, Yiannis",['plan-generation']
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text,"While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM's adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.",https://arxiv.org/abs/2307.07696,"Yang, Zhun and Ishay, Adam and Lee, Joohyung",['language-translation']
Creative Robot Tool Use with Large Language Models,"Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's ""hands and eyes,"" while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at https://say-can.github.io/.",https://arxiv.org/abs/2204.01691,"Xu, Mengdi and Huang, Peide and Yu, Wenhao and Liu, Shiqi and Zhang, Xilun and Niu, Yaru and Zhang, Tingnan and Xia, Fei and Tan, Jie and Zhao, Ding",['language-translation']
"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents","We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
""$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect"" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",https://arxiv.org/abs/2302.01560,"Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao",['plan-generation']
Distilling Script Knowledge from Large Language Models for Constrained Language Planning,"In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., ""make a cake""), but leaves more specific goals with multi-facet
constraints understudied (e.g., ""make a cake for diabetics""). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.",https://arxiv.org/abs/2305.05252,"Yuan, Siyu and Chen, Jiangjie and Fu, Ziquan and Ge, Xuyang and Shah, Soham and Jankowski, Charles Robert and Yang, Deqing and Xiao, Yanghua",['plan-generation']
Diversity of Thought Improves Reasoning Abilities of Large Language Models,"Large language models (LLMs) are documented to struggle in settings that
require complex reasoning. Nevertheless, instructing the model to break down
the problem into smaller reasoning steps, or ensembling various generations
through modifying decoding steps boosts performance. However, these methods
assume that the input prompt is fixed and expect the decoding strategies to
introduce the diversity needed for ensembling. In this work, we discuss how one
can create and leverage variations of the input prompt as a means of diversity
of thought. We propose a method that automatically improves prompt diversity by
soliciting feedback from the LLM to ideate approaches that are apt for the
problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse
reasoning path Self-Ensemble) across multiple inference calls, or use diverse
approaches within a single inference call; we call the latter IDIV-SE (In-call
DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming
prior work, DIV-SE(in particular) advances state-of-the-art performance on the
challenging planning and graph coloring benchmarks. Our results improve the
Pareto frontier of the accuracy-cost trade-off.",https://arxiv.org/abs/2310.07088,"Naik, Ranjita and Chandrasekaran, Varun and Yuksekgonul, Mert and Palangi, Hamid and Nushi, Besmira",['interactive-planning']
"Do as i can, not as i say: Grounding language in robotic affordances","Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ""hands and eyes,"" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at [this https URL](https://say-can.github.io/).
",https://arxiv.org/abs/2204.01691,"Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others",['language-translation']
Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling,"Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.",https://arxiv.org/abs/2301.12050,"Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy",['model-construction']
DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs,"Mobile robots often rely on pre-existing maps for effective path planning and
navigation. However, when these maps are unavailable, particularly in
unfamiliar environments, a different approach become essential. This paper
introduces DynaCon, a novel system designed to provide mobile robots with
contextual awareness and dynamic adaptability during navigation, eliminating
the reliance of traditional maps. DynaCon integrates real-time feedback with an
object server, prompt engineering, and navigation modules. By harnessing the
capabilities of Large Language Models (LLMs), DynaCon not only understands
patterns within given numeric series but also excels at categorizing objects
into matched spaces. This facilitates dynamic path planner imbued with
contextual awareness. We validated the effectiveness of DynaCon through an
experiment where a robot successfully navigated to its goal using reasoning.
Source code and experiment videos for this work can be found at:
https://sites.google.com/view/dynacon.",https://arxiv.org/abs/2309.16031,"Kim, Gyeongmin and Kim, Taehyeon and Kannan, Shyam Sundar and Venkatesh, Vishnunandan LN and Kim, Donghan and Min, Byung-Cheol",['plan-generation']
Dynamic Planning with a LLM,"While Large Language Models (LLMs) can solve many NLP tasks in zero-shot
settings, applications involving embodied agents remain problematic. In
particular, complex plans that require multi-step reasoning become difficult
and too costly as the context window grows. Planning requires understanding the
likely effects of one's actions and identifying whether the current environment
satisfies the goal state. While symbolic planners find optimal solutions
quickly, they require a complete and accurate representation of the planning
problem, severely limiting their use in practical scenarios. In contrast,
modern LLMs cope with noisy observations and high levels of uncertainty when
reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a
neuro-symbolic framework where an LLM works hand-in-hand with a traditional
planner to solve an embodied task. Given action-descriptions, LLM-DP solves
Alfworld faster and more efficiently than a naive LLM ReAct baseline.",https://arxiv.org/abs/2308.06391,"Dagan, Gautier and Keller, Frank and Lascarides, Alex",['plan-generation']
EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation,"Plan-and-Write is a common hierarchical approach in long-form narrative text
generation, which first creates a plan to guide the narrative writing.
Following this approach, several studies rely on simply prompting large
language models for planning, which often yields suboptimal results. In this
paper, we propose a new framework called Evaluation-guided Iterative Plan
Extraction for long-form narrative text generation (EIPE-text), which extracts
plans from the corpus of narratives and utilizes the extracted plans to
construct a better planner. EIPE-text has three stages: plan extraction,
learning, and inference. In the plan extraction stage, it iteratively extracts
and improves plans from the narrative corpus and constructs a plan corpus. We
propose a question answer (QA) based evaluation mechanism to automatically
evaluate the plans and generate detailed plan refinement instructions to guide
the iterative improvement. In the learning stage, we build a better planner by
fine-tuning with the plan corpus or in-context learning with examples in the
plan corpus. Finally, we leverage a hierarchical approach to generate long-form
narratives. We evaluate the effectiveness of EIPE-text in the domains of novels
and storytelling. Both GPT-4-based evaluations and human evaluations
demonstrate that our method can generate more coherent and relevant long-form
narratives. Our code will be released in the future.",https://arxiv.org/abs/2310.08185,"You, Wang and Wu, Wenshan and Liang, Yaobo and Mao, Shaoguang and Wu, Chenfei and Cao, Maosong and Cai, Yuzhe and Guo, Yiduo and Xia, Yan and Wei, Furu and others",['language-translation']
Embodied task planning with large language models,"Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.",https://arxiv.org/abs/2307.01848,"Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin",['model-construction']
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach,"Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.",https://arxiv.org/abs/2306.03604,"Hu, Bin and Zhao, Chenyang and Zhang, Pu and Zhou, Zihao and Yang, Yuanhang and Xu, Zenglin and Liu, Bin",['model-construction']
Evaluating Cognitive Maps and Planning in Large Language Models with CogEval,"Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/dc9d5dcf3e86b83e137bad367227c8ca-Abstract-Conference.html,"Momennejad, Ida and Hasanbeig, Hosein and Vieira, Felipe and Sharma, Hiteshi and Ness, Robert Osazuwa and Jojic, Nebojsa and Palangi, Hamid and Larson, Jonathan",['brain-inspired-planning']
Evaluation of Pretrained Large Language Models in Embodied Planning Tasks,"Modern pretrained large language models (LLMs) are increasingly being used in zero-shot or few-shot learning modes. Recent years have seen increased interest in applying such models to embodied artificial intelligence and robotics tasks. When given in a natural language, the agent needs to build a plan based on this prompt. The best solutions use LLMs through APIs or models that are not publicly available, making it difficult to reproduce the results. In this paper, we use publicly available LLMs to build a plan for an embodied agent and evaluate them in three modes of operation: 1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3) the step-by-step autoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples of different tasks. Through extensive experiments, we have shown that the subtask evaluation mode, in most cases, outperforms others with a task-specific prompt, whereas the step-by-step autoregressive plan generation posts better performance in the mixed prompt setting.

",https://link.springer.com/chapter/10.1007/978-3-031-33469-6_23,"Sarkisyan, Christina and Korchemnyi, Alexandr and Kovalev, Alexey K and Panov, Aleksandr I",['plan-generation']
Exploiting Language Models as a Source of Knowledge for Cognitive Agents,"Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.",https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690,"Kirk, James R and Wray, Robert E and Laird, John E",['model-construction']
Exploring the Limitations of using Large Language Models to Fix Planning Tasks,"Large language models (LLMs) have revolutionized natural
language processing (NLP), enabling human-like text generation, question answering, and translation. Despite claims
of emergent reasoning capabilities, it has been demonstrated
their lack of planning skills in tasks such as plan generation,
plan reuse or replanning. In this work, we present ongoing
efforts on exploring the limitations of LLMs in another task
requiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks.",https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf,"Gragera, Alba and Pozanco, Alberto",['model-construction']
Fast and Slow Planning,"The concept of Artificial Intelligence has gained a lot of attention over the
last decade. In particular, AI-based tools have been employed in several
scenarios and are, by now, pervading our everyday life. Nonetheless, most of
these systems lack many capabilities that we would naturally consider to be
included in a notion of ""intelligence"". In this work, we present an
architecture that, inspired by the cognitive theory known as Thinking Fast and
Slow by D. Kahneman, is tasked with solving planning problems in different
settings, specifically: classical and multi-agent epistemic. The system
proposed is an instance of a more general AI paradigm, referred to as SOFAI
(for Slow and Fast AI). SOFAI exploits multiple solving approaches, with
different capabilities that characterize them as either fast or slow, and a
metacognitive module to regulate them. This combination of components, which
roughly reflects the human reasoning process according to D. Kahneman, allowed
us to enhance the reasoning process that, in this case, is concerned with
planning in two different settings. The behavior of this system is then
compared to state-of-the-art solvers, showing that the newly introduced system
presents better results in terms of generality, solving a wider set of problems
with an acceptable trade-off between solving times and solution accuracy.",https://arxiv.org/abs/2303.04283,"Fabiano, Francesco and Pallagani, Vishal and Ganapini, Marianna Bergamaschi and Horesh, Lior and Loreggia, Andrea and Murugesan, Keerthiram and Rossi, Francesca and Srivastava, Biplav",['plan-generation']
From Cooking Recipes to Robot Task Trees--Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,"Task planning for robotic cooking involves generating a sequence of actions
for a robot to prepare a meal successfully. This paper introduces a novel task
tree generation pipeline producing correct planning and efficient execution for
cooking tasks. Our method first uses a large language model (LLM) to retrieve
recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a
task tree, capturing sequential and parallel dependencies among subtasks. The
pipeline then mitigates the uncertainty and unreliable features of LLM outputs
using task tree retrieval. We combine multiple LLM task tree outputs into a
graph and perform a task tree retrieval to avoid questionable nodes and
high-cost nodes to improve planning correctness and improve execution
efficiency. Our evaluation results show its superior performance compared to
previous works in task planning accuracy and efficiency.",https://arxiv.org/abs/2309.09181,"Sakib, Md Sadman and Sun, Yu",['language-translation']
From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought,"How does language inform our downstream thinking? In particular, how do
humans make meaning from language--and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose rational meaning construction, a computational framework
for language-informed thinking that combines neural language models with
probabilistic models for rational inference. We frame linguistic meaning as a
context-sensitive mapping from natural language into a probabilistic language
of thought (PLoT)--a general-purpose symbolic substrate for generative world
modeling. Our architecture integrates two computational tools that have not
previously come together: we model thinking with probabilistic programs, an
expressive representation for commonsense reasoning; and we model meaning
construction with large language models (LLMs), which support broad-coverage
translation from natural language utterances to code expressions in a
probabilistic programming language. We illustrate our framework through
examples covering four core domains from cognitive science: probabilistic
reasoning, logical and relational reasoning, visual and physical reasoning, and
social reasoning. In each, we show that LLMs can generate context-sensitive
translations that capture pragmatically-appropriate linguistic meanings, while
Bayesian inference with the generated programs supports coherent and robust
commonsense reasoning. We extend our framework to integrate
cognitively-motivated symbolic modules (physics simulators, graphics engines,
and planning algorithms) to provide a unified commonsense thinking interface
from language. Finally, we explore how language can drive the construction of
world models themselves. We hope this work will provide a roadmap towards
cognitive models and AI systems that synthesize the insights of both modern and
classical computational perspectives.",https://arxiv.org/abs/2306.12672,"Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B",['language-translation']
GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning,"A robot in a human-centric environment needs to account for the human's
intent and future motion in its task and motion planning to ensure safe and
effective operation. This requires symbolic reasoning about probable future
actions and the ability to tie these actions to specific locations in the
physical environment. While one can train behavioral models capable of
predicting human motion from past activities, this approach requires large
amounts of data to achieve acceptable long-horizon predictions. More
importantly, the resulting models are constrained to specific data formats and
modalities. Moreover, connecting predictions from such models to the
environment at hand to ensure the applicability of these predictions is an
unsolved problem. We present a system that utilizes a Large Language Model
(LLM) to infer a human's next actions from a range of modalities without
fine-tuning. A novel aspect of our system that is critical to robotics
applications is that it links the predicted actions to specific locations in a
semantic map of the environment. Our method leverages the fact that LLMs,
trained on a vast corpus of text describing typical human behaviors, encode
substantial world knowledge, including probable sequences of human actions and
activities. We demonstrate how these localized activity predictions can be
incorporated in a human-aware task planner for an assistive robot to reduce the
occurrences of undesirable human-robot interactions by 29.2% on average.",https://arxiv.org/abs/2310.20034,"Graule, Moritz A and Isler, Volkan",['interactive-planning']
Generalized Planning in PDDL Domains with Pretrained Large Language Models,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",https://arxiv.org/abs/2305.11014,"Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B and Kaelbling, Leslie Pack and Katz, Michael",['plan-generation']
Generating executable action plans with environmentally-aware language models,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable,
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that agents are better able to execute. Our approach involves integrating
environmental objects and object relations as additional inputs into LLM action
plan generation to provide the system with an awareness of its surroundings,
resulting in plans where each generated action is mapped to objects present in
the scene. We also design a novel scoring function that, along with generating
the action steps and associating them with objects, helps the system
disambiguate among object instances and take into account their states. We
evaluated our approach using the VirtualHome simulator and the ActivityPrograms
knowledge base and found that action plans generated from our system had a 310%
improvement in executability and a 147% improvement in correctness over prior
work. The complete code and a demo of our method is publicly available at
https://github.com/hri-ironlab/scene_aware_language_planner.",https://arxiv.org/abs/2210.04964,"Gramopadhye, Maitrey and Szafir, Daniel",['plan-generation']
Gentopia: A collaborative platform for tool-augmented llms,"Augmented Language Models (ALMs) empower large language models with the
ability to use tools, transforming them into intelligent agents for real-world
interactions. However, most existing frameworks for ALMs, to varying degrees,
are deficient in the following critical features: flexible customization,
collaborative democratization, and holistic evaluation. We present gentopia, an
ALM framework enabling flexible customization of agents through simple
configurations, seamlessly integrating various language models, task formats,
prompting modules, and plugins into a unified paradigm. Furthermore, we
establish gentpool, a public platform enabling the registration and sharing of
user-customized agents. Agents registered in gentpool are composable such that
they can be assembled together for agent collaboration, advancing the
democratization of artificial intelligence. To ensure high-quality agents,
gentbench, an integral component of gentpool, is designed to thoroughly
evaluate user-customized agents across diverse aspects such as safety,
robustness, efficiency, etc. We release gentopia on Github and will
continuously move forward.",https://arxiv.org/abs/2308.04030,"Xu, Binfeng and Liu, Xukun and Shen, Hua and Han, Zeyu and Li, Yuhan and Yue, Murong and Peng, Zhiyuan and Liu, Yuchen and Yao, Ziyu and Xu, Dongkuan",['tool-integration']
Graph of thoughts: Solving elaborate problems with large language models,"We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information (""LLM thoughts"") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by >31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.",https://arxiv.org/abs/2308.09687,"Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others",['plan-generation']
Grounded decoding: Guiding text generation with grounded models for robot control,"Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project's website can be found at
grounded-decoding.github.io.",https://arxiv.org/abs/2303.00855,"Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and others",['plan-generation']
Grounding large language models in interactive environments with online reinforcement learning,"Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.",https://arxiv.org/abs/2302.02662,"Carta, Thomas and Romac, Clement and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves",['interactive-planning']
Guiding language model reasoning with planning tokens,"Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
(CoT) reasoning. However, most of the existing approaches to enhance this
ability rely heavily on data-driven methods, while neglecting the structural
aspects of the model's reasoning capacity. To encourage a more structural
generation of CoT steps, we propose a hierarchical generation scheme: we let
the LM generate a planning token at the start of each reasoning step,
intuitively serving as a high-level plan of the current step, and add their
embeddings to the model parameters. Our approach requires a negligible increase
in trainable parameters (0.001%) and can be applied through either full
fine-tuning or a more parameter-efficient scheme. We demonstrate our method's
effectiveness by applying it to three different LLMs, showing notable accuracy
improvements across three math word problem datasets and one multihop QA
dataset with respect to standard fine-tuning baselines.",https://arxiv.org/abs/2310.05707,"Wang, Xinyi and Caccia, Lucas and Ostapenko, Oleksiy and Yuan, Xingdi and Sordoni, Alessandro",['plan-generation']
Housekeep: Tidying virtual households using commonsense reasoning,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",https://arxiv.org/abs/2205.10712,"Kant, Yash and Ramachandran, Arun and Yenamandra, Sriram and Gilitschenski, Igor and Batra, Dhruv and Szot, Andrew and Agrawal, Harsh",['plan-generation']
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/77c33e6a367922d003ff102ffb92b658-Abstract-Conference.html,"Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting",['tool-integration']
Human-Assisted Continual Robot Learning with Foundation Models,"Large Language Models (LLMs) have been shown to act like planners that can
decompose high-level instructions into a sequence of executable instructions.
However, current LLM-based planners are only able to operate with a fixed set
of skills. We overcome this critical limitation and present a method for using
LLM-based planners to query new skills and teach robots these skills in a data
and time-efficient manner for rigid object manipulation. Our system can re-use
newly acquired skills for future tasks, demonstrating the potential of open
world and lifelong learning. We evaluate the proposed framework on multiple
tasks in simulation and the real world. Videos are available at:
https://sites.google.com/mit.edu/halp-robot-learning.",https://arxiv.org/abs/2309.14321,"Parakh, Meenal and Fong, Alisha and Simeonov, Anthony and Gupta, Abhishek and Chen, Tao and Agrawal, Pulkit",['language-translation']
Human-Centered Planning,"LLMs have recently made impressive inroads on tasks whose output is
structured, such as coding, robotic planning and querying databases. The vision
of creating AI-powered personal assistants also involves creating structured
outputs, such as a plan for one's day, or for an overseas trip. Here, since the
plan is executed by a human, the output doesn't have to satisfy strict
syntactic constraints. A useful assistant should also be able to incorporate
vague constraints specified by the user in natural language. This makes LLMs an
attractive option for planning.
  We consider the problem of planning one's day. We develop an LLM-based
planner (LLMPlan) extended with the ability to self-reflect on its output and a
symbolic planner (SymPlan) with the ability to translate text constraints into
a symbolic representation. Despite no formal specification of constraints, we
find that LLMPlan performs explicit constraint satisfaction akin to the
traditional symbolic planners on average (2% performance difference), while
retaining the reasoning of implicit requirements. Consequently, LLM-based
planners outperform their symbolic counterparts in user satisfaction (70.5% vs.
40.4%) during interactive evaluation with 40 users.",https://arxiv.org/abs/2311.04403,"Li, Yuliang and Kamra, Nitin and Desai, Ruta and Halevy, Alon",['plan-generation']
ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,"Motivated by the substantial achievements observed in Large Language Models
(LLMs) in the field of natural language processing, recent research has
commenced investigations into the application of LLMs for complex, long-horizon
sequential task planning challenges in robotics. LLMs are advantageous in
offering the potential to enhance the generalizability as task-agnostic
planners and facilitate flexible interaction between human instructors and
planning systems. However, task plans generated by LLMs often lack feasibility
and correctness. To address this challenge, we introduce ISR-LLM, a novel
framework that improves LLM-based planning through an iterative self-refinement
process. The framework operates through three sequential steps: preprocessing,
planning, and iterative self-refinement. During preprocessing, an LLM
translator is employed to convert natural language input into a Planning Domain
Definition Language (PDDL) formulation. In the planning phase, an LLM planner
formulates an initial plan, which is then assessed and refined in the iterative
self-refinement step by using a validator. We examine the performance of
ISR-LLM across three distinct planning domains. The results show that ISR-LLM
is able to achieve markedly higher success rates in task accomplishments
compared to state-of-the-art LLM-based planners. Moreover, it also preserves
the broad applicability and generalizability of working with natural language
instructions.",https://arxiv.org/abs/2308.13724,"Zhou, Zhehua and Song, Jiayang and Yao, Kunpeng and Shu, Zhan and Ma, Lei",['interactive-planning']
Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans,"Task-oriented dialogue is difficult in part because it involves understanding
user intent, collecting information from the user, executing API calls, and
generating helpful and fluent responses. However, for complex tasks one must
also correctly do all of these things over multiple steps, and in a specific
order. While large pre-trained language models can be fine-tuned end-to-end to
create multi-step task-oriented dialogue agents that generate fluent text, our
experiments confirm that this approach alone cannot reliably perform new
multi-step tasks that are unseen during training. To address these limitations,
we augment the dialogue contexts given to \textmd{text2text} transformers with
known \textit{valid workflow names} and \textit{action plans}. Action plans
consist of sequences of actions required to accomplish a task, and are encoded
as simple sequences of keywords (e.g. verify-identity, pull-up-account,
reset-password, etc.). We perform extensive experiments on the Action-Based
Conversations Dataset (ABCD) with T5-small, base and large models, and show
that such models: a) are able to more readily generalize to unseen workflows by
following the provided plan, and b) are able to generalize to executing unseen
actions if they are provided in the plan. In contrast, models are unable to
fully accomplish new multi-step tasks when they are not provided action plan
information, even when given new valid workflow names.",https://arxiv.org/abs/2306.01729,"Raimondo, Stefania and Pal, Christopher and Liu, Xiaotian and Vazquez, David and Palacios, Hector",['heuristics-optimization']
Inner monologue: Embodied reasoning through planning with language models,"Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.",https://arxiv.org/abs/2207.05608,"Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",['interactive-planning']
Integrating Common Sense and Planning with Large Language Models for Room Tidying,"Do you want a personal housekeeper robot? This
project seeks to endow robots with the capability of tidying
up messy rooms with brief natural language descriptions of the
environment. We address three key challenges: (i) incomplete map
information in the description, (ii) commonsense understanding
of object locations, and (iii) long-horizon planning and acting
to achieve the objective. To tackle these challenges, we leverage
Large Language Models (LLMs) understanding of typical layouts
of human-living environments and object locations, as well as
programming and control skills for action execution. Specifically,
we prompt ChatGPT to reconstruct complete map representations
from partial descriptions, then generate a high-level action plan
in the form of Python functions, and finally refine the plans
with atomic actions executable by the robot. We show that our
framework enables effective room tidying with limited human
instruction guidance. On simulation and real-world maps, it is
able to find a place missing out from human description within
three interactions with humans. In the simulation environment,
it is capable of putting more than 80% household objects in
their desired place. This study provides preliminary evidence that
LLMs have common sense about the spatial layout of humanliving environments and object arrangements, and this work
connects this knowledge to robotics tasks.",https://openreview.net/pdf?id=vuSI9mhDaBZ,"Wu, Zhanxin and Ai, Bo and Hsu, David",['interactive-planning']
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,"3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .",https://arxiv.org/abs/2309.12311,"Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce",['language-translation']
Language models as zero-shot planners: Extracting actionable knowledge for embodied agents,"Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. ""make
breakfast""), to a chosen set of actionable steps (e.g. ""open fridge""). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner",https://arxiv.org/abs/2201.07207,"Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor",['plan-generation']
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change),"Generating plans of action, and reasoning about change have long been
considered a core competence of intelligent agents. It is thus no surprise that
evaluating the planning and reasoning capabilities of large language models
(LLMs) has become a hot topic of research. Most claims about LLM planning
capabilities are however based on common sense tasks-where it becomes hard to
tell whether LLMs are planning or merely retrieving from their vast world
knowledge. There is a strong need for systematic and extensible planning
benchmarks with sufficient diversity to evaluate whether LLMs have innate
planning capabilities. Motivated by this, we propose PlanBench, an extensible
benchmark suite based on the kinds of domains used in the automated planning
community, especially in the International Planning Competition, to test the
capabilities of LLMs in planning or reasoning about actions and change.
PlanBench provides sufficient diversity in both the task domains and the
specific planning capabilities. Our studies also show that on many critical
capabilities-including plan generation-LLM performance falls quite short, even
with the SOTA models. PlanBench can thus function as a useful marker of
progress of LLMs in planning and reasoning.",https://arxiv.org/abs/2206.10498,"Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao",['plan-generation']
Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners,"The emergent few-shot reasoning capabilities of Large Language Models (LLMs)
have excited the natural language and machine learning community over recent
years. Despite of numerous successful applications, the underlying mechanism of
such in-context capabilities still remains unclear. In this work, we
hypothesize that the learned \textit{semantics} of language tokens do the most
heavy lifting during the reasoning process. Different from human's symbolic
reasoning process, the semantic representations of LLMs could create strong
connections among tokens, thus composing a superficial logical chain. To test
our hypothesis, we decouple semantics from the language reasoning process and
evaluate three kinds of reasoning abilities, i.e., deduction, induction and
abduction. Our findings reveal that semantics play a vital role in LLMs'
in-context reasoning -- LLMs perform significantly better when semantics are
consistent with commonsense but struggle to solve symbolic or
counter-commonsense reasoning tasks by leveraging in-context new knowledge. The
surprising observations question whether modern LLMs have mastered the
inductive, deductive and abductive reasoning abilities as in human
intelligence, and motivate research on unveiling the magic existing within the
black-box LLMs. On the whole, our analysis provides a novel perspective on the
role of semantics in developing and evaluating language models' reasoning
abilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}.",https://arxiv.org/abs/2305.14825,"Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan",['plan-generation']
Large Language Models as Commonsense Knowledge for Large-Scale Task Planning,"Large-scale task planning is a major challenge. Recent work exploits large language
models (LLMs) directly as a policy and shows surprisingly interesting results. This
paper shows that LLMs provide a commonsense model of the world in addition
to a policy that acts on it. The world model and the policy can be combined in
a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task
planning. In our new LLM-MCTS algorithm, the LLM-induced world model
provides a commonsense prior belief for MCTS to achieve effective reasoning; the
LLM-induced policy acts as a heuristic to guide the search, vastly improving search
efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone
and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex,
novel tasks. Further experiments and analyses on multiple tasksmultiplication,
travel planning, object rearrangementsuggest minimum description length (MDL)
as a general guiding principle: if the description length of the world model is
substantially smaller than that of the policy, using LLM as a world model for
model-based planning is likely better than using LLM solely as a policy.",https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf,"Zhao, Zirui and Lee, Wee Sun and Hsu, David",['model-construction']
Large language models as zero-shot human models for human-robot interaction,"Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.",https://arxiv.org/abs/2303.03548,"Zhang, Bowen and Soh, Harold",['model-construction']
Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models,"Automaton-based representations of task knowledge play an important role in
control and planning for sequential decision-making problems. However,
obtaining the high-level task knowledge required to build such automata is
often difficult. Meanwhile, large-scale generative language models (GLMs) can
automatically generate relevant task knowledge. However, the textual outputs
from GLMs cannot be formally verified or used for sequential decision-making.
We propose a novel algorithm named GLM2FSA, which constructs a finite state
automaton (FSA) encoding high-level task knowledge from a brief
natural-language description of the task goal. GLM2FSA first sends queries to a
GLM to extract task knowledge in textual form, and then it builds an FSA to
represent this text-based knowledge. The proposed algorithm thus fills the gap
between natural-language task descriptions and automaton-based representations,
and the constructed FSA can be formally verified against user-defined
specifications. We accordingly propose a method to iteratively refine the
queries to the GLM based on the outcomes, e.g., counter-examples, from
verification. We demonstrate GLM2FSA's ability to build and refine
automaton-based representations of everyday tasks (e.g., crossing a road), and
also of tasks that require highly-specialized knowledge (e.g., executing secure
multi-party computation).",https://arxiv.org/abs/2212.01944,"Yang, Yunhao and Gaglione, Jean-Raphael and Topcu, Ufuk",['language-translation']
Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,"There have been wide spread claims in the literature about the emergent
reasoning capabilities of Pretrained Large Language Models. However, recent
studies, have found that their ability to plan remains questionable. Through
our experiments using GPT-2, we empirically demonstrate that the performance of
a finetuned baseline remains poor because it violates pre-conditions of actions
in the plans that it generates. To improve the planning capabilities of a
finetuned LLM, we train a verifier, which can classify actions as being valid
or invalid in a particular state. By randomly sampling actions from the same
dataset, we generate examples of invalid actions which are then used to train a
verifier which can check for action applicability. In the presence of diverse
sampling from a generator and a verifier which can prune invalid trajectories,
we show significant gains in the success rate on the Blocksworld domain.
Additionally, we show that finetuning the GPT-2 generator itself to create the
verifier generalizes better than finetuning the base GPT-2. Lastly, we
investigate the role of the sampling temperature which can be used to control
the exploration-exploitation tradeoff.",https://arxiv.org/abs/2305.17077,"Arora, Daman and Kambhampati, Subbarao",['plan-generation']
Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,"Long-horizon task planning is essential for the development of intelligent
assistive and service robots. In this work, we investigate the applicability of
a smaller class of large language models (LLMs), specifically GPT-2, in robotic
task planning by learning to decompose tasks into subgoal specifications for a
planner to execute sequentially. Our method grounds the input of the LLM on the
domain that is represented as a scene graph, enabling it to translate human
requests into executable robot plans, thereby learning to reason over
long-horizon tasks, as encountered in the ALFRED benchmark. We compare our
approach with classical planning and baseline methods to examine the
applicability and generalizability of LLM-based planners. Our findings suggest
that the knowledge stored in an LLM can be effectively grounded to perform
long-horizon task planning, demonstrating the promising potential for the
future application of neuro-symbolic planning methods in robotics.",https://arxiv.org/abs/2305.07716,"Chalvatzaki, Georgia and Younes, Ali and Nandha, Daljeet and Le, An Thai and Ribeiro, Leonardo FR and Gurevych, Iryna",['language-translation']
Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning,"Multi-object rearrangement is a crucial skill for
service robots, and commonsense reasoning is frequently needed
in this process. However, achieving commonsense arrangements
requires knowledge about objects, which is hard to transfer to
robots. Large language models (LLMs) are one potential source of
this knowledge, but they do not naively capture information about
plausible physical arrangements of the world. We propose LLMGROP, which uses prompting to extract commonsense knowledge
about semantically valid object configurations from an LLM
and instantiates them with a task and motion planner in order
to generalize to varying scene geometry. LLM-GROP allows
us to go from natural-language commands to human-aligned
object rearrangement in varied environments. Based on human
evaluations, our approach achieves the highest rating while
outperforming competitive baselines in terms of success rate
while maintaining comparable cumulative action costs. Finally,
we demonstrate a practical implementation of LLM-GROP on
a mobile manipulator in real-world scenarios. Supplementary
materials are available at: https://sites.google.com/view/llm-grop",https://openreview.net/pdf?id=LMiTmpZgSZ,"Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi",['language-translation']
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",https://arxiv.org/abs/2305.14909,"Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao",['language-translation']
Llm+ p: Empowering large language models with optimal planning proficiency,"Large language models (LLMs) have demonstrated remarkable zero-shot
generalization abilities: state-of-the-art chatbots can provide plausible
answers to many common questions that arise in daily life. However, so far,
LLMs cannot reliably solve long-horizon planning problems. By contrast,
classical planners, once a problem is given in a formatted way, can use
efficient search algorithms to quickly identify correct, or even optimal,
plans. In an effort to get the best of both worlds, this paper introduces
LLM+P, the first framework that incorporates the strengths of classical
planners into LLMs. LLM+P takes in a natural language description of a planning
problem, then returns a correct (or optimal) plan for solving that problem in
natural language. LLM+P does so by first converting the language description
into a file written in the planning domain definition language (PDDL), then
leveraging classical planners to quickly find a solution, and then translating
the found solution back into natural language. Along with LLM+P, we define a
diverse set of different benchmark problems taken from common planning
scenarios. Via a comprehensive set of experiments on these benchmark problems,
we find that LLM+P is able to provide optimal solutions for most problems,
while LLMs fail to provide even feasible plans for most problems.\footnote{The
code and results are publicly available at
https://github.com/Cranial-XIX/llm-pddl.git.",https://arxiv.org/abs/2304.11477,"Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter",['language-translation']
Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games,"There is an growing interest in using Large Language Models (LLMs) in
multi-agent systems to tackle interactive real-world tasks that require
effective collaboration and assessing complex situations. Yet, we still have a
limited understanding of LLMs' communication and decision-making abilities in
multi-agent setups. The fundamental task of negotiation spans many key features
of communication, such as cooperation, competition, and manipulation
potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We
create a testbed of complex multi-agent, multi-issue, and semantically rich
negotiation games. To reach an agreement, agents must have strong arithmetic,
inference, exploration, and planning capabilities while integrating them in a
dynamic and multi-turn setup. We propose multiple metrics to rigorously
quantify agents' performance and alignment with the assigned role. We provide
procedures to create new games and increase games' difficulty to have an
evolving benchmark. Importantly, we evaluate critical safety aspects such as
the interaction dynamics between agents influenced by greedy and adversarial
players. Our benchmark is highly challenging; GPT-3.5 and small models mostly
fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.",https://arxiv.org/abs/2309.17234,"Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\""o}nherr, Lea and Fritz, Mario",['multiagent-planning']
Llm-planner: Few-shot grounded planning for embodied agents with large language models,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner",https://arxiv.org/abs/2212.04088,"Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu",['plan-generation']
Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning,"Large Language Models (LLMs) have shown human-like reasoning abilities but
still struggle with complex logical problems. This paper introduces a novel
framework, Logic-LM, which integrates LLMs with symbolic solvers to improve
logical problem-solving. Our method first utilizes LLMs to translate a natural
language problem into a symbolic formulation. Afterward, a deterministic
symbolic solver performs inference on the formulated problem. We also introduce
a self-refinement module, which utilizes the symbolic solver's error messages
to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on
five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,
LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant
performance boost of 39.2% over using LLM alone with standard prompting and
18.4% over LLM with chain-of-thought prompting. Our findings suggest that
Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for
faithful logical reasoning. Code and data are publicly available at
https://github.com/teacherpeterpan/Logic-LLM.",https://arxiv.org/abs/2305.12295,"Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang",['language-translation']
Multimodal Procedural Planning via Dual Text-Image Prompting,"Embodied agents have achieved prominent performance in following human
instructions to complete tasks. However, the potential of providing
instructions informed by texts and images to assist humans in completing tasks
remains underexplored. To uncover this capability, we present the multimodal
procedural planning (MPP) task, in which models are given a high-level goal and
generate plans of paired text-image steps, providing more complementary and
informative guidance than unimodal plans. The key challenges of MPP are to
ensure the informativeness, temporal coherence,and accuracy of plans across
modalities. To tackle this, we propose Text-Image Prompting (TIP), a
dual-modality prompting method that jointly leverages zero-shot reasoning
ability in large language models (LLMs) and compelling text-to-image generation
ability from diffusion-based models. TIP improves the interaction in the dual
modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs
to guide the textual-grounded image plan generation and leveraging the
descriptions of image plans to ground the textual plan reversely. To address
the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed
for MPP. Our results show compelling human preferences and automatic scores
against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms
of informativeness, temporal coherence, and plan accuracy. Our code and data:
https://github.com/YujieLu10/MPP.",https://arxiv.org/abs/2305.01795,"Lu, Yujie and Lu, Pan and Chen, Zhiyu and Zhu, Wanrong and Wang, Xin Eric and Wang, William Yang",['plan-generation']
Navigation with large language models: Semantic guesswork as a heuristic for planning,"Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics  e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the semantic guesswork produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
",https://proceedings.mlr.press/v229/shah23c.html,"Shah, Dhruv and Equi, Michael and Osinski, Blazej and Xia, Fei and Ichter, Brian and Levine, Sergey",['heuristics-optimization']
Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving,"Generative large language models (LLMs) with instruct training such as GPT-4
can follow human-provided instruction prompts and generate human-like responses
to these prompts. Apart from natural language responses, they have also been
found to be effective at generating formal artifacts such as code, plans, and
logical specifications from natural language prompts. Despite their remarkably
improved accuracy, these models are still known to produce factually incorrect
or contextually inappropriate results despite their syntactic coherence - a
phenomenon often referred to as hallucination. This limitation makes it
difficult to use these models to synthesize formal artifacts that are used in
safety-critical applications. Unlike tasks such as text summarization and
question-answering, bugs in code, plan, and other formal artifacts produced by
LLMs can be catastrophic. We posit that we can use the satisfiability modulo
theory (SMT) solvers as deductive reasoning engines to analyze the generated
solutions from the LLMs, produce counterexamples when the solutions are
incorrect, and provide that feedback to the LLMs exploiting the dialog
capability of instruct-trained LLMs. This interaction between inductive LLMs
and deductive SMT solvers can iteratively steer the LLM to generate the correct
response. In our experiments, we use planning over the domain of blocks as our
synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,
Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our
method allows the user to communicate the planning problem in natural language;
even the formulation of queries to SMT solvers is automatically generated from
natural language. Thus, the proposed technique can enable non-expert users to
describe their problems in natural language, and the combination of LLMs and
SMT solvers can produce provably correct solutions.",https://arxiv.org/abs/2309.16436,"Jha, Sumit Kumar and Jha, Susmit and Lincoln, Patrick and Bastian, Nathaniel D and Velasquez, Alvaro and Ewetz, Rickard and Neema, Sandeep",['interactive-planning']
Neuro-Symbolic AI Approaches to Enhance Deep Neural Networks with Logical Reasoning and Knowledge Integration,"One of the challenges in Artificial Intelligence (AI) is to integrate fast, automatic, and intuitive System-1 thinking with slow, deliberate, and logical System-2 thinking. While deep
learning approaches excel at perception tasks for System-1, their reasoning capabilities for
System-2 are limited. Besides, deep learning approaches are usually data-hungry, hard to
make use of explicit knowledge, and struggling with interpretability and justification. This
dissertation presents three neuro-symbolic AI approaches that integrate neural networks
(NNs) with symbolic AI methods to address these issues.
The first approach presented in this dissertation is NeurASP, which combines NNs with
Answer Set Programming (ASP), a logic programming formalism. NeurASP provides an
effective way to integrate sub-symbolic and symbolic computation by treating NN outputs
as probability distributions over atomic facts in ASP. The explicit knowledge encoded in
ASP corrects mistakes in NN outputs and allows for better training with less data.
To avoid NeurASPs bottleneck in symbolic computation, this dissertation presents a
Constraint Loss via Straight-Through Estimators (CL-STE). CL-STE provides a systematic
way to compile discrete logical constraints into a loss function over discretized NN outputs
and scales significantly better than state-of-the-art neuro-symbolic methods. This dissertation also presents a finding when CL-STE was applied to Transformers. Transformers can
be extended with recurrence to enhance its power for multi-step reasoning. Such Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems
while successfully addressing the symbol grounding problem.
Lastly, this dissertation addresses the limitation of pre-trained Large Language Models
(LLMs) on multi-step logical reasoning problems with a dual-process neuro-symbolic reasoning system called LLM+ASP, where an LLM (e.g., GPT-3) serves as a highly effective
few-shot semantic parser that turns natural language sentences into a logical form that can
be used as input to ASP. LLM+ASP achieves state-of-the-art performance on several textual reasoning benchmarks and can handle robot planning tasks that an LLM alone fails to
solve.",https://www.proquest.com/docview/2851056844,"Yang, Zhun",['language-translation']
Neuro-symbolic causal language planning with commonsense prompting,"Procedural planning aims to implement complex high-level goals by
decomposition into sequential simpler low-level steps. Although procedural
planning is a basic skill set for humans in daily life, it remains a challenge
for large language models (LLMs) that lack a deep understanding of the
cause-effect relations in procedures. Previous methods require manual exemplars
to acquire procedural planning knowledge from LLMs in the zero-shot setting.
However, such elicited pre-trained knowledge in LLMs induces spurious
correlations between goals and steps, which impair the model generalization to
unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural
PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with
commonsense-infused prompting. To mitigate spurious goal-step correlations, we
use symbolic program executors on the latent procedural representations to
formalize prompts from commonsense knowledge bases as a causal intervention
toward the Structural Causal Model. Both automatic and human evaluations on
WikiHow and RobotHow show the superiority of PLAN on procedural planning
without further training or manual exemplars.",https://arxiv.org/abs/2206.02928,"Lu, Yujie and Feng, Weixi and Zhu, Wanrong and Xu, Wenda and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang",['plan-generation']
OceanChat: Piloting Autonomous Underwater Vehicles in Natural Language,"In the trending research of fusing Large Language Models (LLMs) and robotics,
we aim to pave the way for innovative development of AI systems that can enable
Autonomous Underwater Vehicles (AUVs) to seamlessly interact with humans in an
intuitive manner. We propose OceanChat, a system that leverages a closed-loop
LLM-guided task and motion planning framework to tackle AUV missions in the
wild. LLMs translate an abstract human command into a high-level goal, while a
task planner further grounds the goal into a task sequence with logical
constraints. To assist the AUV with understanding the task sequence, we utilize
a motion planner to incorporate real-time Lagrangian data streams received by
the AUV, thus mapping the task sequence into an executable motion plan.
Considering the highly dynamic and partially known nature of the underwater
environment, an event-triggered replanning scheme is developed to enhance the
system's robustness towards uncertainty. We also build a simulation platform
HoloEco that generates photo-realistic simulation for a wide range of AUV
applications. Experimental evaluation verifies that the proposed system can
achieve improved performance in terms of both success rate and computation
time. Project website: \url{https://sites.google.com/view/oceanchat}",https://arxiv.org/abs/2309.16052,"Yang, Ruochu and Hou, Mengxue and Wang, Junkai and Zhang, Fumin",['language-translation']
"On the Planning, Search, and Memorization Capabilities of Large Language Models","The rapid advancement of large language models, such as the Generative
Pre-trained Transformer (GPT) series, has had significant implications across
various disciplines. In this study, we investigate the potential of the
state-of-the-art large language model (GPT-4) for planning tasks. We explore
its effectiveness in multiple planning subfields, highlighting both its
strengths and limitations. Through a comprehensive examination, we identify
areas where large language models excel in solving planning problems and reveal
the constraints that limit their applicability. Our empirical analysis focuses
on GPT-4's performance in planning domain extraction, graph search path
planning, and adversarial planning. We then propose a way of fine-tuning a
domain-specific large language model to improve its Chain of Thought (CoT)
capabilities for the above-mentioned tasks. The results provide valuable
insights into the potential applications of large language models in the
planning domain and pave the way for future research to overcome their
limitations and expand their capabilities.",https://arxiv.org/abs/2309.01868,"Yang, Yunhao and Tomar, Anshul",['plan-generation']
On the planning abilities of large language models (a critical investigation with a proposed benchmark),"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) how good LLMs are by themselves in
generating and validating simple plans in commonsense planning tasks (of the
type that humans are generally quite good at) and (2) how good LLMs are in
being a source of heuristic guidance for other agents--either AI planners or
human planners--in their planning tasks. To investigate these questions in a
systematic rather than anecdotal manner, we start by developing a benchmark
suite based on the kinds of domains employed in the International Planning
Competition. On this benchmark, we evaluate LLMs in three modes: autonomous,
heuristic and human-in-the-loop. Our results show that LLM's ability to
autonomously generate executable plans is quite meager, averaging only about 3%
success rate. The heuristic and human-in-the-loop modes show slightly more
promise. In addition to these results, we also make our benchmark and
evaluation tools available to support investigations by research community.",https://arxiv.org/abs/2302.06706,"Valmeekam, Karthik and Sreedharan, Sarath and Marquez, Matthew and Olmo, Alberto and Kambhampati, Subbarao",['plan-generation']
Open-vocabulary queryable scene representations for real world planning,"Large language models (LLMs) have unlocked new capabilities of task planning
from human instructions. However, prior attempts to apply LLMs to real-world
robotic tasks are limited by the lack of grounding in the surrounding scene. In
this paper, we develop NLMap, an open-vocabulary and queryable scene
representation to address this problem. NLMap serves as a framework to gather
and integrate contextual information into LLM planners, allowing them to see
and query available objects in the scene before generating a
context-conditioned plan. NLMap first establishes a natural language queryable
scene representation with Visual Language models (VLMs). An LLM based object
proposal module parses instructions and proposes involved objects to query the
scene representation for object availability and location. An LLM planner then
plans with such information about the scene. NLMap allows robots to operate
without a fixed list of objects nor executable options, enabling real robot
operation unachievable by previous methods. Project website:
https://nlmap-saycan.github.io",https://arxiv.org/abs/2209.09874,"Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S and Stone, Austin and Kappler, Daniel",['language-translation']
Openagi: When llm meets domain experts,"Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html,"Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng",['tool-integration']
Optimal Scene Graph Planning with Large Language Model Guidance,"Recent advances in metric, semantic, and topological mapping have equipped
autonomous robots with semantic concept grounding capabilities to interpret
natural language tasks. This work aims to leverage these new capabilities with
an efficient task planning algorithm for hierarchical metric-semantic models.
We consider a scene graph representation of the environment and utilize a large
language model (LLM) to convert a natural language task into a linear temporal
logic (LTL) automaton. Our main contribution is to enable optimal hierarchical
LTL planning with LLM guidance over scene graphs. To achieve efficiency, we
construct a hierarchical planning domain that captures the attributes and
connectivity of the scene graph and the task automaton, and provide semantic
guidance via an LLM heuristic function. To guarantee optimality, we design an
LTL heuristic function that is provably consistent and supplements the
potentially inadmissible LLM guidance in multi-heuristic planning. We
demonstrate efficient planning of complex natural language tasks in scene
graphs of virtualized real environments.",https://arxiv.org/abs/2309.09182,"Dai, Zhirui and Asgharivaskasi, Arash and Duong, Thai and Lin, Shusen and Tzes, Maria-Elizabeth and Pappas, George and Atanasov, Nikolay",['language-translation']
PDDL planning with pretrained large language models,"We study few-shot prompting of pretrained large language models (LLMs) towards solving PDDL planning problems. We are interested in two questions: (1)
To what extent can LLMs solve PDDL planning problems on their own? (2) How
and to what extent can LLMs be used to guide AI planners? Recent work by
Valmeekam et al. (2022) presents negative evidence for (1) in the classic blocks
world domain. We confirm this finding, but expand the inquiry to 18 domains and
find more mixed results with a few clear successes. For (2), we propose a simple
mechanism for using good-but-imperfect LLM outputs to aid a heuristic-search
planner. We also find that the LLM performance is due not only to syntactic pattern matching, but also to its commonsense understanding of English terms that
appear in the PDDL. Code: https://tinyurl.com/llm4pddl",https://openreview.net/pdf?id=1QMMUB4zfl,"Silver, Tom and Hariprasad, Varun and Shuttleworth, Reece S and Kumar, Nishanth and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie Pack",['plan-generation']
Palm-e: An embodied multimodal language model,"Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.",https://arxiv.org/abs/2303.03378,"Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",['interactive-planning']
Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,"Despite recent success in large language model (LLM) reasoning, LLMs struggle
with hierarchical multi-step reasoning tasks like generating complex programs.
For these tasks, humans often start with a high-level algorithmic design and
implement each part gradually. We introduce Parsel, a framework enabling
automatic implementation and validation of complex algorithms with code LLMs.
With Parsel, we automatically decompose algorithmic tasks into hierarchical
natural language function descriptions and then search over combinations of
possible function implementations using tests. We show that Parsel can be used
across domains requiring hierarchical reasoning, including program synthesis
and robotic planning. We find that, using Parsel, LLMs solve more
competition-level problems in the APPS dataset, resulting in pass rates over
75\% higher than prior results from directly sampling AlphaCode and Codex,
while often using a smaller sample budget. Moreover, with automatically
generated tests, we find that Parsel can improve the state-of-the-art pass@1
performance on HumanEval from 67\% to 85\%. We also find that LLM-generated
robotic plans using Parsel are more than twice as likely to be considered
accurate than directly generated plans. Lastly, we explore how Parsel addresses
LLM limitations and discuss how Parsel may be useful for human programmers. We
release our code at https://github.com/ezelikman/parsel",https://arxiv.org/abs/2212.10561,"Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah and Haber, Nick",['language-translation']
"Plan, Eliminate, and Track--Language Models are Good Teachers for Embodied Agents","Pre-trained large language models (LLMs) capture procedural knowledge about
the world. Recent work has leveraged LLM's ability to generate abstract plans
to simplify challenging control tasks, either by action scoring, or action
modeling (fine-tuning). However, the transformer architecture inherits several
constraints that make it difficult for the LLM to directly serve as the agent:
e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,
and incompatibility with non-text environments. To maintain compatibility with
a low-level trainable actor, we propose to instead use the knowledge in LLMs to
simplify the control problem, rather than solving it. We propose the Plan,
Eliminate, and Track (PET) framework. The Plan module translates a task
description into a list of high-level sub-tasks. The Eliminate module masks out
irrelevant objects and receptacles from the observation for the current
sub-task. Finally, the Track module determines whether the agent has
accomplished each sub-task. On the AlfWorld instruction following benchmark,
the PET framework leads to a significant 15% improvement over SOTA for
generalization to human goal specifications.",https://arxiv.org/abs/2305.02412,"Wu, Yue and Min, So Yeon and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom and Prabhumoye, Shrimai",['plan-generation']
Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks,"Large Language Models (LLMs) are highly capable of performing
planning for long-horizon robotics tasks, yet existing methods require access to
a pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating).
However, LLM planning does not address how to design or learn those behaviors,
which remains challenging particularly in long-horizon settings. Furthermore,
for many tasks of interest, the robot needs to be able to adjust its behavior in a
fine-grained manner, requiring the agent to be capable of modifying low-level
control actions. Can we instead use the internet-scale knowledge from LLMs for
high-level policies, guiding reinforcement learning (RL) policies to efficiently solve
robotic control tasks online without requiring a pre-determined set of skills? In this
paper, we propose Plan-Seq-Learn (PSL): a modular approach that uses motion
planning to bridge the gap between abstract language and learned low-level control
for solving long-horizon robotics tasks from scratch. We demonstrate that PSL is
capable of solving 20+ challenging single and multi-stage robotics tasks on four
benchmarks at success rates of over 80% from raw visual input, out-performing
language-based, classical, and end-to-end approaches. Video results and code at
mihdalal.github.io/planseqlearn.",https://openreview.net/pdf?id=2q14O7h8Zm,"Dalal, Murtaza and Chiruvolu, Tarun and Chaplot, Devendra Singh and Salakhutdinov, Ruslan",['plan-generation']
Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models,"Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
""Let's think step by step"" as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",https://arxiv.org/abs/2305.04091,"Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng",['plan-generation']
Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks,"We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.",https://arxiv.org/abs/2303.16563,"Yuan, Haoqi and Zhang, Chi and Wang, Hongcheng and Xie, Feiyang and Cai, Penglin and Dong, Hao and Lu, Zongqing",['model-construction']
Planning with Logical Graph-based Language Model for Instruction Generation,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",https://arxiv.org/abs/2308.13782,Fan Zhang and Kebing Jin and Hankz Hankui Zhuo,['plan-generation']
Planning with large language models via corrective re-prompting,"Extracting the common sense knowledge present in Large Language Models
(LLMs) offers a path to designing intelligent, embodied agents. Related works
have queried LLMs with a wide-range of contextual information, such as goals,
sensor observations and scene descriptions, to generate high-level action plans for
specific tasks; however these approaches often involve human intervention or additional machinery to enable sensor-motor interactions. In this work, we propose
a prompting-based strategy for extracting executable plans from an LLM, which
leverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain
contexts, i.e., implicit preconditions must be met for an action to execute (e.g., a
door must be unlocked to open it), and that the embodied agent has the ability to
determine if the action is/is not executable in the current context (e.g., detect if a
precondition error is present). When an agent is unable to execute an action, our
approach re-prompts the LLM with precondition error information to extract an
executable corrective action to achieve the intended goal in the current context.
We evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to
methods that naively re-sample actions from the LLM. Our approach, using precondition errors, improves executability and semantic correctness of plans, while
also reducing the number of re-prompts required when querying actions.",https://openreview.net/pdf?id=cMDMRBe1TKs,"Raman, Shreyas Sundara and Cohen, Vanya and Rosen, Eric and Idrees, Ifrah and Paulius, David and Tellex, Stefanie",['interactive-planning']
Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers,"Plansformer is a novel tool that utilizes a fine-tuned
language model based on transformer architecture
to generate symbolic plans. Transformers are a
type of neural network architecture that have been
shown to be highly effective in a range of natural
language processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans
that are both fluent and feasible. Plansformer takes
the domain and problem files as input (in PDDL)
and outputs a sequence of actions that can be executed to solve the problem. We demonstrate the
effectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and
quantitative results obtained during our evaluation,
including its limitations. Plansformer has the potential to significantly improve the efficiency and
effectiveness of planning in various domains, from
logistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer
via a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at
https://youtu.be/_1rlctCGsrk.",https://www.ijcai.org/proceedings/2023/0839.pdf,"Pallagani, Vishal and Muppasani, Bharath and Srivastava, Biplav and Rossi, Francesca and Horesh, Lior and Murugesan, Keerthiram and Loreggia, Andrea and Fabiano, Francesco and Joseph, Rony and Kethepalli, Yathin and others",['plan-generation']
Plansformer: Generating symbolic plans using transformers,"Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain.",https://arxiv.org/abs/2212.08681,"Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Horesh, Lior and Srivastava, Biplav and Fabiano, Francesco and Loreggia, Andrea",['plan-generation']
ProgPrompt: program generation for situated robot task planning using large language models,"Task planning can require defining myriad domain knowledge about the world in
which a robot needs to act. To ameliorate that effort, large language models
(LLMs) can be used to score potential next actions during task planning, and
even generate action sequences directly, given an instruction in natural
language with no additional domain information. However, such methods either
require enumerating all possible next steps for scoring, or generate free-form
text that may contain actions not possible on a given robot in its current
context. We present a programmatic LLM prompt structure that enables plan
generation functional across situated environments, robot capabilities, and
tasks. Our key insight is to prompt the LLM with program-like specifications of
the available actions and objects in an environment, as well as with example
programs that can be executed. We make concrete recommendations about prompt
structure and generation constraints through ablation experiments, demonstrate
state of the art success rates in VirtualHome household tasks, and deploy our
method on a physical robot arm for tabletop tasks. Website at
progprompt.github.io",https://arxiv.org/abs/2209.11302,"Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh",['plan-generation']
Reasoning on graphs: Faithful and interpretable large language model reasoning,"Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.",https://arxiv.org/abs/2310.01061,"Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui",['plan-generation']
Reasoning with language model is planning with world model,"Large language models (LLMs) have shown remarkable reasoning capabilities,
especially when prompted to generate intermediate reasoning steps (e.g.,
Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are
easy for humans, such as generating action plans for executing tasks in a given
environment, or performing complex math, logical, and commonsense reasoning.
The deficiency stems from the key fact that LLMs lack an internal
$\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment
status, intermediate variable values) and simulate long-term outcomes of
actions. This prevents LLMs from performing deliberate planning akin to human
brains, which involves exploring alternative reasoning paths, anticipating
future states and rewards, and iteratively refining existing reasoning steps.
To overcome the limitations, we propose a new LLM reasoning framework,
$\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning
$\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning
agent, and incorporates a principled planning algorithm (based on Monto Carlo
Tree Search) for strategic exploration in the vast reasoning space. During
reasoning, the LLM (as agent) incrementally builds a reasoning tree under the
guidance of the LLM (as world model) and task-specific rewards, and obtains a
high-reward reasoning path efficiently with a proper balance between
exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of
challenging reasoning problems including plan generation, math reasoning, and
logical inference. Empirical results on these tasks demonstrate the superiority
of RAP over various strong baselines, including CoT and least-to-most prompting
with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%
relative improvement in a plan generation setting.",https://arxiv.org/abs/2305.14992,"Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",['plan-generation']
Reflect: Summarizing robot experiences for failure explanation and correction,"The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.",https://arxiv.org/abs/2306.15724,"Liu, Zeyi and Bahety, Arpit and Song, Shuran",['interactive-planning']
RoboVQA: Multimodal Long-Horizon Reasoning for Robotics,"We present a scalable, bottom-up and intrinsically diverse data collection
scheme that can be used for high-level reasoning with long and medium horizons
and that has 2.2x higher throughput compared to traditional narrow top-down
step-by-step collection. We collect realistic data by performing any user
requests within the entirety of 3 office buildings and using multiple robot and
human embodiments. With this data, we show that models trained on all
embodiments perform better than ones trained on the robot data only, even when
evaluated solely on robot episodes. We find that for a fixed collection budget
it is beneficial to take advantage of cheaper human collection along with robot
collection. We release a large and highly diverse (29,520 unique instructions)
dataset dubbed RoboVQA containing 829,502 (video, text) pairs for
robotics-focused visual question answering. We also demonstrate how evaluating
real robot experiments with an intervention mechanism enables performing tasks
to completion, making it deployable with human oversight even if imperfect
while also providing a single performance metric. We demonstrate a single
video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is
capable of performing a variety of grounded high-level reasoning tasks in broad
realistic settings with a cognitive intervention rate 46% lower than the
zero-shot state of the art visual language model (VLM) baseline and is able to
guide real robots through long-horizon tasks. The performance gap with
zero-shot state-of-the-art models indicates that a lot of grounded data remains
to be collected for real-world deployment, emphasizing the critical need for
scalable data collection approaches. Finally, we show that video VLMs
significantly outperform single-image VLMs with an average error rate reduction
of 19% across all VQA tasks. Data and videos available at
https://robovqa.github.io",https://arxiv.org/abs/2311.00899,"Sermanet, Pierre and Ding, Tianli and Zhao, Jeffrey and Xia, Fei and Dwibedi, Debidatta and Gopalakrishnan, Keerthana and Chan, Christine and Dulac-Arnold, Gabriel and Maddineni, Sharath and Joshi, Nikhil J and others",['plan-generation']
Robots that ask for help: Uncertainty alignment for large language model planners,"Large language models (LLMs) exhibit a wide range of promising capabilities
-- from step-by-step planning to commonsense reasoning -- that may provide
utility for robots, but remain prone to confidently hallucinated predictions.
In this work, we present KnowNo, which is a framework for measuring and
aligning the uncertainty of LLM-based planners such that they know when they
don't know and ask for help when needed. KnowNo builds on the theory of
conformal prediction to provide statistical guarantees on task completion while
minimizing human help in complex multi-step planning settings. Experiments
across a variety of simulated and real robot setups that involve tasks with
different modes of ambiguity (e.g., from spatial to numeric uncertainties, from
human preferences to Winograd schemas) show that KnowNo performs favorably over
modern baselines (which may involve ensembles or extensive prompt tuning) in
terms of improving efficiency and autonomy, while providing formal assurances.
KnowNo can be used with LLMs out of the box without model-finetuning, and
suggests a promising lightweight approach to modeling uncertainty that can
complement and scale with the growing capabilities of foundation models.
Website: https://robot-help.github.io",https://arxiv.org/abs/2307.01928,"Ren, Allen Z and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and others",['interactive-planning']
Roco: Dialectic multi-robot collaboration with large language models,"We propose a novel approach to multi-robot collaboration that harnesses the
power of pre-trained large language models (LLMs) for both high-level
communication and low-level path planning. Robots are equipped with LLMs to
discuss and collectively reason task strategies. They then generate sub-task
plans and task space waypoint paths, which are used by a multi-arm motion
planner to accelerate trajectory planning. We also provide feedback from the
environment, such as collision checking, and prompt the LLM agents to improve
their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a
6-task benchmark covering a wide range of multi-robot collaboration scenarios,
accompanied by a text-only dataset for agent representation and reasoning. We
experimentally demonstrate the effectiveness of our approach -- it achieves
high success rates across all tasks in RoCoBench and adapts to variations in
task semantics. Our dialog setup offers high interpretability and flexibility
-- in real world experiments, we show RoCo easily incorporates
human-in-the-loop, where a user can communicate and collaborate with a robot
agent to complete tasks together. See project website
https://project-roco.github.io for videos and code.",https://arxiv.org/abs/2307.04738,"Mandi, Zhao and Jain, Shreeya and Song, Shuran",['model-construction']
SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"In this work, we introduce SMART-LLM, an innovative framework designed for
embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task
Planning using Large Language Models (LLMs), harnesses the power of LLMs to
convert high-level task instructions provided as input into a multi-robot task
plan. It accomplishes this by executing a series of stages, including task
decomposition, coalition formation, and task allocation, all guided by
programmatic LLM prompts within the few-shot prompting paradigm. We create a
benchmark dataset designed for validating the multi-robot task planning
problem, encompassing four distinct categories of high-level instructions that
vary in task complexity. Our evaluation experiments span both simulation and
real-world scenarios, demonstrating that the proposed model can achieve
promising results for generating multi-robot task plans. The experimental
videos, code, and datasets from the work can be found at
https://sites.google.com/view/smart-llm/.",https://arxiv.org/abs/2309.10062,"Kannan, Shyam Sundar and Venkatesh, Vishnunandan LN and Min, Byung-Cheol",['plan-generation']
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",https://arxiv.org/abs/2308.12682,"Hazra, Rishi and Martires, Pedro Zuidberg Dos and De Raedt, Luc",['heuristics-optimization']
Saynav: Grounding large language models for dynamic planning to navigation in new environments,"Semantic reasoning and dynamic planning capabilities are crucial for an
autonomous agent to perform complex navigation tasks in unknown environments.
It requires a large amount of common-sense knowledge, that humans possess, to
succeed in these tasks. We present SayNav, a new approach that leverages human
knowledge from Large Language Models (LLMs) for efficient generalization to
complex navigation tasks in unknown large-scale environments. SayNav uses a
novel grounding mechanism, that incrementally builds a 3D scene graph of the
explored environment as inputs to LLMs, for generating feasible and
contextually appropriate high-level plans for navigation. The LLM-generated
plan is then executed by a pre-trained low-level planner, that treats each
planned step as a short-distance point-goal navigation sub-task. SayNav
dynamically generates step-by-step instructions during navigation and
continuously refines future steps based on newly perceived information. We
evaluate SayNav on multi-object navigation (MultiON) task, that requires the
agent to utilize a massive amount of human knowledge to efficiently search
multiple different objects in an unknown environment. We also introduce a
benchmark dataset for MultiON task employing ProcTHOR framework that provides
large photo-realistic indoor environments with variety of objects. SayNav
achieves state-of-the-art results and even outperforms an oracle based baseline
with strong ground-truth assumptions by more than 8% in terms of success rate,
highlighting its ability to generate dynamic plans for successfully locating
objects in large-scale new environments. The code, benchmark dataset and
demonstration videos are accessible at
https://www.sri.com/ics/computer-vision/saynav.",https://arxiv.org/abs/2309.04077,"Rajvanshi, Abhinav and Sikka, Karan and Lin, Xiao and Lee, Bhoram and Chiu, Han-Pang and Velasquez, Alvaro",['plan-generation']
Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,"Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.",https://arxiv.org/abs/2307.06135,"Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko",['plan-generation']
Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?,"A flurry of recent work has demonstrated that pre-trained large language
models (LLMs) can be effective task planners for a variety of single-robot
tasks. The planning performance of LLMs is significantly improved via prompting
techniques, such as in-context learning or re-prompting with state feedback,
placing new importance on the token budget for the context window. An
under-explored but natural next direction is to investigate LLMs as multi-robot
task planners. However, long-horizon, heterogeneous multi-robot planning
introduces new challenges of coordination while also pushing up against the
limits of context window length. It is therefore critical to find
token-efficient LLM planning frameworks that are also able to reason about the
complexities of multi-robot coordination. In this work, we compare the task
success rate and token efficiency of four multi-agent communication frameworks
(centralized, decentralized, and two hybrid) as applied to four
coordination-dependent multi-agent 2D task scenarios for increasing numbers of
agents. We find that a hybrid framework achieves better task success rates
across all four tasks and scales better to more agents. We further demonstrate
the hybrid frameworks in 3D simulations where the vision-to-text problem and
dynamical errors are considered. See our project website
https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and
code.",https://arxiv.org/abs/2309.15943,"Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu",['multiagent-planning']
Statler: State-maintaining language models for embodied reasoning,"There has been a significant research interest in employing large language
models to empower intelligent robots with complex reasoning. Existing work
focuses on harnessing their abilities to reason about the histories of their
actions and observations. In this paper, we explore a new dimension in which
large language models may benefit robotics planning. In particular, we propose
Statler, a framework in which large language models are prompted to maintain an
estimate of the world state, which are often unobservable, and track its
transition as new actions are taken. Our framework then conditions each action
on the estimate of the current world state. Despite being conceptually simple,
our Statler framework significantly outperforms strong competing methods (e.g.,
Code-as-Policies) on several robot planning tasks. Additionally, it has the
potential advantage of scaling up to more challenging long-horizon planning
tasks.",https://arxiv.org/abs/2306.17840,"Yoneda, Takuma and Fang, Jiading and Li, Peng and Zhang, Huanyu and Jiang, Tianchong and Lin, Shengjie and Picker, Ben and Yunis, David and Mei, Hongyuan and Walter, Matthew R",['model-construction']
Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact with
the world, which marks an initial step toward versatile robotics. However, these
efforts tend to overlook the visual richness of open worlds, rendering the entire
interactive process akin to a blindfolded text-based game. Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their
surroundings and producing responses that are easy to understand. In this paper,
we propose Steve-Eye, an end-to-end trained large multimodal model to address
this limitation. Steve-Eye integrates the LLM with a visual encoder to process
visual-text inputs and generate multimodal feedback. We adopt a semi-automatic
strategy to collect an extensive dataset comprising 850K open-world instruction
pairs, enabling our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks and carry
out experiments from a wide range of perspectives to validate our models capability to strategically act and plan. The projects website and code can be found at
https://sites.google.com/view/steve-eye.",https://openreview.net/pdf?id=NltzxpG0nz,"Zheng, Sipeng and Liu, Jiazheng and Feng, Yicheng and Lu, Zongqing",['interactive-planning']
Strategic Reasoning with Language Models,"Strategic reasoning enables agents to cooperate, communicate, and compete
with other agents in diverse situations. Existing approaches to solving
strategic games rely on extensive training, yielding strategies that do not
generalize to new scenarios or games without retraining. Large Language Models
(LLMs), with their ability to comprehend and generate complex, context-rich
language, could prove powerful as tools for strategic gameplay. This paper
introduces an approach that uses pretrained LLMs with few-shot chain-of-thought
examples to enable strategic reasoning for AI agents. Our approach uses
systematically generated demonstrations of reasoning about states, values, and
beliefs to prompt the model. Using extensive variations of simple matrix games,
we show that strategies that are derived based on systematically generated
prompts generalize almost perfectly to new game structures, alternate
objectives, and hidden information. Additionally, we demonstrate our approach
can lead to human-like negotiation strategies in realistic scenarios without
any extra training or fine-tuning. Our results highlight the ability of LLMs,
guided by systematic reasoning demonstrations, to adapt and excel in diverse
strategic scenarios.",https://arxiv.org/abs/2305.19165,"Gandhi, Kanishk and Sadigh, Dorsa and Goodman, Noah D",['plan-generation']
SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,"We introduce S WIFT S AGE , a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. S WIFT S AGE integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the S WIFT module,
representing fast and intuitive thinking, and the S AGE module, emulating deliberate
thought processes. The S WIFT module is a small encoder-decoder LM fine-tuned
on the oracle agents action trajectories, while the S AGE module employs LLMs
such as GPT-4 for subgoal planning and grounding. We develop a heuristic method
to harmoniously integrate the two modules, resulting in a more efficient and
robust problem-solving process. In 30 tasks from the ScienceWorld benchmark,
S WIFT S AGE significantly outperforms other methods such as SayCan, ReAct, and
Reflexion, demonstrating its effectiveness in solving complex interactive tasks.",https://proceedings.neurips.cc/paper_files/paper/2023/file/4b0eea69deea512c9e2c469187643dc2-Paper-Conference.pdf,"Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Ammanabrolu, Prithviraj and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang",['brain-inspired-planning']
Task and motion planning with large language models for object rearrangement,"Multi-object rearrangement is a crucial skill for service robots, and
commonsense reasoning is frequently needed in this process. However, achieving
commonsense arrangements requires knowledge about objects, which is hard to
transfer to robots. Large language models (LLMs) are one potential source of
this knowledge, but they do not naively capture information about plausible
physical arrangements of the world. We propose LLM-GROP, which uses prompting
to extract commonsense knowledge about semantically valid object configurations
from an LLM and instantiates them with a task and motion planner in order to
generalize to varying scene geometry. LLM-GROP allows us to go from
natural-language commands to human-aligned object rearrangement in varied
environments. Based on human evaluations, our approach achieves the highest
rating while outperforming competitive baselines in terms of success rate while
maintaining comparable cumulative action costs. Finally, we demonstrate a
practical implementation of LLM-GROP on a mobile manipulator in real-world
scenarios. Supplementary materials are available at:
https://sites.google.com/view/llm-grop",https://arxiv.org/abs/2303.06247,"Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi",['plan-generation']
Text2motion: From natural language instructions to feasible plans,"We propose Text2Motion, a language-based planning framework enabling robots
to solve sequential manipulation tasks that require long-horizon reasoning.
Given a natural language instruction, our framework constructs both a task- and
motion-level plan that is verified to reach inferred symbolic goals.
Text2Motion uses feasibility heuristics encoded in Q-functions of a library of
skills to guide task planning with Large Language Models. Whereas previous
language-based planners only consider the feasibility of individual skills,
Text2Motion actively resolves geometric dependencies spanning skill sequences
by performing geometric feasibility planning during its search. We evaluate our
method on a suite of problems that require long-horizon reasoning,
interpretation of abstract goals, and handling of partial affordance
perception. Our experiments show that Text2Motion can solve these challenging
problems with a success rate of 82%, while prior state-of-the-art
language-based planning methods only achieve 13%. Text2Motion thus provides
promising generalization characteristics to semantically diverse sequential
manipulation tasks with geometric dependencies between skills.",https://arxiv.org/abs/2303.12153,"Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette",['language-translation']
There and back again: extracting formal domains for controllable neurosymbolic story authoring,"Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences.",https://ojs.aaai.org/index.php/AIIDE/article/view/27502,"Kelly, Jack and Calderwood, Alex and Wardrip-Fruin, Noah and Mateas, Michael",['language-translation']
Tool documentation enables zero-shot tool-usage with large language models,"Today, large language models (LLMs) are taught to use new tools by providing
a few demonstrations of the tool's usage. Unfortunately, demonstrations are
hard to acquire, and can result in undesirable biased usage if the wrong
demonstration is chosen. Even in the rare scenario that demonstrations are
readily available, there is no principled selection protocol to determine how
many and which ones to provide. As tasks grow more complex, the selection
search grows combinatorially and invariably becomes intractable. Our work
provides an alternative to demonstrations: tool documentation. We advocate the
use of tool documentation, descriptions for the individual tool usage, over
demonstrations. We substantiate our claim through three main empirical findings
on 6 tasks across both vision and language modalities. First, on existing
benchmarks, zero-shot prompts with only tool documentation are sufficient for
eliciting proper tool usage, achieving performance on par with few-shot
prompts. Second, on a newly collected realistic tool-use dataset with hundreds
of available tool APIs, we show that tool documentation is significantly more
valuable than demonstrations, with zero-shot documentation significantly
outperforming few-shot without documentation. Third, we highlight the benefits
of tool documentations by tackling image generation and video tracking using
just-released unseen state-of-the-art models as tools. Finally, we highlight
the possibility of using tool documentation to automatically enable new
applications: by using nothing more than the documentation of GroundingDino,
Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the
just-released Grounded-SAM and Track Anything models.",https://arxiv.org/abs/2308.00675,"Hsieh, Cheng-Yu and Chen, Si-An and Li, Chun-Liang and Fujii, Yasuhisa and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and Pfister, Tomas",['tool-integration']
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings,"Integrating large language models (LLMs) with various tools has led to increased attention in the field. Existing approaches either involve fine-tuning the LLM, which is both computationally costly and limited to a fixed set of tools, or prompting LLMs by in-context tool demonstrations. Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named ToolkenGPT, wherein LLMs effectively learn to master tools as predicting tokens through tool embeddings for solving complex tasks. In this framework, each tool is transformed into vector embeddings and plugged into the language model head. Once the function is triggered during text generation, the LLM enters a special function mode to execute the tool calls. Our experiments show that function embeddings effectively help LLMs understand tool use and improve on several tasks, including numerical reasoning, knowledge-based question answering and embodied decision-making.

",https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html,"Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting",['tool-integration']
Tptu: Task planning and tool usage of large language model-based ai agents,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",https://arxiv.org/abs/2308.03427,"Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Zeng, Xingyu and Zhao, Rui",['tool-integration']
Translating natural language to planning goals with large-language models,"Recent large language models (LLMs) have demonstrated remarkable performance
on a variety of natural language processing (NLP) tasks, leading to intense
excitement about their applicability across various domains. Unfortunately,
recent work has also shown that LLMs are unable to perform accurate reasoning
nor solve planning problems, which may limit their usefulness for
robotics-related tasks. In this work, our central question is whether LLMs are
able to translate goals specified in natural language to a structured planning
language. If so, LLM can act as a natural interface between the planner and
human users; the translated goal can be handed to domain-independent AI
planners that are very effective at planning. Our empirical results on GPT 3.5
variants show that LLMs are much better suited towards translation rather than
planning. We find that LLMs are able to leverage commonsense knowledge and
reasoning to furnish missing details from under-specified goals (as is often
the case in natural language). However, our experiments also reveal that LLMs
can fail to generate goals in tasks that involve numerical or physical (e.g.,
spatial) reasoning, and that LLMs are sensitive to the prompts used. As such,
these models are promising for translation to structured planning languages,
but care should be taken in their use.",https://arxiv.org/abs/2302.05128,"Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold",['language-translation']
Tree of thoughts: Deliberate problem solving with large language models,"Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.",https://arxiv.org/abs/2305.10601,"Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik",['plan-generation']
Tree-Planner: Efficient Close-loop Task Planning with Large Language Models,"This paper studies close-loop task planning, which refers to the process of
generating a sequence of skills (a plan) to accomplish a specific goal while
adapting the plan based on real-time observations. Recently, prompting Large
Language Models (LLMs) to generate actions iteratively has become a prevalent
paradigm due to its superior performance and user-friendliness. However, this
paradigm is plagued by two inefficiencies: high token consumption and redundant
error correction, both of which hinder its scalability for large-scale testing
and applications. To address these issues, we propose Tree-Planner, which
reframes task planning with LLMs into three distinct phases: plan sampling,
action tree construction, and grounded deciding. Tree-Planner starts by using
an LLM to sample a set of potential plans before execution, followed by the
aggregation of them to form an action tree. Finally, the LLM performs a
top-down decision-making process on the tree, taking into account real-time
environmental information. Experiments show that Tree-Planner achieves
state-of-the-art performance while maintaining high efficiency. By decomposing
LLM queries into a single plan-sampling call and multiple grounded-deciding
calls, a considerable part of the prompt are less likely to be repeatedly
consumed. As a result, token consumption is reduced by 92.2% compared to the
previously best-performing model. Additionally, by enabling backtracking on the
action tree as needed, the correction process becomes more flexible, leading to
a 40.5% decrease in error corrections.",https://arxiv.org/abs/2310.08582,"Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping",['interactive-planning']
Tree-of-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning,"There emerges a promising trend of using large language models (LLMs) to
generate code-like plans for complex inference tasks such as visual reasoning.
This paradigm, known as LLM-based planning, provides flexibility in problem
solving and endows better interpretability. However, current research is mostly
limited to basic scenarios of simple questions that can be straightforward
answered in a few inference steps. Planning for the more challenging multi-hop
visual reasoning tasks remains under-explored. Specifically, under multi-hop
reasoning situations, the trade-off between accuracy and the complexity of
plan-searching becomes prominent. The prevailing algorithms either address the
efficiency issue by employing the fast one-stop generation or adopt a complex
iterative generation method to improve accuracy. Both fail to balance the need
for efficiency and performance. Drawing inspiration from the dual system of
cognition in the human brain, the fast and the slow think processes, we propose
a hierarchical plan-searching algorithm that integrates the one-stop reasoning
(fast) and the Tree-of-thought (slow). Our approach succeeds in performance
while significantly saving inference steps. Moreover, we repurpose the PTR and
the CLEVER datasets, developing a systematic framework for evaluating the
performance and efficiency of LLMs-based plan-search algorithms under reasoning
tasks at different levels of difficulty. Extensive experiments demonstrate the
superiority of our proposed algorithm in terms of performance and efficiency.
The dataset and code will be release soon.",https://arxiv.org/abs/2308.09658,"Hu, Pengbo and Qi, Ji and Li, Xingyu and Li, Hong and Wang, Xinqi and Quan, Bing and Wang, Ruiyu and Zhou, Yi",['brain-inspired-planning']
Understanding the Capabilities of Large Language Models for Automated Planning,"Automated planning is concerned with developing efficient algorithms to
generate plans or sequences of actions to achieve a specific goal in a given
environment. Emerging Large Language Models (LLMs) can answer questions, write
high-quality programming code, and predict protein folding, showcasing their
versatility in solving various tasks beyond language-based problems. In this
paper, we aim to explore how LLMs can also be used for automated planning. To
do so, we seek to answer four key questions. Firstly, we want to understand the
extent to which LLMs can be used for plan generation. Secondly, we aim to
identify which pre-training data is most effective in facilitating plan
generation. Thirdly, we investigate whether fine-tuning or prompting is a more
effective approach for plan generation. Finally, we explore whether LLMs are
capable of plan generalization. By answering these questions, the study seeks
to shed light on the capabilities of LLMs in solving complex planning problems
and provide insights into the most effective approaches for using LLMs in this
context.",https://arxiv.org/abs/2305.16151,"Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Srivastava, Biplav and Horesh, Lior and Fabiano, Francesco and Loreggia, Andrea",['plan-generation']
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents,"Designing versatile graph learning approaches is important, considering the
diverse graphs and tasks existing in real-world applications. Existing methods
have attempted to achieve this target through automated machine learning
techniques, pre-training and fine-tuning strategies, and large language models.
However, these methods are not versatile enough for graph learning, as they
work on either limited types of graphs or a single task. In this paper, we
propose to explore versatile graph learning approaches with LLM-based agents,
and the key insight is customizing the graph learning procedures for diverse
graphs and tasks. To achieve this, we develop several LLM-based agents,
equipped with diverse profiles, tools, functions and human experience. They
collaborate to configure each procedure with task and data-specific settings
step by step towards versatile solutions, and the proposed method is dubbed
GL-Agent. By evaluating on diverse tasks and graphs, the correct results of the
agent and its comparable performance showcase the versatility of the proposed
method, especially in complex scenarios.The low resource cost and the potential
to use open-source LLMs highlight the efficiency of GL-Agent.",https://arxiv.org/abs/2309.04565,"Wei, Lanning and He, Zhiqiang and Zhao, Huan and Yao, Quanming",['multiagent-planning']
Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning,"Recent text-to-video (T2V) generation methods have seen significant
advancements. However, the majority of these works focus on producing short
video clips of a single event (i.e., single-scene videos). Meanwhile, recent
large language models (LLMs) have demonstrated their capability in generating
layouts and programs to control downstream visual modules. This prompts an
important question: can we leverage the knowledge embedded in these LLMs for
temporally consistent long video generation? In this paper, we propose
VideoDirectorGPT, a novel framework for consistent multi-scene video generation
that uses the knowledge of LLMs for video content planning and grounded video
generation. Specifically, given a single text prompt, we first ask our video
planner LLM (GPT-4) to expand it into a 'video plan', which includes the scene
descriptions, the entities with their respective layouts, the background for
each scene, and consistency groupings of the entities. Next, guided by this
video plan, our video generator, named Layout2Vid, has explicit control over
spatial layouts and can maintain temporal consistency of entities across
multiple scenes, while being trained only with image-level annotations. Our
experiments demonstrate that our proposed VideoDirectorGPT framework
substantially improves layout and movement control in both single- and
multi-scene video generation and can generate multi-scene videos with
consistency, while achieving competitive performance with SOTAs in open-domain
single-scene T2V generation. Detailed ablation studies, including dynamic
adjustment of layout control strength with an LLM and video generation with
user-provided images, confirm the effectiveness of each component of our
framework and its future potential.",https://arxiv.org/abs/2309.15091,"Lin, Han and Zala, Abhay and Cho, Jaemin and Bansal, Mohit",['plan-generation']
Vision-Language Interpreter for Robot Task Planning,"Large language models (LLMs) are accelerating the development of
language-guided robot planners. Meanwhile, symbolic planners offer the
advantage of interpretability. This paper proposes a new task that bridges
these two trends, namely, multimodal planning problem specification. The aim is
to generate a problem description (PD), a machine-readable file used by the
planners to find a plan. By generating PDs from language instruction and scene
observation, we can drive symbolic planners in a language-guided framework. We
propose a Vision-Language Interpreter (ViLaIn), a new framework that generates
PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine
generated PDs via error message feedback from the symbolic planner. Our aim is
to answer the question: How accurately can ViLaIn and the symbolic planner
generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset
called the problem description generation (ProDG) dataset. The framework is
evaluated with four new evaluation metrics. Experimental results show that
ViLaIn can generate syntactically correct problems with more than 99\% accuracy
and valid plans with more than 58\% accuracy. Our code and dataset are
available at https://github.com/omron-sinicx/ViLaIn.",https://arxiv.org/abs/2311.00967,"Shirai, Keisuke and Beltran-Hernandez, Cristian C and Hamaya, Masashi and Hashimoto, Atsushi and Tanaka, Shohei and Kawaharazuka, Kento and Tanaka, Kazutoshi and Ushiku, Yoshitaka and Mori, Shinsuke",['language-translation']
Voxposer: Composable 3d value maps for robotic manipulation with language models,"Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a vision-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Videos and code at https://voxposer.github.io",https://arxiv.org/abs/2307.05973,"Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li",['plan-generation']
War and peace (waragent): Large language model-based multi-agent simulation of world wars,"Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.",https://arxiv.org/abs/2311.17227,"Hua, Wenyue and Fan, Lizhou and Li, Lingyao and Mei, Kai and Ji, Jianchao and Ge, Yingqiang and Hemphill, Libby and Zhang, Yongfeng",['multiagent-planning']
LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning,"Effective planning is essential for the success of any task, from organizing
a vacation to routing autonomous vehicles and developing corporate strategies.
It involves setting goals, formulating plans, and allocating resources to
achieve them. LLMs are particularly well-suited for automated planning due to
their strong capabilities in commonsense reasoning. They can deduce a sequence
of actions needed to achieve a goal from a given state and identify an
effective course of action. However, it is frequently observed that plans
generated through direct prompting often fail upon execution. Our survey aims
to highlight the existing challenges in planning with language models, focusing
on key areas such as embodied environments, optimal scheduling, competitive and
cooperative games, task decomposition, reasoning, and planning. Through this
study, we explore how LLMs transform AI planning and provide unique insights
into the future of LM-assisted planning.",http://arxiv.org/abs/2409.01806v1,"Haoming Li, Zhaoliang Chen, Jonathan Zhang, Fei Liu","['plan-generation', 'goal-decomposition']"
Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages,"Many recent works have explored using language models for planning problems.
One line of research focuses on translating natural language descriptions of
planning tasks into structured planning languages, such as the planning domain
definition language (PDDL). While this approach is promising, accurately
measuring the quality of generated PDDL code continues to pose significant
challenges. First, generated PDDL code is typically evaluated using planning
validators that check whether the problem can be solved with a planner. This
method is insufficient because a language model might generate valid PDDL code
that does not align with the natural language description of the task. Second,
existing evaluation sets often have natural language descriptions of the
planning task that closely resemble the ground truth PDDL, reducing the
challenge of the task. To bridge this gap, we introduce \benchmarkName, a
benchmark designed to evaluate language models' ability to generate PDDL code
from natural language descriptions of planning tasks. We begin by creating a
PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL
code generated by language models by flexibly comparing it against a ground
truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across
13 different tasks, with varying levels of difficulty. Finally, we evaluate
several API-access and open-weight language models that reveal this task's
complexity. For example, $87.6\%$ of the PDDL problem descriptions generated by
GPT-4o are syntactically parseable, $82.2\%$ are valid, solve-able problems,
but only $35.1\%$ are semantically correct, highlighting the need for a more
rigorous benchmark for this problem.",http://arxiv.org/abs/2407.03321v1,"Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, Stephen H. Bach",['language-translation']
Automated radiotherapy treatment planning guided by GPT-4Vision,"Radiotherapy treatment planning is a time-consuming and potentially
subjective process that requires the iterative adjustment of model parameters
to balance multiple conflicting objectives. Recent advancements in large
foundation models offer promising avenues for addressing the challenges in
planning and clinical decision-making. This study introduces GPT-RadPlan, a
fully automated treatment planning framework that harnesses prior radiation
oncology knowledge encoded in multi-modal large language models, such as
GPT-4Vision (GPT-4V) from OpenAI. GPT-RadPlan is made aware of planning
protocols as context and acts as an expert human planner, capable of guiding a
treatment planning process. Via in-context learning, we incorporate clinical
protocols for various disease sites as prompts to enable GPT-4V to acquire
treatment planning domain knowledge. The resulting GPT-RadPlan agent is
integrated into our in-house inverse treatment planning system through an API.
The efficacy of the automated planning system is showcased using multiple
prostate and head & neck cancer cases, where we compared GPT-RadPlan results to
clinical plans. In all cases, GPT-RadPlan either outperformed or matched the
clinical plans, demonstrating superior target coverage and organ-at-risk
sparing. Consistently satisfying the dosimetric objectives in the clinical
protocol, GPT-RadPlan represents the first multimodal large language model
agent that mimics the behaviors of human planners in radiation oncology
clinics, achieving remarkable results in automating the treatment planning
process without the need for additional training.",http://arxiv.org/abs/2406.15609v2,"Sheng Liu, Oscar Pastor-Serrano, Yizheng Chen, Matthew Gopaulchan, Weixing Liang, Mark Buyyounouski, Erqi Pollom, Quynh-Thu Le, Michael Gensheimer, Peng Dong, Yong Yang, James Zou, Lei Xing","['plan-generation', 'interactive-planning']"
RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents,"In this past year, large language models (LLMs) have had remarkable success
in domains outside the traditional natural language processing, and people are
starting to explore the usage of LLMs in more general and close to application
domains like code generation, travel planning, and robot controls. Connecting
these LLMs with great capacity and external tools, people are building the
so-called LLM agents, which are supposed to help people do all kinds of work in
everyday life. In all these domains, the prompt to the LLMs has been shown to
make a big difference in what the LLM would generate and thus affect the
performance of the LLM agents. Therefore, automatic prompt engineering has
become an important question for many researchers and users of LLMs. In this
paper, we propose a novel method, \textsc{RePrompt}, which does ""gradient
descent"" to optimize the step-by-step instructions in the prompt of the LLM
agents based on the chat history obtained from interactions with LLM agents. By
optimizing the prompt, the LLM will learn how to plan in specific domains. We
have used experiments in PDDL generation and travel planning to show that our
method could generally improve the performance for different reasoning tasks
when using the updated prompt as the initial prompt.",http://arxiv.org/abs/2406.11132v1,"Weizhe Chen, Sven Koenig, Bistra Dilkina","['plan-generation', 'interactive-planning']"
DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning,"Dual-arm robots offer enhanced versatility and efficiency over single-arm
counterparts by enabling concurrent manipulation of multiple objects or
cooperative execution of tasks using both arms. However, effectively
coordinating the two arms for complex long-horizon tasks remains a significant
challenge. Existing task planning methods predominantly focus on single-arm
robots or rely on predefined bimanual operations, failing to fully leverage the
capabilities of dual-arm systems. To address this limitation, we introduce
DAG-Plan, a structured task planning framework tailored for dual-arm robots.
DAG-Plan harnesses large language models (LLMs) to decompose intricate tasks
into actionable sub-tasks represented as nodes within a directed acyclic graph
(DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the
appropriate arm based on real-time environmental observations, enabling
parallel and adaptive execution. We evaluate DAG-Plan on the novel Dual-Arm
Kitchen Benchmark, comprising 9 sequential tasks with 78 sub-tasks and 26
objects. Extensive experiments demonstrate the superiority of DAG-Plan over
directly using LLM to generate plans, achieving nearly 50% higher efficiency
compared to the single-arm task planning baseline and nearly double the success
rate of the dual-arm task planning baseline.",http://arxiv.org/abs/2406.09953v2,"Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu",['model-construction']
CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning,"Large Language Models (LLMs) possess extensive foundational knowledge and
moderate reasoning abilities, making them suitable for general task planning in
open-world scenarios. However, it is challenging to ground a LLM-generated plan
to be executable for the specified robot with certain restrictions. This paper
introduces CLMASP, an approach that couples LLMs with Answer Set Programming
(ASP) to overcome the limitations, where ASP is a non-monotonic logic
programming formalism renowned for its capacity to represent and reason about a
robot's action knowledge. CLMASP initiates with a LLM generating a basic
skeleton plan, which is subsequently tailored to the specific scenario using a
vector database. This plan is then refined by an ASP program with a robot's
action knowledge, which integrates implementation details into the skeleton,
grounding the LLM's abstract outputs in practical robot contexts. Our
experiments conducted on the VirtualHome platform demonstrate CLMASP's
efficacy. Compared to the baseline executable rate of under 2% with LLM
approaches, CLMASP significantly improves this to over 90%.",http://arxiv.org/abs/2406.03367v1,"Xinrui Lin, Yangfan Wu, Huanyu Yang, Yu Zhang, Yanyong Zhang, Jianmin Ji","['plan-generation', 'heuristics-optimization']"
GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games,"With their prominent scene understanding and reasoning capabilities,
pre-trained visual-language models (VLMs) such as GPT-4V have attracted
increasing attention in robotic task planning. Compared with traditional task
planning strategies, VLMs are strong in multimodal information parsing and code
generation and show remarkable efficiency. Although VLMs demonstrate great
potential in robotic task planning, they suffer from challenges like
hallucination, semantic complexity, and limited context. To handle such issues,
this paper proposes a multi-agent framework, i.e., GameVLM, to enhance the
decision-making process in robotic task planning. In this study, VLM-based
decision and expert agents are presented to conduct the task planning.
Specifically, decision agents are used to plan the task, and the expert agent
is employed to evaluate these task plans. Zero-sum game theory is introduced to
resolve inconsistencies among different agents and determine the optimal
solution. Experimental results on real robots demonstrate the efficacy of the
proposed framework, with an average success rate of 83.3%.",http://arxiv.org/abs/2405.13751v1,"Aoran Mei, Jianhua Wang, Guo-Niu Zhu, Zhongxue Gan","['plan-generation', 'multiagent-planning']"
LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs,"Recent availability of Large Language Models (LLMs) has led to the
development of numerous LLM-based approaches aimed at providing natural
language interfaces for various end-user tasks. These end-user tasks in turn
can typically be accomplished by orchestrating a given set of APIs. In
practice, natural language task requests (user queries) are often incomplete,
i.e., they may not contain all the information required by the APIs. While LLMs
excel at natural language processing (NLP) tasks, they frequently hallucinate
on missing information or struggle with orchestrating the APIs. The key idea
behind our proposed approach is to leverage logical reasoning and classical AI
planning along with an LLM for accurately answering user queries including
identification and gathering of any missing information in these queries. Our
approach uses an LLM and ASP (Answer Set Programming) solver to translate a
user query to a representation in Planning Domain Definition Language (PDDL)
via an intermediate representation in ASP. We introduce a special API
""get_info_api"" for gathering missing information. We model all the APIs as PDDL
actions in a way that supports dataflow between the APIs. Our approach then
uses a classical AI planner to generate an orchestration of API calls
(including calls to get_info_api) to answer the user query. Our evaluation
results show that our approach significantly outperforms a pure LLM based
approach by achieving over 95\% success rate in most cases on a dataset
containing complete and incomplete single goal and multi-goal queries where the
multi-goal queries may or may not require dataflow among the APIs.",http://arxiv.org/abs/2405.12433v1,"Sudhir Agarwal, Anu Sreepathy, David H. Alonso, Prarit Lamba","['language-translation', 'tool-integration']"
NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions,"Today's classical planners are powerful, but modeling input tasks in formats
such as PDDL is tedious and error-prone. In contrast, planning with Large
Language Models (LLMs) allows for almost any input text, but offers no
guarantees on plan quality or even soundness. In an attempt to merge the best
of these two approaches, some work has begun to use LLMs to automate parts of
the PDDL creation process. However, these methods still require various degrees
of expert input. We present NL2Plan, the first domain-agnostic offline
LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the
necessary information from a short text prompt before creating a complete PDDL
description of both the domain and the problem, which is finally solved by a
classical planner. We evaluate NL2Plan on four planning domains and find that
it solves 10 out of 15 tasks - a clear improvement over a plain
chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover,
in two out of the five failure cases, instead of returning an invalid plan,
NL2Plan reports that it failed to solve the task. In addition to using NL2Plan
in end-to-end mode, users can inspect and correct all of its intermediate
results, such as the PDDL representation, increasing explainability and making
it an assistive tool for PDDL creation.",http://arxiv.org/abs/2405.04215v1,"Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp",['language-translation']
Generating consistent PDDL domains with Large Language Models,"Large Language Models (LLMs) are capable of transforming natural language
domain descriptions into plausibly looking PDDL markup. However, ensuring that
actions are consistent within domains still remains a challenging task. In this
paper we present a novel concept to significantly improve the quality of
LLM-generated PDDL models by performing automated consistency checking during
the generation process. Although the proposed consistency checking strategies
still can't guarantee absolute correctness of generated models, they can serve
as valuable source of feedback reducing the amount of correction efforts
expected from a human in the loop. We demonstrate the capabilities of our error
detection approach on a number of classical and custom planning domains
(logistics, gripper, tyreworld, household, pizza).",http://arxiv.org/abs/2404.07751v1,"Pavel Smirnov, Frank Joublin, Antonello Ceravola, Michael Gienger","['language-translation', 'model-construction']"
The Case for Developing a Foundation Model for Planning-like Tasks from Scratch,"Foundation Models (FMs) have revolutionized many areas of computing,
including Automated Planning and Scheduling (APS). For example, a recent study
found them useful for planning problems: plan generation, language translation,
model construction, multi-agent planning, interactive planning, heuristics
optimization, tool integration, and brain-inspired planning. Besides APS, there
are many seemingly related tasks involving the generation of a series of
actions with varying guarantees of their executability to achieve intended
goals, which we collectively call planning-like (PL) tasks like business
processes, programs, workflows, and guidelines, where researchers have
considered using FMs. However, previous works have primarily focused on
pre-trained, off-the-shelf FMs and optionally fine-tuned them. This paper
discusses the need for a comprehensive FM for PL tasks from scratch and
explores its design considerations. We argue that such an FM will open new and
efficient avenues for PL problem-solving, just like LLMs are creating for APS.",http://arxiv.org/abs/2404.04540v1,"Biplav Srivastava, Vishal Pallagani",['plan-generation']
DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models,"Recent advancements in Large Language Models (LLMs) have sparked a revolution
across many research fields. In robotics, the integration of common-sense
knowledge from LLMs into task and motion planning has drastically advanced the
field by unlocking unprecedented levels of context awareness. Despite their
vast collection of knowledge, large language models may generate infeasible
plans due to hallucinations or missing domain information. To address these
challenges and improve plan feasibility and computational efficiency, we
introduce DELTA, a novel LLM-informed task planning approach. By using scene
graphs as environment representations within LLMs, DELTA achieves rapid
generation of precise planning problem descriptions. To enhance planning
performance, DELTA decomposes long-term task goals with LLMs into an
autoregressive sequence of sub-goals, enabling automated task planners to
efficiently solve complex problems. In our extensive evaluation, we show that
DELTA enables an efficient and fully automatic task planning pipeline,
achieving higher planning success rates and significantly shorter planning
times compared to the state of the art.",http://arxiv.org/abs/2404.03275v2,"Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello","['model-construction', 'goal-decomposition']"
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches,"Task and Motion Planning (TAMP) integrates high-level task planning and
low-level motion planning to equip robots with the autonomy to effectively
reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on
hybrid optimization approaches that define goal conditions via objective
functions and are capable of handling open-ended goals, robotic dynamics, and
physical interaction between the robot and the environment. Therefore,
optimization-based TAMP is particularly suited to solve highly complex,
contact-rich locomotion and manipulation problems. This survey provides a
comprehensive review on optimization-based TAMP, covering (i) planning domain
representations, including action description languages and temporal logic,
(ii) individual solution strategies for components of TAMP, including AI
planning and trajectory optimization (TO), and (iii) the dynamic interplay
between logic-based task planning and model-based TO. A particular focus of
this survey is to highlight the algorithm structures to efficiently solve TAMP,
especially hierarchical and distributed approaches. Additionally, the survey
emphasizes the synergy between the classical methods and contemporary
learning-based innovations such as large language models. Furthermore, the
future research directions for TAMP is discussed in this survey, highlighting
both algorithmic and application-specific challenges.",http://arxiv.org/abs/2404.02817v4,"Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao",['plan-generation']
Large Language Models as Planning Domain Generators,"Developing domain models is one of the few remaining places that require
manual human labor in AI planning. Thus, in order to make planning more
accessible, it is desirable to automate the process of domain model generation.
To this end, we investigate if large language models (LLMs) can be used to
generate planning domain models from simple textual descriptions. Specifically,
we introduce a framework for automated evaluation of LLM-generated domains by
comparing the sets of plans for domain instances. Finally, we perform an
empirical analysis of 7 large language models, including coding and chat models
across 9 different planning domains, and under three classes of natural
language domain descriptions. Our results indicate that LLMs, particularly
those with high parameter counts, exhibit a moderate level of proficiency in
generating correct planning domains from natural language descriptions. Our
code is available at https://github.com/IBM/NL2PDDL.",http://arxiv.org/abs/2405.06650v1,"James Oswald, Kavitha Srinivas, Harsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi","['language-translation', 'model-construction']"
TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,"Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep",http://arxiv.org/abs/2403.17246v1,"Ishika Singh, David Traum, Jesse Thomason",['goal-decomposition']
LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,"Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.",http://arxiv.org/abs/2403.11552v3,"Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu","['plan-generation', 'interactive-planning']"
Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping,"While Transformers have enabled tremendous progress in various application
settings, such architectures still trail behind traditional symbolic planners
for solving complex decision making tasks. In this work, we demonstrate how to
train Transformers to solve complex planning tasks. This is accomplished by
training an encoder-decoder Transformer model to predict the search dynamics of
the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a
Transformer model that optimally solves previously unseen Sokoban puzzles 93.7%
of the time, while using up to 26.8% fewer search steps than the $A^*$
implementation that was used for training initially. In our training method,
$A^*$'s search dynamics are expressed as a token sequence outlining when task
states are added and removed into the search tree during symbolic planning.
Searchformer significantly outperforms baselines that predict the optimal plan
directly with a 5-10$\times$ smaller model size and a 10$\times$ smaller
training dataset. Lastly, we demonstrate how Searchformer scales to larger and
more complex decision making tasks with improved percentage of solved tasks and
shortened search dynamics.",http://arxiv.org/abs/2402.14083v2,"Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, Yuandong Tian",['plan-generation']
AutoGPT+P: Affordance-based Task Planning with Large Language Models,"Recent advances in task planning leverage Large Language Models (LLMs) to
improve generalizability by combining such models with classical planning
algorithms to address their inherent limitations in reasoning capabilities.
However, these approaches face the challenge of dynamically capturing the
initial state of the task planning problem. To alleviate this issue, we propose
AutoGPT+P, a system that combines an affordance-based scene representation with
a planning system. Affordances encompass the action possibilities of an agent
on the environment and objects present in it. Thus, deriving the planning
domain from an affordance-based scene representation allows symbolic planning
with arbitrary objects. AutoGPT+P leverages this representation to derive and
execute a plan for a task specified by the user in natural language. In
addition to solving planning tasks under a closed-world assumption, AutoGPT+P
can also handle planning with incomplete information, e. g., tasks with missing
objects by exploring the scene, suggesting alternatives, or providing a partial
plan. The affordance-based scene representation combines object detection with
an automatically generated object-affordance-mapping using ChatGPT. The core
planning tool extends existing work by automatically correcting semantic and
syntactic errors. Our approach achieves a success rate of 98%, surpassing the
current 81% success rate of the current state-of-the-art LLM-based planning
method SayCan on the SayCan instruction set. Furthermore, we evaluated our
approach on our newly created dataset with 150 scenarios covering a wide range
of complex tasks with missing objects, achieving a success rate of 79% on our
dataset. The dataset and the code are publicly available at
https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.",http://arxiv.org/abs/2402.10778v2,"Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour",['tool-integration']
LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents,"Large language models (LLMs) have recently received considerable attention as
alternative solutions for task planning. However, comparing the performance of
language-oriented task planners becomes difficult, and there exists a dearth of
detailed exploration regarding the effects of various factors such as
pre-trained model selection and prompt construction. To address this, we
propose a benchmark system for automatically quantifying performance of task
planning for home-service embodied agents. Task planners are tested on two
pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of
Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform
extensive experiments with LLMs and prompts, and explore several enhancements
of the baseline planner. We expect that the proposed benchmark tool would
accelerate the development of language-oriented task planners.",http://arxiv.org/abs/2402.08178v1,"Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang",['plan-generation']
"TIC: Translate-Infer-Compile for accurate ""text to plan"" using LLMs and Logical Representations","We study the problem of generating plans for given natural language planning
task requests. On one hand, LLMs excel at natural language processing but do
not perform well on planning. On the other hand, classical planning tools excel
at planning tasks but require input in a structured language such as the
Planning Domain Definition Language (PDDL). We leverage the strengths of both
the techniques by using an LLM for generating the PDDL representation (task
PDDL) of planning task requests followed by using a classical planner for
computing a plan. Unlike previous approaches that use LLMs for generating task
PDDLs directly, our approach comprises of (a) translate: using an LLM only for
generating a logically interpretable intermediate representation of natural
language task description, (b) infer: deriving additional logically dependent
information from the intermediate representation using a logic reasoner
(currently, Answer Set Programming solver), and (c) compile: generating the
target task PDDL from the base and inferred information. We observe that using
an LLM to only output the intermediate representation significantly reduces LLM
errors. Consequently, TIC approach achieves, for at least one LLM, high
accuracy on task PDDL generation for all seven domains of our evaluation
dataset.",http://arxiv.org/abs/2402.06608v2,"Sudhir Agarwal, Anu Sreepathy",['language-translation']
Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,"The inherent probabilistic nature of Large Language Models (LLMs) introduces
an element of unpredictability, raising concerns about potential discrepancies
in their output. This paper introduces an innovative approach aims to generate
correct and optimal robotic task plans for diverse real-world demands and
scenarios. LLMs have been used to generate task plans, but they are unreliable
and may contain wrong, questionable, or high-cost steps. The proposed approach
uses LLM to generate a number of task plans as trees and amalgamates them into
a graph by removing questionable paths. Then an optimal task tree can be
retrieved to circumvent questionable and high-cost nodes, thereby improving
planning accuracy and execution efficiency. The approach is further improved by
incorporating a large knowledge network. Leveraging GPT-4 further, the
high-level task plan is converted into a low-level Planning Domain Definition
Language (PDDL) plan executable by a robot. Evaluation results highlight the
superior accuracy and efficiency of our approach compared to previous
methodologies in the field of task planning.",http://arxiv.org/abs/2401.07868v1,"Md Sadman Sakib, Yu Sun",['plan-generation']
"Large Language Models for Robotics: Opportunities, Challenges, and Perspectives","Large language models (LLMs) have undergone significant expansion and have
been increasingly integrated across various domains. Notably, in the realm of
robot task planning, LLMs harness their advanced reasoning and language
comprehension capabilities to formulate precise and efficient action plans
based on natural language instructions. However, for embodied tasks, where
robots interact with complex environments, text-only LLMs often face challenges
due to a lack of compatibility with robotic visual perception. This study
provides a comprehensive overview of the emerging integration of LLMs and
multimodal LLMs into various robotic tasks. Additionally, we propose a
framework that utilizes multimodal GPT-4V to enhance embodied task planning
through the combination of natural language instructions and robot visual
perceptions. Our results, based on diverse datasets, indicate that GPT-4V
effectively enhances robot performance in embodied tasks. This extensive survey
and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks
enriches the understanding of LLM-centric embodied intelligence and provides
forward-looking insights toward bridging the gap in Human-Robot-Environment
interaction.",http://arxiv.org/abs/2401.04334v1,"Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Huaqin Zhao, Zhengliang Liu, Haixing Dai, Lin Zhao, Bao Ge, Xiang Li, Tianming Liu, Shu Zhang",['plan-generation']
On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS),"Automated Planning and Scheduling is among the growing areas in Artificial
Intelligence (AI) where mention of LLMs has gained popularity. Based on a
comprehensive review of 126 papers, this paper investigates eight categories
based on the unique applications of LLMs in addressing various aspects of
planning problems: language translation, plan generation, model construction,
multi-agent planning, interactive planning, heuristics optimization, tool
integration, and brain-inspired planning. For each category, we articulate the
issues considered and existing gaps. A critical insight resulting from our
review is that the true potential of LLMs unfolds when they are integrated with
traditional symbolic planners, pointing towards a promising neuro-symbolic
approach. This approach effectively combines the generative aspects of LLMs
with the precision of classical planning methods. By synthesizing insights from
existing literature, we underline the potential of this integration to address
complex planning challenges. Our goal is to encourage the ICAPS community to
recognize the complementary strengths of LLMs and symbolic planners, advocating
for a direction in automated planning that leverages these synergistic
capabilities to develop more advanced and intelligent planning systems.",http://arxiv.org/abs/2401.02500v2,"Vishal Pallagani, Kaushik Roy, Bharath Muppasani, Francesco Fabiano, Andrea Loreggia, Keerthiram Murugesan, Biplav Srivastava, Francesca Rossi, Lior Horesh, Amit Sheth","['plan-generation', 'model-construction', 'heuristics-optimization', 'multiagent-planning', 'tool-integration', 'brain-inspired-planning', 'language-translation', 'interactive-planning']"
Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning,"In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa's superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.",http://arxiv.org/abs/2311.17842v2,"Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao",['plan-generation']
Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,"This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this union, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) -- an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.",http://arxiv.org/abs/2311.13720v2,"Turgay Caglar, Sirine Belhaj, Tathagata Chakraborti, Michael Katz, Sarath Sreedharan","['model-construction', 'heuristics-optimization']"
Physical Reasoning and Object Planning for Household Embodied Agents,"In this study, we explore the sophisticated domain of task planning for
robust household embodied agents, with a particular emphasis on the intricate
task of selecting substitute objects. We introduce the CommonSense Object
Affordance Task (COAT), a novel framework designed to analyze reasoning
capabilities in commonsense scenarios. This approach is centered on
understanding how these agents can effectively identify and utilize alternative
objects when executing household tasks, thereby offering insights into the
complexities of practical decision-making in real-world environments.Drawing
inspiration from human decision-making, we explore how large language models
tackle this challenge through three meticulously crafted commonsense
question-and-answer datasets, featuring refined rules and human annotations.
Our evaluation of state-of-the-art language models on these datasets sheds
light on three pivotal considerations: 1) aligning an object's inherent utility
with the task at hand, 2) navigating contextual dependencies (societal norms,
safety, appropriateness, and efficiency), and 3) accounting for the current
physical state of the object. To maintain accessibility, we introduce five
abstract variables reflecting an object's physical condition, modulated by
human insights to simulate diverse household scenarios. Our contributions
include insightful Object-Utility mappings addressing the first consideration
and two extensive QA datasets (15k and 130k questions) probing the intricacies
of contextual dependencies and object states. The datasets, along with our
findings, are accessible at: \url{https://github.com/com-phy-affordance/COAT}.
This research not only advances our understanding of physical commonsense
reasoning in language models but also paves the way for future improvements in
household agent intelligence.",http://arxiv.org/abs/2311.13577v1,"Ayush Agrawal, Raghav Prabhakar, Anirudh Goyal, Dianbo Liu",['replanning']
TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems,"Large Language Models (LLMs) have demonstrated proficiency in addressing
tasks that necessitate a combination of task planning and the usage of external
tools that require a blend of task planning and the utilization of external
tools, such as APIs. However, real-world complex systems present three
prevalent challenges concerning task planning and tool usage: (1) The real
system usually has a vast array of APIs, so it is impossible to feed the
descriptions of all APIs to the prompt of LLMs as the token length is limited;
(2) the real system is designed for handling complex tasks, and the base LLMs
can hardly plan a correct sub-task order and API-calling order for such tasks;
(3) Similar semantics and functionalities among APIs in real systems create
challenges for both LLMs and even humans in distinguishing between them. In
response, this paper introduces a comprehensive framework aimed at enhancing
the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating
within real-world systems. Our framework comprises three key components
designed to address these challenges: (1) the API Retriever selects the most
pertinent APIs for the user task among the extensive array available; (2) LLM
Finetuner tunes a base LLM so that the finetuned LLM can be more capable for
task planning and API calling; (3) the Demo Selector adaptively retrieves
different demonstrations related to hard-to-distinguish APIs, which is further
used for in-context learning to boost the final performance. We validate our
methods using a real-world commercial system as well as an open-sourced
academic dataset, and the outcomes clearly showcase the efficacy of each
individual component as well as the integrated framework.",http://arxiv.org/abs/2311.11315v1,"Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao","['tool-integration', 'plan-generation']"
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,"LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present
AutoPlanBench, a novel method for automatically converting planning benchmarks
written in PDDL into textual descriptions and offer a benchmark dataset created
with our method. We show that while the best LLM planners do well on some
planning tasks, others remain out of reach of current methods.",http://arxiv.org/abs/2311.09830v2,"Katharina Stein, Daniel Fier, Jrg Hoffmann, Alexander Koller","['plan-generation', 'interactive-planning']"
Symbolic Planning and Code Generation for Grounded Dialogue,"Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.",http://arxiv.org/abs/2310.17140v1,"Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried",['tool-integration']
From Cooking Recipes to Robot Task Trees -- Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,"Task planning for robotic cooking involves generating a sequence of actions
for a robot to prepare a meal successfully. This paper introduces a novel task
tree generation pipeline producing correct planning and efficient execution for
cooking tasks. Our method first uses a large language model (LLM) to retrieve
recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a
task tree, capturing sequential and parallel dependencies among subtasks. The
pipeline then mitigates the uncertainty and unreliable features of LLM outputs
using task tree retrieval. We combine multiple LLM task tree outputs into a
graph and perform a task tree retrieval to avoid questionable nodes and
high-cost nodes to improve planning correctness and improve execution
efficiency. Our evaluation results show its superior performance compared to
previous works in task planning accuracy and efficiency.",http://arxiv.org/abs/2309.09181v1,"Md Sadman Sakib, Yu Sun",['plan-generation']
Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations,"Recommender models excel at providing domain-specific item recommendations by
leveraging extensive user behavior data. Despite their ability to act as
lightweight domain experts, they struggle to perform versatile tasks such as
providing explanations and engaging in conversations. On the other hand, large
language models (LLMs) represent a significant step towards artificial general
intelligence, showcasing remarkable capabilities in instruction comprehension,
commonsense reasoning, and human interaction. However, LLMs lack the knowledge
of domain-specific item catalogs and behavioral patterns, particularly in areas
that diverge from general world knowledge, such as online e-commerce.
Finetuning LLMs for each domain is neither economic nor efficient.
  In this paper, we bridge the gap between recommender models and LLMs,
combining their respective strengths to create a versatile and interactive
recommender system. We introduce an efficient framework called
\textbf{InteRecAgent}, which employs LLMs as the brain and recommender models
as tools. We first outline a minimal set of essential tools required to
transform LLMs into InteRecAgent. We then propose an efficient workflow within
InteRecAgent for task execution, incorporating key components such as memory
components, dynamic demonstration-augmented task planning, and reflection.
InteRecAgent enables traditional recommender systems, such as those ID-based
matrix factorization models, to become interactive systems with a natural
language interface through the integration of LLMs. Experimental results on
several public datasets show that InteRecAgent achieves satisfying performance
as a conversational recommender system, outperforming general-purpose LLMs. The
source code of InteRecAgent is released at https://aka.ms/recagent.",http://arxiv.org/abs/2308.16505v3,"Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie",['tool-integration']
ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,"Motivated by the substantial achievements observed in Large Language Models
(LLMs) in the field of natural language processing, recent research has
commenced investigations into the application of LLMs for complex, long-horizon
sequential task planning challenges in robotics. LLMs are advantageous in
offering the potential to enhance the generalizability as task-agnostic
planners and facilitate flexible interaction between human instructors and
planning systems. However, task plans generated by LLMs often lack feasibility
and correctness. To address this challenge, we introduce ISR-LLM, a novel
framework that improves LLM-based planning through an iterative self-refinement
process. The framework operates through three sequential steps: preprocessing,
planning, and iterative self-refinement. During preprocessing, an LLM
translator is employed to convert natural language input into a Planning Domain
Definition Language (PDDL) formulation. In the planning phase, an LLM planner
formulates an initial plan, which is then assessed and refined in the iterative
self-refinement step by using a validator. We examine the performance of
ISR-LLM across three distinct planning domains. The results show that ISR-LLM
is able to achieve markedly higher success rates in task accomplishments
compared to state-of-the-art LLM-based planners. Moreover, it also preserves
the broad applicability and generalizability of working with natural language
instructions.",http://arxiv.org/abs/2308.13724v1,"Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, Lei Ma","['model-construction', 'plan-generation', 'interactive-planning']"
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",http://arxiv.org/abs/2308.12682v2,"Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt","['plan-generation', 'heuristics-optimization']"
TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",http://arxiv.org/abs/2308.03427v3,"Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao","['tool-integration', 'plan-generation']"
New Interaction Paradigm for Complex EDA Software Leveraging GPT,"In the rapidly growing field of electronic design automation (EDA),
professional software such as KiCad, Cadence , and Altium Designer provide
increasingly extensive design functionalities. However, the intricate command
structure and high learning curve create a barrier, particularly for novice
printed circuit board (PCB) designers. This results in difficulties in
selecting appropriate functions or plugins for varying design purposes,
compounded by the lack of intuitive learning methods beyond traditional
documentation, videos, and online forums. To address this challenge, an
artificial intelligence (AI) interaction assist plugin for EDA software named
SmartonAl is developed here, also KiCad is taken as the first example.
SmartonAI is inspired by the HuggingGPT framework and employs large language
models, such as GPT and BERT, to facilitate task planning and execution. On
receiving a designer request, SmartonAI conducts a task breakdown and
efficiently executes relevant subtasks, such as analysis of help documentation
paragraphs and execution of different plugins, along with leveraging the
built-in schematic and PCB manipulation functions in both SmartonAl itself and
software. Our preliminary results demonstrate that SmartonAI can significantly
streamline the PCB design process by simplifying complex commands into
intuitive language-based interactions. By harnessing the powerful language
capabilities of ChatGPT and the rich design functions of KiCad, the plugin
effectively bridges the gap between complex EDA software and user-friendly
interaction. Meanwhile, the new paradigm behind SmartonAI can also extend to
other complex software systems, illustrating the immense potential of
AI-assisted user interfaces in advancing digital interactions across various
domains.",http://arxiv.org/abs/2307.14740v1,"Boyu Han, Xinyu Wang, Yifan Wang, Junyu Yan, Yidong Tian",['tool-integration']
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.",http://arxiv.org/abs/2307.06135v2,"Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf","['plan-generation', 'interactive-planning']"
Grammar Prompting for Domain-Specific Language Generation with Large Language Models,"Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.",http://arxiv.org/abs/2305.19234v3,"Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, Yoon Kim","['language-translation', 'plan-generation']"
Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds,"Task planning systems have been developed to help robots use human knowledge
(about actions) to complete long-horizon tasks. Most of them have been
developed for ""closed worlds"" while assuming the robot is provided with
complete world knowledge. However, the real world is generally open, and the
robots frequently encounter unforeseen situations that can potentially break
the planner's completeness. Could we leverage the recent advances on
pre-trained Large Language Models (LLMs) to enable classical planning systems
to deal with novel situations?
  This paper introduces a novel framework, called COWP, for open-world task
planning and situation handling. COWP dynamically augments the robot's action
knowledge, including the preconditions and effects of actions, with
task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and
is grounded to specific domains via action knowledge. For systematic
evaluations, we collected a dataset that includes 1,085 execution-time
situations. Each situation corresponds to a state instance wherein a robot is
potentially unable to complete a task using a solution that normally works.
Experimental results show that our approach outperforms competitive baselines
from the literature in the success rate of service tasks. Additionally, we have
demonstrated COWP using a mobile manipulator. Supplementary materials are
available at: https://cowplanning.github.io/",http://arxiv.org/abs/2305.17590v2,"Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang",['model-construction']
Understanding the Capabilities of Large Language Models for Automated Planning,"Automated planning is concerned with developing efficient algorithms to
generate plans or sequences of actions to achieve a specific goal in a given
environment. Emerging Large Language Models (LLMs) can answer questions, write
high-quality programming code, and predict protein folding, showcasing their
versatility in solving various tasks beyond language-based problems. In this
paper, we aim to explore how LLMs can also be used for automated planning. To
do so, we seek to answer four key questions. Firstly, we want to understand the
extent to which LLMs can be used for plan generation. Secondly, we aim to
identify which pre-training data is most effective in facilitating plan
generation. Thirdly, we investigate whether fine-tuning or prompting is a more
effective approach for plan generation. Finally, we explore whether LLMs are
capable of plan generalization. By answering these questions, the study seeks
to shed light on the capabilities of LLMs in solving complex planning problems
and provide insights into the most effective approaches for using LLMs in this
context.",http://arxiv.org/abs/2305.16151v1,"Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, Andrea Loreggia",['plan-generation']
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",http://arxiv.org/abs/2305.14909v2,"Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati","['model-construction', 'interactive-planning', 'language-translation']"
Generalized Planning in PDDL Domains with Pretrained Large Language Models,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",http://arxiv.org/abs/2305.11014v2,"Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz",['plan-generation']
Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning,"Long-horizon task planning is essential for the development of intelligent
assistive and service robots. In this work, we investigate the applicability of
a smaller class of large language models (LLMs), specifically GPT-2, in robotic
task planning by learning to decompose tasks into subgoal specifications for a
planner to execute sequentially. Our method grounds the input of the LLM on the
domain that is represented as a scene graph, enabling it to translate human
requests into executable robot plans, thereby learning to reason over
long-horizon tasks, as encountered in the ALFRED benchmark. We compare our
approach with classical planning and baseline methods to examine the
applicability and generalizability of LLM-based planners. Our findings suggest
that the knowledge stored in an LLM can be effectively grounded to perform
long-horizon task planning, demonstrating the promising potential for the
future application of neuro-symbolic planning methods in robotics.",http://arxiv.org/abs/2305.07716v1,"Georgia Chalvatzaki, Ali Younes, Daljeet Nandha, An Le, Leonardo F. R. Ribeiro, Iryna Gurevych",['goal-decomposition']
LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,"Large language models (LLMs) have demonstrated remarkable zero-shot
generalization abilities: state-of-the-art chatbots can provide plausible
answers to many common questions that arise in daily life. However, so far,
LLMs cannot reliably solve long-horizon planning problems. By contrast,
classical planners, once a problem is given in a formatted way, can use
efficient search algorithms to quickly identify correct, or even optimal,
plans. In an effort to get the best of both worlds, this paper introduces
LLM+P, the first framework that incorporates the strengths of classical
planners into LLMs. LLM+P takes in a natural language description of a planning
problem, then returns a correct (or optimal) plan for solving that problem in
natural language. LLM+P does so by first converting the language description
into a file written in the planning domain definition language (PDDL), then
leveraging classical planners to quickly find a solution, and then translating
the found solution back into natural language. Along with LLM+P, we define a
diverse set of different benchmark problems taken from common planning
scenarios. Via a comprehensive set of experiments on these benchmark problems,
we find that LLM+P is able to provide optimal solutions for most problems,
while LLMs fail to provide even feasible plans for most problems.\footnote{The
code and results are publicly available at
https://github.com/Cranial-XIX/llm-pddl.git.",http://arxiv.org/abs/2304.11477v3,"Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone",['model-construction']
A Framework for Neurosymbolic Robot Action Planning using Large Language Models,"Symbolic task planning is a widely used approach to enforce robot autonomy
due to its ease of understanding and deployment in robot architectures.
However, techniques for symbolic task planning are difficult to scale in
real-world, human-robot collaboration scenarios because of the poor performance
in complex planning domains or when frequent re-planning is needed. We present
a framework, Teriyaki, specifically aimed at bridging the gap between symbolic
task planning and machine learning approaches. The rationale is training Large
Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner
compatible with the Planning Domain Definition Language (PDDL), and then
leveraging its generative capabilities to overcome a number of limitations
inherent to symbolic task planners. Potential benefits include (i) a better
scalability in so far as the planning domain complexity increases, since LLMs'
response time linearly scales with the combined length of the input and the
output, and (ii) the ability to synthesize a plan action-by-action instead of
end-to-end, making each action available for execution as soon as it is
generated instead of waiting for the whole plan to be available, which in turn
enables concurrent planning and execution. Recently, significant efforts have
been devoted by the research community to evaluate the cognitive capabilities
of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an
overall planning performance comparable to traditional planners in specific
planning domains, while leveraging LLMs capabilities to build a look-ahead
predictive planning model. Preliminary results in selected domains show that
our method can: (i) solve 95.5% of problems in a test data set of 1,000
samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic
planner; (iii) reduce average overall waiting times for a plan availability by
up to 61.4%",http://arxiv.org/abs/2303.00438v3,"Alessio Capitanelli, Fulvio Mastrogiovanni",['plan-generation']
"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
""$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect"" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",http://arxiv.org/abs/2302.01560v3,"Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang","['plan-generation', 'interactive-planning']"
Plansformer: Generating Symbolic Plans using Transformers,"Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain.",http://arxiv.org/abs/2212.08681v1,"Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, Andrea Loreggia",['plan-generation']
Robot Task Planning and Situation Handling in Open Worlds,"Automated task planning algorithms have been developed to help robots
complete complex tasks that require multiple actions. Most of those algorithms
have been developed for ""closed worlds"" assuming complete world knowledge is
provided. However, the real world is generally open, and the robots frequently
encounter unforeseen situations that can potentially break the planner's
completeness. This paper introduces a novel algorithm (COWP) for open-world
task planning and situation handling that dynamically augments the robot's
action knowledge with task-oriented common sense. In particular, common sense
is extracted from Large Language Models based on the current task at hand and
robot skills. For systematic evaluations, we collected a dataset that includes
561 execution-time situations in a dining domain, where each situation
corresponds to a state instance of a robot being potentially unable to complete
a task using a solution that normally works. Experimental results show that our
approach significantly outperforms competitive baselines from the literature in
the success rate of service tasks. Additionally, we have demonstrated COWP
using a mobile manipulator. Supplementary materials are available at:
https://cowplanning.github.io/",http://arxiv.org/abs/2210.01287v1,"Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Chad Esselink, Shiqi Zhang",['model-construction']
