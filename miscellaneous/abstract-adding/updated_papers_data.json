[
  {
    "title": "Llm+ p: Empowering large language models with optimal planning proficiency",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2304.11477",
    "authors": "Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter",
    "year": "2023",
    "abstract": "Large language models (LLMs) have demonstrated remarkable zero-shot\ngeneralization abilities: state-of-the-art chatbots can provide plausible\nanswers to many common questions that arise in daily life. However, so far,\nLLMs cannot reliably solve long-horizon planning problems. By contrast,\nclassical planners, once a problem is given in a formatted way, can use\nefficient search algorithms to quickly identify correct, or even optimal,\nplans. In an effort to get the best of both worlds, this paper introduces\nLLM+P, the first framework that incorporates the strengths of classical\nplanners into LLMs. LLM+P takes in a natural language description of a planning\nproblem, then returns a correct (or optimal) plan for solving that problem in\nnatural language. LLM+P does so by first converting the language description\ninto a file written in the planning domain definition language (PDDL), then\nleveraging classical planners to quickly find a solution, and then translating\nthe found solution back into natural language. Along with LLM+P, we define a\ndiverse set of different benchmark problems taken from common planning\nscenarios. Via a comprehensive set of experiments on these benchmark problems,\nwe find that LLM+P is able to provide optimal solutions for most problems,\nwhile LLMs fail to provide even feasible plans for most problems.\\footnote{The\ncode and results are publicly available at\nhttps://github.com/Cranial-XIX/llm-pddl.git."
  },
  {
    "title": "Translating natural language to planning goals with large-language models",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2302.05128",
    "authors": "Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold",
    "year": "2023",
    "abstract": "Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use."
  },
  {
    "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2305.14909",
    "authors": "Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "There is a growing interest in applying pre-trained large language models\n(LLMs) to planning problems. However, methods that use LLMs directly as\nplanners are currently impractical due to several factors, including limited\ncorrectness of plans, strong reliance on feedback from interactions with\nsimulators or even the actual environment, and the inefficiency in utilizing\nhuman feedback. In this work, we introduce a novel alternative paradigm that\nconstructs an explicit world (domain) model in planning domain definition\nlanguage (PDDL) and then uses it to plan with sound domain-independent\nplanners. To address the fact that LLMs may not generate a fully functional\nPDDL model initially, we employ LLMs as an interface between PDDL and sources\nof corrective feedback, such as PDDL validators and humans. For users who lack\na background in PDDL, we show that LLMs can translate PDDL into natural\nlanguage and effectively encode corrective feedback back to the underlying\ndomain model. Our framework not only enjoys the correctness guarantee offered\nby the external planners but also reduces human involvement by allowing users\nto correct domain models at the beginning, rather than inspecting and\ncorrecting (through interactive prompting) every generated plan as in previous\nwork. On two IPC domains and a Household domain that is more complicated than\ncommonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be\nleveraged to produce high-quality PDDL models for over 40 actions, and the\ncorrected PDDL models are then used to successfully solve 48 challenging\nplanning tasks. Resources, including the source code, are released at:\nhttps://guansuns.github.io/pages/llm-dm."
  },
  {
    "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2305.07716",
    "authors": "Chalvatzaki, Georgia and Younes, Ali and Nandha, Daljeet and Le, An Thai and Ribeiro, Leonardo FR and Gurevych, Iryna",
    "year": "2023",
    "abstract": "Long-horizon task planning is essential for the development of intelligent\nassistive and service robots. In this work, we investigate the applicability of\na smaller class of large language models (LLMs), specifically GPT-2, in robotic\ntask planning by learning to decompose tasks into subgoal specifications for a\nplanner to execute sequentially. Our method grounds the input of the LLM on the\ndomain that is represented as a scene graph, enabling it to translate human\nrequests into executable robot plans, thereby learning to reason over\nlong-horizon tasks, as encountered in the ALFRED benchmark. We compare our\napproach with classical planning and baseline methods to examine the\napplicability and generalizability of LLM-based planners. Our findings suggest\nthat the knowledge stored in an LLM can be effectively grounded to perform\nlong-horizon task planning, demonstrating the promising potential for the\nfuture application of neuro-symbolic planning methods in robotics."
  },
  {
    "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2307.07696",
    "authors": "Yang, Zhun and Ishay, Adam and Lee, Joohyung",
    "year": "2023",
    "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and\ngeneral, their reasoning ability is not at a level to compete with the best\nmodels trained for specific natural language reasoning problems. In this study,\nwe observe that a large language model can serve as a highly effective few-shot\nsemantic parser. It can convert natural language sentences into a logical form\nthat serves as input for answer set programs, a logic-based declarative\nknowledge representation formalism. The combination results in a robust and\ngeneral system that can handle multiple question-answering tasks without\nrequiring retraining for each new task. It only needs a few examples to guide\nthe LLM's adaptation to a specific task, along with reusable ASP knowledge\nmodules that can be applied to multiple tasks. We demonstrate that this method\nachieves state-of-the-art performance on several NLP benchmarks, including\nbAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot\nplanning tasks that an LLM alone fails to solve."
  },
  {
    "title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2306.12672",
    "authors": "Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B",
    "year": "2023",
    "abstract": "How does language inform our downstream thinking? In particular, how do\nhumans make meaning from language--and how can we leverage a theory of\nlinguistic meaning to build machines that think in more human-like ways? In\nthis paper, we propose rational meaning construction, a computational framework\nfor language-informed thinking that combines neural language models with\nprobabilistic models for rational inference. We frame linguistic meaning as a\ncontext-sensitive mapping from natural language into a probabilistic language\nof thought (PLoT)--a general-purpose symbolic substrate for generative world\nmodeling. Our architecture integrates two computational tools that have not\npreviously come together: we model thinking with probabilistic programs, an\nexpressive representation for commonsense reasoning; and we model meaning\nconstruction with large language models (LLMs), which support broad-coverage\ntranslation from natural language utterances to code expressions in a\nprobabilistic programming language. We illustrate our framework through\nexamples covering four core domains from cognitive science: probabilistic\nreasoning, logical and relational reasoning, visual and physical reasoning, and\nsocial reasoning. In each, we show that LLMs can generate context-sensitive\ntranslations that capture pragmatically-appropriate linguistic meanings, while\nBayesian inference with the generated programs supports coherent and robust\ncommonsense reasoning. We extend our framework to integrate\ncognitively-motivated symbolic modules (physics simulators, graphics engines,\nand planning algorithms) to provide a unified commonsense thinking interface\nfrom language. Finally, we explore how language can drive the construction of\nworld models themselves. We hope this work will provide a roadmap towards\ncognitive models and AI systems that synthesize the insights of both modern and\nclassical computational perspectives."
  },
  {
    "title": "There and back again: extracting formal domains for controllable neurosymbolic story authoring",
    "category": "language-translation",
    "link": "https://ojs.aaai.org/index.php/AIIDE/article/view/27502",
    "authors": "Kelly, Jack and Calderwood, Alex and Wardrip-Fruin, Noah and Mateas, Michael",
    "year": "2023",
    "abstract": "Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences."
  },
  {
    "title": "Neuro-Symbolic AI Approaches to Enhance Deep Neural Networks with Logical Reasoning and Knowledge Integration",
    "category": "language-translation",
    "link": "https://www.proquest.com/docview/2851056844",
    "authors": "Yang, Zhun",
    "year": "2023",
    "abstract": "One of the challenges in Artificial Intelligence (AI) is to integrate fast, automatic, and intuitive System-1 thinking with slow, deliberate, and logical System-2 thinking. While deep\nlearning approaches excel at perception tasks for System-1, their reasoning capabilities for\nSystem-2 are limited. Besides, deep learning approaches are usually data-hungry, hard to\nmake use of explicit knowledge, and struggling with interpretability and justification. This\ndissertation presents three neuro-symbolic AI approaches that integrate neural networks\n(NNs) with symbolic AI methods to address these issues.\nThe first approach presented in this dissertation is NeurASP, which combines NNs with\nAnswer Set Programming (ASP), a logic programming formalism. NeurASP provides an\neffective way to integrate sub-symbolic and symbolic computation by treating NN outputs\nas probability distributions over atomic facts in ASP. The explicit knowledge encoded in\nASP corrects mistakes in NN outputs and allows for better training with less data.\nTo avoid NeurASP’s bottleneck in symbolic computation, this dissertation presents a\nConstraint Loss via Straight-Through Estimators (CL-STE). CL-STE provides a systematic\nway to compile discrete logical constraints into a loss function over discretized NN outputs\nand scales significantly better than state-of-the-art neuro-symbolic methods. This dissertation also presents a finding when CL-STE was applied to Transformers. Transformers can\nbe extended with recurrence to enhance its power for multi-step reasoning. Such Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems\nwhile successfully addressing the symbol grounding problem.\nLastly, this dissertation addresses the limitation of pre-trained Large Language Models\n(LLMs) on multi-step logical reasoning problems with a dual-process neuro-symbolic reasoning system called LLM+ASP, where an LLM (e.g., GPT-3) serves as a highly effective\nfew-shot semantic parser that turns natural language sentences into a logical form that can\nbe used as input to ASP. LLM+ASP achieves state-of-the-art performance on several textual reasoning benchmarks and can handle robot planning tasks that an LLM alone fails to\nsolve."
  },
  {
    "title": "Text2motion: From natural language instructions to feasible plans",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2303.12153",
    "authors": "Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette",
    "year": "2023",
    "abstract": "We propose Text2Motion, a language-based planning framework enabling robots\nto solve sequential manipulation tasks that require long-horizon reasoning.\nGiven a natural language instruction, our framework constructs both a task- and\nmotion-level plan that is verified to reach inferred symbolic goals.\nText2Motion uses feasibility heuristics encoded in Q-functions of a library of\nskills to guide task planning with Large Language Models. Whereas previous\nlanguage-based planners only consider the feasibility of individual skills,\nText2Motion actively resolves geometric dependencies spanning skill sequences\nby performing geometric feasibility planning during its search. We evaluate our\nmethod on a suite of problems that require long-horizon reasoning,\ninterpretation of abstract goals, and handling of partial affordance\nperception. Our experiments show that Text2Motion can solve these challenging\nproblems with a success rate of 82%, while prior state-of-the-art\nlanguage-based planning methods only achieve 13%. Text2Motion thus provides\npromising generalization characteristics to semantically diverse sequential\nmanipulation tasks with geometric dependencies between skills."
  },
  {
    "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2309.12311",
    "authors": "Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce",
    "year": "2023",
    "abstract": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ ."
  },
  {
    "title": "From Cooking Recipes to Robot Task Trees--Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2309.09181",
    "authors": "Sakib, Md Sadman and Sun, Yu",
    "year": "2023",
    "abstract": "Task planning for robotic cooking involves generating a sequence of actions\nfor a robot to prepare a meal successfully. This paper introduces a novel task\ntree generation pipeline producing correct planning and efficient execution for\ncooking tasks. Our method first uses a large language model (LLM) to retrieve\nrecipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a\ntask tree, capturing sequential and parallel dependencies among subtasks. The\npipeline then mitigates the uncertainty and unreliable features of LLM outputs\nusing task tree retrieval. We combine multiple LLM task tree outputs into a\ngraph and perform a task tree retrieval to avoid questionable nodes and\nhigh-cost nodes to improve planning correctness and improve execution\nefficiency. Our evaluation results show its superior performance compared to\nprevious works in task planning accuracy and efficiency."
  },
  {
    "title": "OceanChat: Piloting Autonomous Underwater Vehicles in Natural Language",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2309.16052",
    "authors": "Yang, Ruochu and Hou, Mengxue and Wang, Junkai and Zhang, Fumin",
    "year": "2023",
    "abstract": "In the trending research of fusing Large Language Models (LLMs) and robotics,\nwe aim to pave the way for innovative development of AI systems that can enable\nAutonomous Underwater Vehicles (AUVs) to seamlessly interact with humans in an\nintuitive manner. We propose OceanChat, a system that leverages a closed-loop\nLLM-guided task and motion planning framework to tackle AUV missions in the\nwild. LLMs translate an abstract human command into a high-level goal, while a\ntask planner further grounds the goal into a task sequence with logical\nconstraints. To assist the AUV with understanding the task sequence, we utilize\na motion planner to incorporate real-time Lagrangian data streams received by\nthe AUV, thus mapping the task sequence into an executable motion plan.\nConsidering the highly dynamic and partially known nature of the underwater\nenvironment, an event-triggered replanning scheme is developed to enhance the\nsystem's robustness towards uncertainty. We also build a simulation platform\nHoloEco that generates photo-realistic simulation for a wide range of AUV\napplications. Experimental evaluation verifies that the proposed system can\nachieve improved performance in terms of both success rate and computation\ntime. Project website: \\url{https://sites.google.com/view/oceanchat}"
  },
  {
    "title": "Human-Assisted Continual Robot Learning with Foundation Models",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2309.14321",
    "authors": "Parakh, Meenal and Fong, Alisha and Simeonov, Anthony and Gupta, Abhishek and Chen, Tao and Agrawal, Pulkit",
    "year": "2023",
    "abstract": "Large Language Models (LLMs) have been shown to act like planners that can\ndecompose high-level instructions into a sequence of executable instructions.\nHowever, current LLM-based planners are only able to operate with a fixed set\nof skills. We overcome this critical limitation and present a method for using\nLLM-based planners to query new skills and teach robots these skills in a data\nand time-efficient manner for rigid object manipulation. Our system can re-use\nnewly acquired skills for future tasks, demonstrating the potential of open\nworld and lifelong learning. We evaluate the proposed framework on multiple\ntasks in simulation and the real world. Videos are available at:\nhttps://sites.google.com/mit.edu/halp-robot-learning."
  },
  {
    "title": "Optimal Scene Graph Planning with Large Language Model Guidance",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2309.09182",
    "authors": "Dai, Zhirui and Asgharivaskasi, Arash and Duong, Thai and Lin, Shusen and Tzes, Maria-Elizabeth and Pappas, George and Atanasov, Nikolay",
    "year": "2023",
    "abstract": "Recent advances in metric, semantic, and topological mapping have equipped\nautonomous robots with semantic concept grounding capabilities to interpret\nnatural language tasks. This work aims to leverage these new capabilities with\nan efficient task planning algorithm for hierarchical metric-semantic models.\nWe consider a scene graph representation of the environment and utilize a large\nlanguage model (LLM) to convert a natural language task into a linear temporal\nlogic (LTL) automaton. Our main contribution is to enable optimal hierarchical\nLTL planning with LLM guidance over scene graphs. To achieve efficiency, we\nconstruct a hierarchical planning domain that captures the attributes and\nconnectivity of the scene graph and the task automaton, and provide semantic\nguidance via an LLM heuristic function. To guarantee optimality, we design an\nLTL heuristic function that is provably consistent and supplements the\npotentially inadmissible LLM guidance in multi-heuristic planning. We\ndemonstrate efficient planning of complex natural language tasks in scene\ngraphs of virtualized real environments."
  },
  {
    "title": "Vision-Language Interpreter for Robot Task Planning",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2311.00967",
    "authors": "Shirai, Keisuke and Beltran-Hernandez, Cristian C and Hamaya, Masashi and Hashimoto, Atsushi and Tanaka, Shohei and Kawaharazuka, Kento and Tanaka, Kazutoshi and Ushiku, Yoshitaka and Mori, Shinsuke",
    "year": "2023",
    "abstract": "Large language models (LLMs) are accelerating the development of\nlanguage-guided robot planners. Meanwhile, symbolic planners offer the\nadvantage of interpretability. This paper proposes a new task that bridges\nthese two trends, namely, multimodal planning problem specification. The aim is\nto generate a problem description (PD), a machine-readable file used by the\nplanners to find a plan. By generating PDs from language instruction and scene\nobservation, we can drive symbolic planners in a language-guided framework. We\npropose a Vision-Language Interpreter (ViLaIn), a new framework that generates\nPDs using state-of-the-art LLM and vision-language models. ViLaIn can refine\ngenerated PDs via error message feedback from the symbolic planner. Our aim is\nto answer the question: How accurately can ViLaIn and the symbolic planner\ngenerate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset\ncalled the problem description generation (ProDG) dataset. The framework is\nevaluated with four new evaluation metrics. Experimental results show that\nViLaIn can generate syntactically correct problems with more than 99\\% accuracy\nand valid plans with more than 58\\% accuracy. Our code and dataset are\navailable at https://github.com/omron-sinicx/ViLaIn."
  },
  {
    "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning",
    "category": "language-translation",
    "link": "https://openreview.net/pdf?id=LMiTmpZgSZ",
    "authors": "Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi",
    "year": "2023",
    "abstract": "Multi-object rearrangement is a crucial skill for\nservice robots, and commonsense reasoning is frequently needed\nin this process. However, achieving commonsense arrangements\nrequires knowledge about objects, which is hard to transfer to\nrobots. Large language models (LLMs) are one potential source of\nthis knowledge, but they do not naively capture information about\nplausible physical arrangements of the world. We propose LLMGROP, which uses prompting to extract commonsense knowledge\nabout semantically valid object configurations from an LLM\nand instantiates them with a task and motion planner in order\nto generalize to varying scene geometry. LLM-GROP allows\nus to go from natural-language commands to human-aligned\nobject rearrangement in varied environments. Based on human\nevaluations, our approach achieves the highest rating while\noutperforming competitive baselines in terms of success rate\nwhile maintaining comparable cumulative action costs. Finally,\nwe demonstrate a practical implementation of LLM-GROP on\na mobile manipulator in real-world scenarios. Supplementary\nmaterials are available at: https://sites.google.com/view/llm-grop"
  },
  {
    "title": "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2212.10561",
    "authors": "Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah and Haber, Nick",
    "year": "2023",
    "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs.\nWith Parsel, we automatically decompose algorithmic tasks into hierarchical\nnatural language function descriptions and then search over combinations of\npossible function implementations using tests. We show that Parsel can be used\nacross domains requiring hierarchical reasoning, including program synthesis\nand robotic planning. We find that, using Parsel, LLMs solve more\ncompetition-level problems in the APPS dataset, resulting in pass rates over\n75\\% higher than prior results from directly sampling AlphaCode and Codex,\nwhile often using a smaller sample budget. Moreover, with automatically\ngenerated tests, we find that Parsel can improve the state-of-the-art pass@1\nperformance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated\nrobotic plans using Parsel are more than twice as likely to be considered\naccurate than directly generated plans. Lastly, we explore how Parsel addresses\nLLM limitations and discuss how Parsel may be useful for human programmers. We\nrelease our code at https://github.com/ezelikman/parsel"
  },
  {
    "title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2305.12295",
    "authors": "Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang",
    "year": "2023",
    "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but\nstill struggle with complex logical problems. This paper introduces a novel\nframework, Logic-LM, which integrates LLMs with symbolic solvers to improve\nlogical problem-solving. Our method first utilizes LLMs to translate a natural\nlanguage problem into a symbolic formulation. Afterward, a deterministic\nsymbolic solver performs inference on the formulated problem. We also introduce\na self-refinement module, which utilizes the symbolic solver's error messages\nto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on\nfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,\nLogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant\nperformance boost of 39.2% over using LLM alone with standard prompting and\n18.4% over LLM with chain-of-thought prompting. Our findings suggest that\nLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue for\nfaithful logical reasoning. Code and data are publicly available at\nhttps://github.com/teacherpeterpan/Logic-LLM."
  },
  {
    "title": "Creative Robot Tool Use with Large Language Models",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2204.01691",
    "authors": "Xu, Mengdi and Huang, Peide and Yu, Wenhao and Liu, Shiqi and Zhang, Xilun and Niu, Yaru and Zhang, Tingnan and Xia, Fei and Tan, Jie and Zhao, Ding",
    "year": "2023",
    "abstract": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/."
  },
  {
    "title": "Do as i can, not as i say: Grounding language in robotic affordances",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2204.01691",
    "authors": "Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others",
    "year": "2023",
    "abstract": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at [this https URL](https://say-can.github.io/).\n"
  },
  {
    "title": "Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2212.01944",
    "authors": "Yang, Yunhao and Gaglione, Jean-Raphael and Topcu, Ufuk",
    "year": "2022",
    "abstract": "Automaton-based representations of task knowledge play an important role in\ncontrol and planning for sequential decision-making problems. However,\nobtaining the high-level task knowledge required to build such automata is\noften difficult. Meanwhile, large-scale generative language models (GLMs) can\nautomatically generate relevant task knowledge. However, the textual outputs\nfrom GLMs cannot be formally verified or used for sequential decision-making.\nWe propose a novel algorithm named GLM2FSA, which constructs a finite state\nautomaton (FSA) encoding high-level task knowledge from a brief\nnatural-language description of the task goal. GLM2FSA first sends queries to a\nGLM to extract task knowledge in textual form, and then it builds an FSA to\nrepresent this text-based knowledge. The proposed algorithm thus fills the gap\nbetween natural-language task descriptions and automaton-based representations,\nand the constructed FSA can be formally verified against user-defined\nspecifications. We accordingly propose a method to iteratively refine the\nqueries to the GLM based on the outcomes, e.g., counter-examples, from\nverification. We demonstrate GLM2FSA's ability to build and refine\nautomaton-based representations of everyday tasks (e.g., crossing a road), and\nalso of tasks that require highly-specialized knowledge (e.g., executing secure\nmulti-party computation)."
  },
  {
    "title": "Open-vocabulary queryable scene representations for real world planning",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2209.09874",
    "authors": "Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S and Stone, Austin and Kappler, Daniel",
    "year": "2023",
    "abstract": "Large language models (LLMs) have unlocked new capabilities of task planning\nfrom human instructions. However, prior attempts to apply LLMs to real-world\nrobotic tasks are limited by the lack of grounding in the surrounding scene. In\nthis paper, we develop NLMap, an open-vocabulary and queryable scene\nrepresentation to address this problem. NLMap serves as a framework to gather\nand integrate contextual information into LLM planners, allowing them to see\nand query available objects in the scene before generating a\ncontext-conditioned plan. NLMap first establishes a natural language queryable\nscene representation with Visual Language models (VLMs). An LLM based object\nproposal module parses instructions and proposes involved objects to query the\nscene representation for object availability and location. An LLM planner then\nplans with such information about the scene. NLMap allows robots to operate\nwithout a fixed list of objects nor executable options, enabling real robot\noperation unachievable by previous methods. Project website:\nhttps://nlmap-saycan.github.io"
  },
  {
    "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
    "category": "language-translation",
    "link": "https://arxiv.org/abs/2310.08185",
    "authors": "You, Wang and Wu, Wenshan and Liang, Yaobo and Mao, Shaoguang and Wu, Chenfei and Cao, Maosong and Cai, Yuzhe and Guo, Yiduo and Xia, Yan and Wei, Furu and others",
    "year": "2023",
    "abstract": "Plan-and-Write is a common hierarchical approach in long-form narrative text\ngeneration, which first creates a plan to guide the narrative writing.\nFollowing this approach, several studies rely on simply prompting large\nlanguage models for planning, which often yields suboptimal results. In this\npaper, we propose a new framework called Evaluation-guided Iterative Plan\nExtraction for long-form narrative text generation (EIPE-text), which extracts\nplans from the corpus of narratives and utilizes the extracted plans to\nconstruct a better planner. EIPE-text has three stages: plan extraction,\nlearning, and inference. In the plan extraction stage, it iteratively extracts\nand improves plans from the narrative corpus and constructs a plan corpus. We\npropose a question answer (QA) based evaluation mechanism to automatically\nevaluate the plans and generate detailed plan refinement instructions to guide\nthe iterative improvement. In the learning stage, we build a better planner by\nfine-tuning with the plan corpus or in-context learning with examples in the\nplan corpus. Finally, we leverage a hierarchical approach to generate long-form\nnarratives. We evaluate the effectiveness of EIPE-text in the domains of novels\nand storytelling. Both GPT-4-based evaluations and human evaluations\ndemonstrate that our method can generate more coherent and relevant long-form\nnarratives. Our code will be released in the future."
  },
  {
    "title": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2311.00899",
    "authors": "Sermanet, Pierre and Ding, Tianli and Zhao, Jeffrey and Xia, Fei and Dwibedi, Debidatta and Gopalakrishnan, Keerthana and Chan, Christine and Dulac-Arnold, Gabriel and Maddineni, Sharath and Joshi, Nikhil J and others",
    "year": "2023",
    "abstract": "We present a scalable, bottom-up and intrinsically diverse data collection\nscheme that can be used for high-level reasoning with long and medium horizons\nand that has 2.2x higher throughput compared to traditional narrow top-down\nstep-by-step collection. We collect realistic data by performing any user\nrequests within the entirety of 3 office buildings and using multiple robot and\nhuman embodiments. With this data, we show that models trained on all\nembodiments perform better than ones trained on the robot data only, even when\nevaluated solely on robot episodes. We find that for a fixed collection budget\nit is beneficial to take advantage of cheaper human collection along with robot\ncollection. We release a large and highly diverse (29,520 unique instructions)\ndataset dubbed RoboVQA containing 829,502 (video, text) pairs for\nrobotics-focused visual question answering. We also demonstrate how evaluating\nreal robot experiments with an intervention mechanism enables performing tasks\nto completion, making it deployable with human oversight even if imperfect\nwhile also providing a single performance metric. We demonstrate a single\nvideo-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is\ncapable of performing a variety of grounded high-level reasoning tasks in broad\nrealistic settings with a cognitive intervention rate 46% lower than the\nzero-shot state of the art visual language model (VLM) baseline and is able to\nguide real robots through long-horizon tasks. The performance gap with\nzero-shot state-of-the-art models indicates that a lot of grounded data remains\nto be collected for real-world deployment, emphasizing the critical need for\nscalable data collection approaches. Finally, we show that video VLMs\nsignificantly outperform single-image VLMs with an average error rate reduction\nof 19% across all VQA tasks. Data and videos available at\nhttps://robovqa.github.io"
  },
  {
    "title": "Human-Centered Planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2311.04403",
    "authors": "Li, Yuliang and Kamra, Nitin and Desai, Ruta and Halevy, Alon",
    "year": "2023",
    "abstract": "LLMs have recently made impressive inroads on tasks whose output is\nstructured, such as coding, robotic planning and querying databases. The vision\nof creating AI-powered personal assistants also involves creating structured\noutputs, such as a plan for one's day, or for an overseas trip. Here, since the\nplan is executed by a human, the output doesn't have to satisfy strict\nsyntactic constraints. A useful assistant should also be able to incorporate\nvague constraints specified by the user in natural language. This makes LLMs an\nattractive option for planning.\n  We consider the problem of planning one's day. We develop an LLM-based\nplanner (LLMPlan) extended with the ability to self-reflect on its output and a\nsymbolic planner (SymPlan) with the ability to translate text constraints into\na symbolic representation. Despite no formal specification of constraints, we\nfind that LLMPlan performs explicit constraint satisfaction akin to the\ntraditional symbolic planners on average (2% performance difference), while\nretaining the reasoning of implicit requirements. Consequently, LLM-based\nplanners outperform their symbolic counterparts in user satisfaction (70.5% vs.\n40.4%) during interactive evaluation with 40 users."
  },
  {
    "title": "Plansformer: Generating symbolic plans using transformers",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2212.08681",
    "authors": "Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Horesh, Lior and Srivastava, Biplav and Fabiano, Francesco and Loreggia, Andrea",
    "year": "2022",
    "abstract": "Large Language Models (LLMs) have been the subject of active research,\nsignificantly advancing the field of Natural Language Processing (NLP). From\nBERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural\nlanguage tasks such as question answering, summarization, and text generation.\nMany ongoing efforts focus on understanding LLMs' capabilities, including their\nknowledge of the world, syntax, and semantics. However, extending the textual\nprowess of LLMs to symbolic reasoning has been slow and predominantly focused\non tackling problems related to the mathematical field. In this paper, we\nexplore the use of LLMs for automated planning - a branch of AI concerned with\nthe realization of action sequences (plans) to achieve a goal, typically\nexecuted by intelligent agents, autonomous robots, and unmanned vehicles. We\nintroduce Plansformer; an LLM fine-tuned on planning problems and capable of\ngenerating plans with favorable behavior in terms of correctness and length\nwith reduced knowledge-engineering efforts. We also demonstrate the\nadaptability of Plansformer in solving different planning domains with varying\ncomplexities, owing to the transfer learning abilities of LLMs. For one\nconfiguration of Plansformer, we achieve ~97% valid plans, out of which ~95%\nare optimal for Towers of Hanoi - a puzzle-solving domain."
  },
  {
    "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.11014",
    "authors": "Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B and Kaelbling, Leslie Pack and Katz, Michael",
    "year": "2023",
    "abstract": "Recent work has considered whether large language models (LLMs) can function\nas planners: given a task, generate a plan. We investigate whether LLMs can\nserve as generalized planners: given a domain and training tasks, generate a\nprogram that efficiently produces plans for other tasks in the domain. In\nparticular, we consider PDDL domains and use GPT-4 to synthesize Python\nprograms. We also consider (1) Chain-of-Thought (CoT) summarization, where the\nLLM is prompted to summarize the domain and propose a strategy in words before\nsynthesizing the program; and (2) automated debugging, where the program is\nvalidated with respect to the training tasks, and in case of errors, the LLM is\nre-prompted with four types of feedback. We evaluate this approach in seven\nPDDL domains and compare it to four ablations and four baselines. Overall, we\nfind that GPT-4 is a surprisingly powerful generalized planner. We also\nconclude that automated debugging is very important, that CoT summarization has\nnon-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two\ntraining tasks are often sufficient for strong generalization."
  },
  {
    "title": "Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers",
    "category": "plan-generation",
    "link": "https://www.ijcai.org/proceedings/2023/0839.pdf",
    "authors": "Pallagani, Vishal and Muppasani, Bharath and Srivastava, Biplav and Rossi, Francesca and Horesh, Lior and Murugesan, Keerthiram and Loreggia, Andrea and Fabiano, Francesco and Joseph, Rony and Kethepalli, Yathin and others",
    "year": "2023",
    "abstract": "Plansformer is a novel tool that utilizes a fine-tuned\nlanguage model based on transformer architecture\nto generate symbolic plans. Transformers are a\ntype of neural network architecture that have been\nshown to be highly effective in a range of natural\nlanguage processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans\nthat are both fluent and feasible. Plansformer takes\nthe domain and problem files as input (in PDDL)\nand outputs a sequence of actions that can be executed to solve the problem. We demonstrate the\neffectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and\nquantitative results obtained during our evaluation,\nincluding its limitations. Plansformer has the potential to significantly improve the efficiency and\neffectiveness of planning in various domains, from\nlogistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer\nvia a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at\nhttps://youtu.be/_1rlctCGsrk."
  },
  {
    "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.14825",
    "authors": "Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan",
    "year": "2023",
    "abstract": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}."
  },
  {
    "title": "Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.17077",
    "authors": "Arora, Daman and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "There have been wide spread claims in the literature about the emergent\nreasoning capabilities of Pretrained Large Language Models. However, recent\nstudies, have found that their ability to plan remains questionable. Through\nour experiments using GPT-2, we empirically demonstrate that the performance of\na finetuned baseline remains poor because it violates pre-conditions of actions\nin the plans that it generates. To improve the planning capabilities of a\nfinetuned LLM, we train a verifier, which can classify actions as being valid\nor invalid in a particular state. By randomly sampling actions from the same\ndataset, we generate examples of invalid actions which are then used to train a\nverifier which can check for action applicability. In the presence of diverse\nsampling from a generator and a verifier which can prune invalid trajectories,\nwe show significant gains in the success rate on the Blocksworld domain.\nAdditionally, we show that finetuning the GPT-2 generator itself to create the\nverifier generalizes better than finetuning the base GPT-2. Lastly, we\ninvestigate the role of the sampling temperature which can be used to control\nthe exploration-exploitation tradeoff."
  },
  {
    "title": "Fast and Slow Planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2303.04283",
    "authors": "Fabiano, Francesco and Pallagani, Vishal and Ganapini, Marianna Bergamaschi and Horesh, Lior and Loreggia, Andrea and Murugesan, Keerthiram and Rossi, Francesca and Srivastava, Biplav",
    "year": "2023",
    "abstract": "The concept of Artificial Intelligence has gained a lot of attention over the\nlast decade. In particular, AI-based tools have been employed in several\nscenarios and are, by now, pervading our everyday life. Nonetheless, most of\nthese systems lack many capabilities that we would naturally consider to be\nincluded in a notion of \"intelligence\". In this work, we present an\narchitecture that, inspired by the cognitive theory known as Thinking Fast and\nSlow by D. Kahneman, is tasked with solving planning problems in different\nsettings, specifically: classical and multi-agent epistemic. The system\nproposed is an instance of a more general AI paradigm, referred to as SOFAI\n(for Slow and Fast AI). SOFAI exploits multiple solving approaches, with\ndifferent capabilities that characterize them as either fast or slow, and a\nmetacognitive module to regulate them. This combination of components, which\nroughly reflects the human reasoning process according to D. Kahneman, allowed\nus to enhance the reasoning process that, in this case, is concerned with\nplanning in two different settings. The behavior of this system is then\ncompared to state-of-the-art solvers, showing that the newly introduced system\npresents better results in terms of generality, solving a wider set of problems\nwith an acceptable trade-off between solving times and solution accuracy."
  },
  {
    "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.07716",
    "authors": "Chalvatzaki, Georgia and Younes, Ali and Nandha, Daljeet and Le, An Thai and Ribeiro, Leonardo FR and Gurevych, Iryna",
    "year": "2023",
    "abstract": "Long-horizon task planning is essential for the development of intelligent\nassistive and service robots. In this work, we investigate the applicability of\na smaller class of large language models (LLMs), specifically GPT-2, in robotic\ntask planning by learning to decompose tasks into subgoal specifications for a\nplanner to execute sequentially. Our method grounds the input of the LLM on the\ndomain that is represented as a scene graph, enabling it to translate human\nrequests into executable robot plans, thereby learning to reason over\nlong-horizon tasks, as encountered in the ALFRED benchmark. We compare our\napproach with classical planning and baseline methods to examine the\napplicability and generalizability of LLM-based planners. Our findings suggest\nthat the knowledge stored in an LLM can be effectively grounded to perform\nlong-horizon task planning, demonstrating the promising potential for the\nfuture application of neuro-symbolic planning methods in robotics."
  },
  {
    "title": "PDDL planning with pretrained large language models",
    "category": "plan-generation",
    "link": "https://openreview.net/pdf?id=1QMMUB4zfl",
    "authors": "Silver, Tom and Hariprasad, Varun and Shuttleworth, Reece S and Kumar, Nishanth and Lozano-P{\\'e}rez, Tom{\\'a}s and Kaelbling, Leslie Pack",
    "year": "2022",
    "abstract": "We study few-shot prompting of pretrained large language models (LLMs) towards solving PDDL planning problems. We are interested in two questions: (1)\nTo what extent can LLMs solve PDDL planning problems on their own? (2) How\nand to what extent can LLMs be used to guide AI planners? Recent work by\nValmeekam et al. (2022) presents negative evidence for (1) in the classic blocks\nworld domain. We confirm this finding, but expand the inquiry to 18 domains and\nfind more mixed results with a few clear successes. For (2), we propose a simple\nmechanism for using good-but-imperfect LLM outputs to aid a heuristic-search\nplanner. We also find that the LLM performance is due not only to syntactic pattern matching, but also to its commonsense understanding of English terms that\nappear in the PDDL. Code: https://tinyurl.com/llm4pddl"
  },
  {
    "title": "Reasoning with language model is planning with world model",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.14992",
    "authors": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
    "year": "2023",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting."
  },
  {
    "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.05252",
    "authors": "Yuan, Siyu and Chen, Jiangjie and Fu, Ziquan and Ge, Xuyang and Shah, Soham and Jankowski, Charles Robert and Yang, Deqing and Xiao, Yanghua",
    "year": "2023",
    "abstract": "In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability."
  },
  {
    "title": "Strategic Reasoning with Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.19165",
    "authors": "Gandhi, Kanishk and Sadigh, Dorsa and Goodman, Noah D",
    "year": "2023",
    "abstract": "Strategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios."
  },
  {
    "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2310.07263",
    "authors": "Joublin, Frank and Ceravola, Antonello and Smirnov, Pavel and Ocker, Felix and Deigmoeller, Joerg and Belardinelli, Anna and Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Gienger, Michael",
    "year": "2023",
    "abstract": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation."
  },
  {
    "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2306.06531",
    "authors": "Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu",
    "year": "2023",
    "abstract": "For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code."
  },
  {
    "title": "Planning with Logical Graph-based Language Model for Instruction Generation",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2308.13782",
    "authors": "Fan Zhang and Kebing Jin and Hankz Hankui Zhuo",
    "year": "2023",
    "abstract": "Despite the superior performance of large language models to generate natural\nlanguage texts, it is hard to generate texts with correct logic according to a\ngiven task, due to the difficulties for neural models to capture implied rules\nfrom free-form texts. In this paper, we propose a novel graph-based language\nmodel, Logical-GLM, to infuse logic into language models for more valid text\ngeneration and interpretability. Specifically, we first capture information\nfrom natural language instructions and construct logical bayes graphs that\ngenerally describe domains. Next, we generate logical skeletons to guide\nlanguage model training, infusing domain knowledge into language models.\nFinally, we alternately optimize the searching policy of graphs and language\nmodels until convergence. The experimental results show that Logical-GLM is\nboth effective and efficient compared with traditional language models, despite\nusing smaller-scale training data and fewer parameters. Our approach can\ngenerate instructional texts with more correct logic owing to the internalized\ndomain knowledge. Moreover, the usage of logical graphs reflects the inner\nmechanism of the language models, which improves the interpretability of\nblack-box models."
  },
  {
    "title": "A Framework to Generate Neurosymbolic PDDL-compliant Planners",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2303.00438",
    "authors": "Capitanelli, Alessio and Mastrogiovanni, Fulvio",
    "year": "2023",
    "abstract": "Symbolic task planning is a widely used approach to enforce robot autonomy\ndue to its ease of understanding and deployment in robot architectures.\nHowever, techniques for symbolic task planning are difficult to scale in\nreal-world, human-robot collaboration scenarios because of the poor performance\nin complex planning domains or when frequent re-planning is needed. We present\na framework, Teriyaki, specifically aimed at bridging the gap between symbolic\ntask planning and machine learning approaches. The rationale is training Large\nLanguage Models (LLMs), namely GPT-3, into a neurosymbolic task planner\ncompatible with the Planning Domain Definition Language (PDDL), and then\nleveraging its generative capabilities to overcome a number of limitations\ninherent to symbolic task planners. Potential benefits include (i) a better\nscalability in so far as the planning domain complexity increases, since LLMs'\nresponse time linearly scales with the combined length of the input and the\noutput, and (ii) the ability to synthesize a plan action-by-action instead of\nend-to-end, making each action available for execution as soon as it is\ngenerated instead of waiting for the whole plan to be available, which in turn\nenables concurrent planning and execution. Recently, significant efforts have\nbeen devoted by the research community to evaluate the cognitive capabilities\nof LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an\noverall planning performance comparable to traditional planners in specific\nplanning domains, while leveraging LLMs capabilities to build a look-ahead\npredictive planning model. Preliminary results in selected domains show that\nour method can: (i) solve 95.5% of problems in a test data set of 1,000\nsamples; (ii) produce plans up to 13.5% shorter than a traditional symbolic\nplanner; (iii) reduce average overall waiting times for a plan availability by\nup to 61.4%"
  },
  {
    "title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.01868",
    "authors": "Yang, Yunhao and Tomar, Anshul",
    "year": "2023",
    "abstract": "The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities."
  },
  {
    "title": "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2212.04088",
    "authors": "Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu",
    "year": "2023",
    "abstract": "This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner"
  },
  {
    "title": "Dynamic Planning with a LLM",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2308.06391",
    "authors": "Dagan, Gautier and Keller, Frank and Lascarides, Alex",
    "year": "2023",
    "abstract": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline."
  },
  {
    "title": "Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.15091",
    "authors": "Lin, Han and Zala, Abhay and Cho, Jaemin and Bansal, Mohit",
    "year": "2023",
    "abstract": "Recent text-to-video (T2V) generation methods have seen significant\nadvancements. However, the majority of these works focus on producing short\nvideo clips of a single event (i.e., single-scene videos). Meanwhile, recent\nlarge language models (LLMs) have demonstrated their capability in generating\nlayouts and programs to control downstream visual modules. This prompts an\nimportant question: can we leverage the knowledge embedded in these LLMs for\ntemporally consistent long video generation? In this paper, we propose\nVideoDirectorGPT, a novel framework for consistent multi-scene video generation\nthat uses the knowledge of LLMs for video content planning and grounded video\ngeneration. Specifically, given a single text prompt, we first ask our video\nplanner LLM (GPT-4) to expand it into a 'video plan', which includes the scene\ndescriptions, the entities with their respective layouts, the background for\neach scene, and consistency groupings of the entities. Next, guided by this\nvideo plan, our video generator, named Layout2Vid, has explicit control over\nspatial layouts and can maintain temporal consistency of entities across\nmultiple scenes, while being trained only with image-level annotations. Our\nexperiments demonstrate that our proposed VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with\nconsistency, while achieving competitive performance with SOTAs in open-domain\nsingle-scene T2V generation. Detailed ablation studies, including dynamic\nadjustment of layout control strength with an LLM and video generation with\nuser-provided images, confirm the effectiveness of each component of our\nframework and its future potential."
  },
  {
    "title": "On the planning abilities of large language models (a critical investigation with a proposed benchmark)",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2302.06706",
    "authors": "Valmeekam, Karthik and Sreedharan, Sarath and Marquez, Matthew and Olmo, Alberto and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) how good LLMs are by themselves in\ngenerating and validating simple plans in commonsense planning tasks (of the\ntype that humans are generally quite good at) and (2) how good LLMs are in\nbeing a source of heuristic guidance for other agents--either AI planners or\nhuman planners--in their planning tasks. To investigate these questions in a\nsystematic rather than anecdotal manner, we start by developing a benchmark\nsuite based on the kinds of domains employed in the International Planning\nCompetition. On this benchmark, we evaluate LLMs in three modes: autonomous,\nheuristic and human-in-the-loop. Our results show that LLM's ability to\nautonomously generate executable plans is quite meager, averaging only about 3%\nsuccess rate. The heuristic and human-in-the-loop modes show slightly more\npromise. In addition to these results, we also make our benchmark and\nevaluation tools available to support investigations by research community."
  },
  {
    "title": "Sayplan: Grounding large language models using 3d scene graphs for scalable task planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2307.06135",
    "authors": "Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko",
    "year": "2023",
    "abstract": "Large language models (LLMs) have demonstrated impressive results in\ndeveloping generalist planning agents for diverse tasks. However, grounding\nthese plans in expansive, multi-floor, and multi-room environments presents a\nsignificant challenge for robotics. We introduce SayPlan, a scalable approach\nto LLM-based, large-scale task planning for robotics using 3D scene graph\n(3DSG) representations. To ensure the scalability of our approach, we: (1)\nexploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic\nsearch' for task-relevant subgraphs from a smaller, collapsed representation of\nthe full graph; (2) reduce the planning horizon for the LLM by integrating a\nclassical path planner and (3) introduce an 'iterative replanning' pipeline\nthat refines the initial plan using feedback from a scene graph simulator,\ncorrecting infeasible actions and avoiding planning failures. We evaluate our\napproach on two large-scale environments spanning up to 3 floors and 36 rooms\nwith 140 assets and objects and show that our approach is capable of grounding\nlarge-scale, long-horizon task plans from abstract, and natural language\ninstruction for a mobile manipulator robot to execute. We provide real robot\nvideo demonstrations on our project page https://sayplan.github.io."
  },
  {
    "title": "ProgPrompt: program generation for situated robot task planning using large language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2209.11302",
    "authors": "Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh",
    "year": "2023",
    "abstract": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io"
  },
  {
    "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2302.01560",
    "authors": "Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao",
    "year": "2023",
    "abstract": "We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https://github.com/CraftJarvis/MC-Planner."
  },
  {
    "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2201.07207",
    "authors": "Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor",
    "year": "2022",
    "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner"
  },
  {
    "title": "Saynav: Grounding large language models for dynamic planning to navigation in new environments",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.04077",
    "authors": "Rajvanshi, Abhinav and Sikka, Karan and Lin, Xiao and Lee, Bhoram and Chiu, Han-Pang and Velasquez, Alvaro",
    "year": "2023",
    "abstract": "Semantic reasoning and dynamic planning capabilities are crucial for an\nautonomous agent to perform complex navigation tasks in unknown environments.\nIt requires a large amount of common-sense knowledge, that humans possess, to\nsucceed in these tasks. We present SayNav, a new approach that leverages human\nknowledge from Large Language Models (LLMs) for efficient generalization to\ncomplex navigation tasks in unknown large-scale environments. SayNav uses a\nnovel grounding mechanism, that incrementally builds a 3D scene graph of the\nexplored environment as inputs to LLMs, for generating feasible and\ncontextually appropriate high-level plans for navigation. The LLM-generated\nplan is then executed by a pre-trained low-level planner, that treats each\nplanned step as a short-distance point-goal navigation sub-task. SayNav\ndynamically generates step-by-step instructions during navigation and\ncontinuously refines future steps based on newly perceived information. We\nevaluate SayNav on multi-object navigation (MultiON) task, that requires the\nagent to utilize a massive amount of human knowledge to efficiently search\nmultiple different objects in an unknown environment. We also introduce a\nbenchmark dataset for MultiON task employing ProcTHOR framework that provides\nlarge photo-realistic indoor environments with variety of objects. SayNav\nachieves state-of-the-art results and even outperforms an oracle based baseline\nwith strong ground-truth assumptions by more than 8% in terms of success rate,\nhighlighting its ability to generate dynamic plans for successfully locating\nobjects in large-scale new environments. The code, benchmark dataset and\ndemonstration videos are accessible at\nhttps://www.sri.com/ics/computer-vision/saynav."
  },
  {
    "title": "Task and motion planning with large language models for object rearrangement",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2303.06247",
    "authors": "Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi",
    "year": "2023",
    "abstract": "Multi-object rearrangement is a crucial skill for service robots, and\ncommonsense reasoning is frequently needed in this process. However, achieving\ncommonsense arrangements requires knowledge about objects, which is hard to\ntransfer to robots. Large language models (LLMs) are one potential source of\nthis knowledge, but they do not naively capture information about plausible\nphysical arrangements of the world. We propose LLM-GROP, which uses prompting\nto extract commonsense knowledge about semantically valid object configurations\nfrom an LLM and instantiates them with a task and motion planner in order to\ngeneralize to varying scene geometry. LLM-GROP allows us to go from\nnatural-language commands to human-aligned object rearrangement in varied\nenvironments. Based on human evaluations, our approach achieves the highest\nrating while outperforming competitive baselines in terms of success rate while\nmaintaining comparable cumulative action costs. Finally, we demonstrate a\npractical implementation of LLM-GROP on a mobile manipulator in real-world\nscenarios. Supplementary materials are available at:\nhttps://sites.google.com/view/llm-grop"
  },
  {
    "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.10062",
    "authors": "Kannan, Shyam Sundar and Venkatesh, Vishnunandan LN and Min, Byung-Cheol",
    "year": "2023",
    "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for\nembodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task\nPlanning using Large Language Models (LLMs), harnesses the power of LLMs to\nconvert high-level task instructions provided as input into a multi-robot task\nplan. It accomplishes this by executing a series of stages, including task\ndecomposition, coalition formation, and task allocation, all guided by\nprogrammatic LLM prompts within the few-shot prompting paradigm. We create a\nbenchmark dataset designed for validating the multi-robot task planning\nproblem, encompassing four distinct categories of high-level instructions that\nvary in task complexity. Our evaluation experiments span both simulation and\nreal-world scenarios, demonstrating that the proposed model can achieve\npromising results for generating multi-robot task plans. The experimental\nvideos, code, and datasets from the work can be found at\nhttps://sites.google.com/view/smart-llm/."
  },
  {
    "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.01795",
    "authors": "Lu, Yujie and Lu, Pan and Chen, Zhiyu and Zhu, Wanrong and Wang, Xin Eric and Wang, William Yang",
    "year": "2023",
    "abstract": "Embodied agents have achieved prominent performance in following human\ninstructions to complete tasks. However, the potential of providing\ninstructions informed by texts and images to assist humans in completing tasks\nremains underexplored. To uncover this capability, we present the multimodal\nprocedural planning (MPP) task, in which models are given a high-level goal and\ngenerate plans of paired text-image steps, providing more complementary and\ninformative guidance than unimodal plans. The key challenges of MPP are to\nensure the informativeness, temporal coherence,and accuracy of plans across\nmodalities. To tackle this, we propose Text-Image Prompting (TIP), a\ndual-modality prompting method that jointly leverages zero-shot reasoning\nability in large language models (LLMs) and compelling text-to-image generation\nability from diffusion-based models. TIP improves the interaction in the dual\nmodalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs\nto guide the textual-grounded image plan generation and leveraging the\ndescriptions of image plans to ground the textual plan reversely. To address\nthe lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed\nfor MPP. Our results show compelling human preferences and automatic scores\nagainst unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms\nof informativeness, temporal coherence, and plan accuracy. Our code and data:\nhttps://github.com/YujieLu10/MPP."
  },
  {
    "title": "Plan, Eliminate, and Track--Language Models are Good Teachers for Embodied Agents",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.02412",
    "authors": "Wu, Yue and Min, So Yeon and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom and Prabhumoye, Shrimai",
    "year": "2023",
    "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications."
  },
  {
    "title": "Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.10092",
    "authors": "Wang, Jun and Tong, Jiaming and Tan, Kaiyuan and Vorobeychik, Yevgeniy and Kantaros, Yiannis",
    "year": "2023",
    "abstract": "This paper addresses planning problems for mobile robots. We consider\nmissions that require accomplishing multiple high-level sub-tasks, expressed in\nnatural language (NL), in a temporal and logical order. To formally define the\nmission, we treat these sub-tasks as atomic predicates in a Linear Temporal\nLogic (LTL) formula. We refer to this task specification framework as LTL-NL.\nOur goal is to design plans, defined as sequences of robot actions,\naccomplishing LTL-NL tasks. This action planning problem cannot be solved\ndirectly by existing LTL planners because of the NL nature of atomic\npredicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolic\nplanner that relies on a novel integration of (i) existing symbolic planners\ngenerating high-level task plans determining the order at which the NL\nsub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)\nto design sequences of robot actions based on these task plans; and (iii)\nconformal prediction acting as a formal interface between (i) and (ii) and\nmanaging uncertainties due to LLM imperfections. We show, both theoretically\nand empirically, that HERACLEs can achieve user-defined mission success rates.\nFinally, we provide comparative experiments demonstrating that HERACLEs\noutperforms LLM-based planners that require the mission to be defined solely\nusing NL. Additionally, we present examples demonstrating that our approach\nenhances user-friendliness compared to conventional symbolic approaches."
  },
  {
    "title": "Guiding language model reasoning with planning tokens",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2310.05707",
    "authors": "Wang, Xinyi and Caccia, Lucas and Ostapenko, Oleksiy and Yuan, Xingdi and Sordoni, Alessandro",
    "year": "2023",
    "abstract": "Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines."
  },
  {
    "title": "DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.16031",
    "authors": "Kim, Gyeongmin and Kim, Taehyeon and Kannan, Shyam Sundar and Venkatesh, Vishnunandan LN and Kim, Donghan and Min, Byung-Cheol",
    "year": "2023",
    "abstract": "Mobile robots often rely on pre-existing maps for effective path planning and\nnavigation. However, when these maps are unavailable, particularly in\nunfamiliar environments, a different approach become essential. This paper\nintroduces DynaCon, a novel system designed to provide mobile robots with\ncontextual awareness and dynamic adaptability during navigation, eliminating\nthe reliance of traditional maps. DynaCon integrates real-time feedback with an\nobject server, prompt engineering, and navigation modules. By harnessing the\ncapabilities of Large Language Models (LLMs), DynaCon not only understands\npatterns within given numeric series but also excels at categorizing objects\ninto matched spaces. This facilitates dynamic path planner imbued with\ncontextual awareness. We validated the effectiveness of DynaCon through an\nexperiment where a robot successfully navigated to its goal using reasoning.\nSource code and experiment videos for this work can be found at:\nhttps://sites.google.com/view/dynacon."
  },
  {
    "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.10276",
    "authors": "Hu, Hanxu and Lu, Hongyuan and Zhang, Huajian and Lam, Wai and Zhang, Yue",
    "year": "2023",
    "abstract": "In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning"
  },
  {
    "title": "Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.16650",
    "authors": "Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others",
    "year": "2023",
    "abstract": "For robots to perform a wide variety of tasks, they require a 3D\nrepresentation of the world that is semantically rich, yet compact and\nefficient for task-driven perception and planning. Recent approaches have\nattempted to leverage features from large vision-language models to encode\nsemantics in 3D representations. However, these approaches tend to produce maps\nwith per-point feature vectors, which do not scale well in larger environments,\nnor do they contain semantic spatial relationships between entities in the\nenvironment, which are useful for downstream planning. In this work, we propose\nConceptGraphs, an open-vocabulary graph-structured representation for 3D\nscenes. ConceptGraphs is built by leveraging 2D foundation models and fusing\ntheir output to 3D by multi-view association. The resulting representations\ngeneralize to novel semantic classes, without the need to collect large 3D\ndatasets or finetune models. We demonstrate the utility of this representation\nthrough a number of downstream planning tasks that are specified through\nabstract (language) prompts and require complex reasoning over spatial and\nsemantic concepts. (Project page: https://concept-graphs.github.io/ Explainer\nvideo: https://youtu.be/mRhNkQwRYnc )"
  },
  {
    "title": "Bootstrap your own skills: Learning to solve new tasks with large language model guidance",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2310.10021",
    "authors": "Zhang, Jesse and Zhang, Jiahui and Pertsch, Karl and Liu, Ziyi and Ren, Xiang and Chang, Minsuk and Sun, Shao-Hua and Lim, Joseph J",
    "year": "2023",
    "abstract": "We propose BOSS, an approach that automatically learns to solve new\nlong-horizon, complex, and meaningful tasks by growing a learned skill library\nwith minimal supervision. Prior work in reinforcement learning require expert\nsupervision, in the form of demonstrations or rich reward functions, to learn\nlong-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)\nlearns to accomplish new tasks by performing \"skill bootstrapping,\" where an\nagent with a set of primitive skills interacts with the environment to practice\nnew skills without receiving reward feedback for tasks outside of the initial\nskill set. This bootstrapping phase is guided by large language models (LLMs)\nthat inform the agent of meaningful skills to chain together. Through this\nprocess, BOSS builds a wide range of complex and useful behaviors from a basic\nset of primitive skills. We demonstrate through experiments in realistic\nhousehold environments that agents trained with our LLM-guided bootstrapping\nprocedure outperform those trained with naive bootstrapping as well as prior\nunsupervised skill acquisition methods on zero-shot execution of unseen,\nlong-horizon tasks in new environments. Website at clvrai.com/boss."
  },
  {
    "title": "Housekeep: Tidying virtual households using commonsense reasoning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2205.10712",
    "authors": "Kant, Yash and Ramachandran, Arun and Yenamandra, Sriram and Gilitschenski, Igor and Batra, Dhruv and Szot, Andrew and Agrawal, Harsh",
    "year": "2022",
    "abstract": "We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the\nhome for embodied AI. In Housekeep, an embodied agent must tidy a house by\nrearranging misplaced objects without explicit instructions specifying which\nobjects need to be rearranged. Instead, the agent must learn from and is\nevaluated against human preferences of which objects belong where in a tidy\nhouse. Specifically, we collect a dataset of where humans typically place\nobjects in tidy and untidy houses constituting 1799 objects, 268 object\ncategories, 585 placements, and 105 rooms. Next, we propose a modular baseline\napproach for Housekeep that integrates planning, exploration, and navigation.\nIt leverages a fine-tuned large language model (LLM) trained on an internet\ntext corpus for effective planning. We show that our baseline agent generalizes\nto rearranging unseen objects in unknown environments. See our webpage for more\ndetails: https://yashkant.github.io/housekeep/"
  },
  {
    "title": "Reasoning on graphs: Faithful and interpretable large language model reasoning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2310.01061",
    "authors": "Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui",
    "year": "2023",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results."
  },
  {
    "title": "Understanding the Capabilities of Large Language Models for Automated Planning",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.16151",
    "authors": "Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Srivastava, Biplav and Horesh, Lior and Fabiano, Francesco and Loreggia, Andrea",
    "year": "2023",
    "abstract": "Automated planning is concerned with developing efficient algorithms to\ngenerate plans or sequences of actions to achieve a specific goal in a given\nenvironment. Emerging Large Language Models (LLMs) can answer questions, write\nhigh-quality programming code, and predict protein folding, showcasing their\nversatility in solving various tasks beyond language-based problems. In this\npaper, we aim to explore how LLMs can also be used for automated planning. To\ndo so, we seek to answer four key questions. Firstly, we want to understand the\nextent to which LLMs can be used for plan generation. Secondly, we aim to\nidentify which pre-training data is most effective in facilitating plan\ngeneration. Thirdly, we investigate whether fine-tuning or prompting is a more\neffective approach for plan generation. Finally, we explore whether LLMs are\ncapable of plan generalization. By answering these questions, the study seeks\nto shed light on the capabilities of LLMs in solving complex planning problems\nand provide insights into the most effective approaches for using LLMs in this\ncontext."
  },
  {
    "title": "Grounded decoding: Guiding text generation with grounded models for robot control",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2303.00855",
    "authors": "Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and others",
    "year": "2023",
    "abstract": "Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io."
  },
  {
    "title": "Evaluation of Pretrained Large Language Models in Embodied Planning Tasks",
    "category": "plan-generation",
    "link": "https://link.springer.com/chapter/10.1007/978-3-031-33469-6_23",
    "authors": "Sarkisyan, Christina and Korchemnyi, Alexandr and Kovalev, Alexey K and Panov, Aleksandr I",
    "year": "2023",
    "abstract": "Modern pretrained large language models (LLMs) are increasingly being used in zero-shot or few-shot learning modes. Recent years have seen increased interest in applying such models to embodied artificial intelligence and robotics tasks. When given in a natural language, the agent needs to build a plan based on this prompt. The best solutions use LLMs through APIs or models that are not publicly available, making it difficult to reproduce the results. In this paper, we use publicly available LLMs to build a plan for an embodied agent and evaluate them in three modes of operation: 1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3) the step-by-step autoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples of different tasks. Through extensive experiments, we have shown that the subtask evaluation mode, in most cases, outperforms others with a task-specific prompt, whereas the step-by-step autoregressive plan generation posts better performance in the mixed prompt setting.\n\n"
  },
  {
    "title": "Neuro-symbolic causal language planning with commonsense prompting",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2206.02928",
    "authors": "Lu, Yujie and Feng, Weixi and Zhu, Wanrong and Xu, Wenda and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang",
    "year": "2022",
    "abstract": "Procedural planning aims to implement complex high-level goals by\ndecomposition into sequential simpler low-level steps. Although procedural\nplanning is a basic skill set for humans in daily life, it remains a challenge\nfor large language models (LLMs) that lack a deep understanding of the\ncause-effect relations in procedures. Previous methods require manual exemplars\nto acquire procedural planning knowledge from LLMs in the zero-shot setting.\nHowever, such elicited pre-trained knowledge in LLMs induces spurious\ncorrelations between goals and steps, which impair the model generalization to\nunseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\nPLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\ncommonsense-infused prompting. To mitigate spurious goal-step correlations, we\nuse symbolic program executors on the latent procedural representations to\nformalize prompts from commonsense knowledge bases as a causal intervention\ntoward the Structural Causal Model. Both automatic and human evaluations on\nWikiHow and RobotHow show the superiority of PLAN on procedural planning\nwithout further training or manual exemplars."
  },
  {
    "title": "Human-Assisted Continual Robot Learning with Foundation Models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2309.14321",
    "authors": "Parakh, Meenal and Fong, Alisha and Simeonov, Anthony and Gupta, Abhishek and Chen, Tao and Agrawal, Pulkit",
    "year": "2023",
    "abstract": "Large Language Models (LLMs) have been shown to act like planners that can\ndecompose high-level instructions into a sequence of executable instructions.\nHowever, current LLM-based planners are only able to operate with a fixed set\nof skills. We overcome this critical limitation and present a method for using\nLLM-based planners to query new skills and teach robots these skills in a data\nand time-efficient manner for rigid object manipulation. Our system can re-use\nnewly acquired skills for future tasks, demonstrating the potential of open\nworld and lifelong learning. We evaluate the proposed framework on multiple\ntasks in simulation and the real world. Videos are available at:\nhttps://sites.google.com/mit.edu/halp-robot-learning."
  },
  {
    "title": "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2212.10561",
    "authors": "Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah and Haber, Nick",
    "year": "2023",
    "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs.\nWith Parsel, we automatically decompose algorithmic tasks into hierarchical\nnatural language function descriptions and then search over combinations of\npossible function implementations using tests. We show that Parsel can be used\nacross domains requiring hierarchical reasoning, including program synthesis\nand robotic planning. We find that, using Parsel, LLMs solve more\ncompetition-level problems in the APPS dataset, resulting in pass rates over\n75\\% higher than prior results from directly sampling AlphaCode and Codex,\nwhile often using a smaller sample budget. Moreover, with automatically\ngenerated tests, we find that Parsel can improve the state-of-the-art pass@1\nperformance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated\nrobotic plans using Parsel are more than twice as likely to be considered\naccurate than directly generated plans. Lastly, we explore how Parsel addresses\nLLM limitations and discuss how Parsel may be useful for human programmers. We\nrelease our code at https://github.com/ezelikman/parsel"
  },
  {
    "title": "Graph of thoughts: Solving elaborate problems with large language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2308.09687",
    "authors": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others",
    "year": "2023",
    "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (\"LLM thoughts\") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks."
  },
  {
    "title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2307.05973",
    "authors": "Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li",
    "year": "2023",
    "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a vision-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Videos and code at https://voxposer.github.io"
  },
  {
    "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
    "category": "plan-generation",
    "link": "https://openreview.net/pdf?id=2q14O7h8Zm",
    "authors": "Dalal, Murtaza and Chiruvolu, Tarun and Chaplot, Devendra Singh and Salakhutdinov, Ruslan",
    "year": "2023",
    "abstract": "Large Language Models (LLMs) are highly capable of performing\nplanning for long-horizon robotics tasks, yet existing methods require access to\na pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating).\nHowever, LLM planning does not address how to design or learn those behaviors,\nwhich remains challenging particularly in long-horizon settings. Furthermore,\nfor many tasks of interest, the robot needs to be able to adjust its behavior in a\nfine-grained manner, requiring the agent to be capable of modifying low-level\ncontrol actions. Can we instead use the internet-scale knowledge from LLMs for\nhigh-level policies, guiding reinforcement learning (RL) policies to efficiently solve\nrobotic control tasks online without requiring a pre-determined set of skills? In this\npaper, we propose Plan-Seq-Learn (PSL): a modular approach that uses motion\nplanning to bridge the gap between abstract language and learned low-level control\nfor solving long-horizon robotics tasks from scratch. We demonstrate that PSL is\ncapable of solving 20+ challenging single and multi-stage robotics tasks on four\nbenchmarks at success rates of over 80% from raw visual input, out-performing\nlanguage-based, classical, and end-to-end approaches. Video results and code at\nmihdalal.github.io/planseqlearn."
  },
  {
    "title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.04091",
    "authors": "Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng",
    "year": "2023",
    "abstract": "Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting."
  },
  {
    "title": "Tree of thoughts: Deliberate problem solving with large language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2305.10601",
    "authors": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik",
    "year": "2023",
    "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm."
  },
  {
    "title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2206.10498",
    "authors": "Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao",
    "year": "2022",
    "abstract": "Generating plans of action, and reasoning about change have long been\nconsidered a core competence of intelligent agents. It is thus no surprise that\nevaluating the planning and reasoning capabilities of large language models\n(LLMs) has become a hot topic of research. Most claims about LLM planning\ncapabilities are however based on common sense tasks-where it becomes hard to\ntell whether LLMs are planning or merely retrieving from their vast world\nknowledge. There is a strong need for systematic and extensible planning\nbenchmarks with sufficient diversity to evaluate whether LLMs have innate\nplanning capabilities. Motivated by this, we propose PlanBench, an extensible\nbenchmark suite based on the kinds of domains used in the automated planning\ncommunity, especially in the International Planning Competition, to test the\ncapabilities of LLMs in planning or reasoning about actions and change.\nPlanBench provides sufficient diversity in both the task domains and the\nspecific planning capabilities. Our studies also show that on many critical\ncapabilities-including plan generation-LLM performance falls quite short, even\nwith the SOTA models. PlanBench can thus function as a useful marker of\nprogress of LLMs in planning and reasoning."
  },
  {
    "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2310.08118",
    "authors": "Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "There have been widespread claims about Large Language Models (LLMs) being\nable to successfully verify or self-critique their candidate solutions in\nreasoning problems in an iterative mode. Intrigued by those claims, in this\npaper we set out to investigate the verification/self-critiquing abilities of\nlarge language models in the context of planning. We evaluate a planning system\nthat employs LLMs for both plan generation and verification. We assess the\nverifier LLM's performance against ground-truth verification, the impact of\nself-critiquing on plan generation, and the influence of varying feedback\nlevels on system performance. Using GPT-4, a state-of-the-art LLM, for both\ngeneration and verification, our findings reveal that self-critiquing appears\nto diminish plan generation performance, especially when compared to systems\nwith external, sound verifiers and the LLM verifiers in that system produce a\nnotable number of false positives, compromising the system's reliability.\nAdditionally, the nature of feedback, whether binary or detailed, showed\nminimal impact on plan generation. Collectively, our results cast doubt on the\neffectiveness of LLMs in a self-critiquing, iterative framework for planning\ntasks."
  },
  {
    "title": "Generating executable action plans with environmentally-aware language models",
    "category": "plan-generation",
    "link": "https://arxiv.org/abs/2210.04964",
    "authors": "Gramopadhye, Maitrey and Szafir, Daniel",
    "year": "2022",
    "abstract": "Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable,\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat agents are better able to execute. Our approach involves integrating\nenvironmental objects and object relations as additional inputs into LLM action\nplan generation to provide the system with an awareness of its surroundings,\nresulting in plans where each generated action is mapped to objects present in\nthe scene. We also design a novel scoring function that, along with generating\nthe action steps and associating them with objects, helps the system\ndisambiguate among object instances and take into account their states. We\nevaluated our approach using the VirtualHome simulator and the ActivityPrograms\nknowledge base and found that action plans generated from our system had a 310%\nimprovement in executability and a 147% improvement in correctness over prior\nwork. The complete code and a demo of our method is publicly available at\nhttps://github.com/hri-ironlab/scene_aware_language_planner."
  },
  {
    "title": "Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2301.12050",
    "authors": "Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy",
    "year": "2023",
    "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior\nknowledge of the world. However, if initialized with knowledge of high-level\nsubgoals and transitions between subgoals, RL agents could utilize this\nAbstract World Model (AWM) for planning and exploration. We propose using\nfew-shot large language models (LLMs) to hypothesize an AWM, that will be\nverified through world experience, to improve sample efficiency of RL agents.\nOur DECKARD agent applies LLM-guided exploration to item crafting in Minecraft\nin two phases: (1) the Dream phase where the agent uses an LLM to decompose a\ntask into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase\nwhere the agent learns a modular policy for each subgoal and verifies or\ncorrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and\nthen verifying the AWM based on agent experience not only increases sample\nefficiency over contemporary methods by an order of magnitude but is also\nrobust to and corrects errors in the LLM, successfully blending noisy\ninternet-scale information from LLMs with knowledge grounded in environment\ndynamics."
  },
  {
    "title": "Reasoning with language model is planning with world model",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2305.14992",
    "authors": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
    "year": "2023",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting."
  },
  {
    "title": "Large language models as zero-shot human models for human-robot interaction",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2303.03548",
    "authors": "Zhang, Bowen and Soh, Harold",
    "year": "2023",
    "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling\nrobots to consider the impact of their actions on people and plan their\nbehavior accordingly. However, crafting good human models is challenging;\ncapturing context-dependent human behavior requires significant prior knowledge\nand/or large amounts of interaction data, both of which are difficult to\nobtain. In this work, we explore the potential of large-language models (LLMs)\n-- which have consumed vast amounts of human-generated text data -- to act as\nzero-shot human models for HRI. Our experiments on three social datasets yield\npromising results; the LLMs are able to achieve performance comparable to\npurpose-built models. That said, we also discuss current limitations, such as\nsensitivity to prompts and spatial/numerical reasoning mishaps. Based on our\nfindings, we demonstrate how LLM-based human models can be integrated into a\nsocial robot's planning process and applied in HRI scenarios. Specifically, we\npresent one case study on a simulated trust-based table-clearing task and\nreplicate past results that relied on custom models. Next, we conduct a new\nrobot utensil-passing experiment (n = 65) where preliminary results show that\nplanning with a LLM-based human model can achieve gains over a basic myopic\nplan. In summary, our results show that LLMs offer a promising (but incomplete)\napproach to human modeling for HRI."
  },
  {
    "title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2306.12672",
    "authors": "Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B",
    "year": "2023",
    "abstract": "How does language inform our downstream thinking? In particular, how do\nhumans make meaning from language--and how can we leverage a theory of\nlinguistic meaning to build machines that think in more human-like ways? In\nthis paper, we propose rational meaning construction, a computational framework\nfor language-informed thinking that combines neural language models with\nprobabilistic models for rational inference. We frame linguistic meaning as a\ncontext-sensitive mapping from natural language into a probabilistic language\nof thought (PLoT)--a general-purpose symbolic substrate for generative world\nmodeling. Our architecture integrates two computational tools that have not\npreviously come together: we model thinking with probabilistic programs, an\nexpressive representation for commonsense reasoning; and we model meaning\nconstruction with large language models (LLMs), which support broad-coverage\ntranslation from natural language utterances to code expressions in a\nprobabilistic programming language. We illustrate our framework through\nexamples covering four core domains from cognitive science: probabilistic\nreasoning, logical and relational reasoning, visual and physical reasoning, and\nsocial reasoning. In each, we show that LLMs can generate context-sensitive\ntranslations that capture pragmatically-appropriate linguistic meanings, while\nBayesian inference with the generated programs supports coherent and robust\ncommonsense reasoning. We extend our framework to integrate\ncognitively-motivated symbolic modules (physics simulators, graphics engines,\nand planning algorithms) to provide a unified commonsense thinking interface\nfrom language. Finally, we explore how language can drive the construction of\nworld models themselves. We hope this work will provide a roadmap towards\ncognitive models and AI systems that synthesize the insights of both modern and\nclassical computational perspectives."
  },
  {
    "title": "There and back again: extracting formal domains for controllable neurosymbolic story authoring",
    "category": "model-construction",
    "link": "https://ojs.aaai.org/index.php/AIIDE/article/view/27502",
    "authors": "Kelly, Jack and Calderwood, Alex and Wardrip-Fruin, Noah and Mateas, Michael",
    "year": "2023",
    "abstract": "Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences.\n"
  },
  {
    "title": "Exploiting Language Models as a Source of Knowledge for Cognitive Agents",
    "category": "model-construction",
    "link": "https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690",
    "authors": "Kirk, James R and Wray, Robert E and Laird, John E",
    "year": "2023",
    "abstract": "Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area."
  },
  {
    "title": "Exploring the Limitations of using Large Language Models to Fix Planning Tasks",
    "category": "model-construction",
    "link": "https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf",
    "authors": "Gragera, Alba and Pozanco, Alberto",
    "year": "2023",
    "abstract": "Large language models (LLMs) have revolutionized natural\nlanguage processing (NLP), enabling human-like text generation, question answering, and translation. Despite claims\nof emergent reasoning capabilities, it has been demonstrated\ntheir lack of planning skills in tasks such as plan generation,\nplan reuse or replanning. In this work, we present ongoing\nefforts on exploring the limitations of LLMs in another task\nrequiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks."
  },
  {
    "title": "Embodied task planning with large language models",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2307.01848",
    "authors": "Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin",
    "year": "2023",
    "abstract": "Equipping embodied agents with commonsense is important for robots to\nsuccessfully complete complex human instructions in general environments.\nRecent large language models (LLM) can embed rich semantic knowledge for agents\nin plan generation of complex tasks, while they lack the information about the\nrealistic world and usually yield infeasible action sequences. In this paper,\nwe propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning\nwith physical scene constraint, where the agent generates executable plans\naccording to the existed objects in the scene by aligning LLMs with the visual\nperception models. Specifically, we first construct a multimodal dataset\ncontaining triplets of indoor scenes, instructions and action plans, where we\nprovide the designed prompts and the list of existing objects in the scene for\nGPT-3.5 to generate a large number of instructions and corresponding planned\nactions. The generated data is leveraged for grounded plan tuning of\npre-trained LLMs. During inference, we discover the objects in the scene by\nextending open-vocabulary object detectors to multi-view RGB images collected\nin different achievable locations. Experimental results show that the generated\nplan from our TaPA framework can achieve higher success rate than LLaVA and\nGPT-3.5 by a sizable margin, which indicates the practicality of embodied task\nplanning in general and complex environments."
  },
  {
    "title": "Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2303.16563",
    "authors": "Yuan, Haoqi and Zhang, Chi and Wang, Hongcheng and Xie, Feiyang and Cai, Penglin and Dong, Hao and Lu, Zongqing",
    "year": "2023",
    "abstract": "We study building multi-task agents in open-world environments. Without human\ndemonstrations, learning to accomplish long-horizon tasks in a large open-world\nenvironment with reinforcement learning (RL) is extremely inefficient. To\ntackle this challenge, we convert the multi-task learning problem into learning\nbasic skills and planning over the skills. Using the popular open-world game\nMinecraft as the testbed, we propose three types of fine-grained basic skills,\nand use RL with intrinsic rewards to acquire skills. A novel Finding-skill that\nperforms exploration to find diverse items provides better initialization for\nother skills, improving the sample efficiency for skill learning. In skill\nplanning, we leverage the prior knowledge in Large Language Models to find the\nrelationships between skills and build a skill graph. When the agent is solving\na task, our skill search algorithm walks on the skill graph and generates the\nproper skill plans for the agent. In experiments, our method accomplishes 40\ndiverse Minecraft tasks, where many tasks require sequentially executing for\nmore than 10 skills. Our method outperforms baselines by a large margin and is\nthe most sample-efficient demonstration-free RL method to solve Minecraft Tech\nTree tasks. The project's website and code can be found at\nhttps://sites.google.com/view/plan4mc."
  },
  {
    "title": "Roco: Dialectic multi-robot collaboration with large language models",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2307.04738",
    "authors": "Mandi, Zhao and Jain, Shreeya and Song, Shuran",
    "year": "2023",
    "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the\npower of pre-trained large language models (LLMs) for both high-level\ncommunication and low-level path planning. Robots are equipped with LLMs to\ndiscuss and collectively reason task strategies. They then generate sub-task\nplans and task space waypoint paths, which are used by a multi-arm motion\nplanner to accelerate trajectory planning. We also provide feedback from the\nenvironment, such as collision checking, and prompt the LLM agents to improve\ntheir plan and waypoints in-context. For evaluation, we introduce RoCoBench, a\n6-task benchmark covering a wide range of multi-robot collaboration scenarios,\naccompanied by a text-only dataset for agent representation and reasoning. We\nexperimentally demonstrate the effectiveness of our approach -- it achieves\nhigh success rates across all tasks in RoCoBench and adapts to variations in\ntask semantics. Our dialog setup offers high interpretability and flexibility\n-- in real world experiments, we show RoCo easily incorporates\nhuman-in-the-loop, where a user can communicate and collaborate with a robot\nagent to complete tasks together. See project website\nhttps://project-roco.github.io for videos and code."
  },
  {
    "title": "Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2306.03604",
    "authors": "Hu, Bin and Zhao, Chenyang and Zhang, Pu and Zhou, Zihao and Yang, Yuanhang and Xu, Zenglin and Liu, Bin",
    "year": "2023",
    "abstract": "Large language models (LLMs) encode a vast amount of world knowledge acquired\nfrom massive text datasets. Recent studies have demonstrated that LLMs can\nassist an embodied agent in solving complex sequential decision making tasks by\nproviding high-level instructions. However, interactions with LLMs can be\ntime-consuming. In many practical scenarios, it requires a significant amount\nof storage space that can only be deployed on remote cloud servers.\nAdditionally, using commercial LLMs can be costly since they may charge based\non usage frequency. In this paper, we explore how to enable intelligent\ncost-effective interactions between a down stream task oriented agent and an\nLLM. We find that this problem can be naturally formulated by a Markov decision\nprocess (MDP), and propose When2Ask, a reinforcement learning based approach\nthat learns when it is necessary to query LLMs for high-level instructions to\naccomplish a target task. On one side, When2Ask discourages unnecessary\nredundant interactions, while on the other side, it enables the agent to\nidentify and follow useful instructions from the LLM. This enables the agent to\nhalt an ongoing plan and transition to a more suitable one based on new\nenvironmental observations. Experiments on MiniGrid and Habitat environments\nthat entail planning sub-goals demonstrate that When2Ask learns to solve target\ntasks with only a few necessary interactions with the LLM, significantly\nreducing interaction costs in testing environments compared with baseline\nmethods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL."
  },
  {
    "title": "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning",
    "category": "model-construction",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf",
    "authors": "Zhao, Zirui and Lee, Wee Sun and Hsu, David",
    "year": "2023",
    "abstract": "Large-scale task planning is a major challenge. Recent work exploits large language\nmodels (LLMs) directly as a policy and shows surprisingly interesting results. This\npaper shows that LLMs provide a commonsense model of the world in addition\nto a policy that acts on it. The world model and the policy can be combined in\na search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task\nplanning. In our new LLM-MCTS algorithm, the LLM-induced world model\nprovides a commonsense prior belief for MCTS to achieve effective reasoning; the\nLLM-induced policy acts as a heuristic to guide the search, vastly improving search\nefficiency. Experiments show that LLM-MCTS outperforms both MCTS alone\nand policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex,\nnovel tasks. Further experiments and analyses on multiple tasks—multiplication,\ntravel planning, object rearrangement—suggest minimum description length (MDL)\nas a general guiding principle: if the description length of the world model is\nsubstantially smaller than that of the policy, using LLM as a world model for\nmodel-based planning is likely better than using LLM solely as a policy."
  },
  {
    "title": "Statler: State-maintaining language models for embodied reasoning",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2306.17840",
    "authors": "Yoneda, Takuma and Fang, Jiading and Li, Peng and Zhang, Huanyu and Jiang, Tianchong and Lin, Shengjie and Picker, Ben and Yunis, David and Mei, Hongyuan and Walter, Matthew R",
    "year": "2023",
    "abstract": "There has been a significant research interest in employing large language\nmodels to empower intelligent robots with complex reasoning. Existing work\nfocuses on harnessing their abilities to reason about the histories of their\nactions and observations. In this paper, we explore a new dimension in which\nlarge language models may benefit robotics planning. In particular, we propose\nStatler, a framework in which large language models are prompted to maintain an\nestimate of the world state, which are often unobservable, and track its\ntransition as new actions are taken. Our framework then conditions each action\non the estimate of the current world state. Despite being conceptually simple,\nour Statler framework significantly outperforms strong competing methods (e.g.,\nCode-as-Policies) on several robot planning tasks. Additionally, it has the\npotential advantage of scaling up to more challenging long-horizon planning\ntasks."
  },
  {
    "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning",
    "category": "model-construction",
    "link": "https://openreview.net/forum?id=LMiTmpZgSZ",
    "authors": "Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi",
    "year": "2023",
    "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop"
  },
  {
    "title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2307.05973",
    "authors": "Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li",
    "year": "2023",
    "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a vision-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Videos and code at https://voxposer.github.io"
  },
  {
    "title": "Creative Robot Tool Use with Large Language Models",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2310.13065",
    "authors": "Xu, Mengdi and Huang, Peide and Yu, Wenhao and Liu, Shiqi and Zhang, Xilun and Niu, Yaru and Zhang, Tingnan and Xia, Fei and Tan, Jie and Zhao, Ding",
    "year": "2023",
    "abstract": "Tool use is a hallmark of advanced intelligence, exemplified in both animal\nbehavior and robotic capabilities. This paper investigates the feasibility of\nimbuing robots with the ability to creatively use tools in tasks that involve\nimplicit physical constraints and long-term planning. Leveraging Large Language\nModels (LLMs), we develop RoboTool, a system that accepts natural language\ninstructions and outputs executable code for controlling robots in both\nsimulated and real-world environments. RoboTool incorporates four pivotal\ncomponents: (i) an \"Analyzer\" that interprets natural language to discern key\ntask-related concepts, (ii) a \"Planner\" that generates comprehensive strategies\nbased on the language input and key concepts, (iii) a \"Calculator\" that\ncomputes parameters for each skill, and (iv) a \"Coder\" that translates these\nplans into executable Python code. Our results show that RoboTool can not only\ncomprehend explicit or implicit physical constraints and environmental factors\nbut also demonstrate creative tool use. Unlike traditional Task and Motion\nPlanning (TAMP) methods that rely on explicit optimization, our LLM-based\nsystem offers a more flexible, efficient, and user-friendly solution for\ncomplex robotics tasks. Through extensive experiments, we validate that\nRoboTool is proficient in handling tasks that would otherwise be infeasible\nwithout the creative use of tools, thereby expanding the capabilities of\nrobotic systems. Demos are available on our project page:\nhttps://creative-robotool.github.io/."
  },
  {
    "title": "Do as i can, not as i say: Grounding language in robotic affordances",
    "category": "model-construction",
    "link": "https://arxiv.org/abs/2204.01691",
    "authors": "Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others",
    "year": "2023",
    "abstract": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/."
  },
  {
    "title": "Building cooperative embodied agents modularly with large language models",
    "category": "multiagent-planning",
    "link": "https://arxiv.org/abs/2307.02485",
    "authors": "Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B and Shu, Tianmin and Gan, Chuang",
    "year": "2023",
    "abstract": "In this work, we address challenging multi-agent cooperation problems with\ndecentralized control, raw sensory observations, costly communication, and\nmulti-objective tasks instantiated in various embodied environments. While\nprevious research either presupposes a cost-free communication channel or\nrelies on a centralized controller with shared observations, we harness the\ncommonsense knowledge, reasoning ability, language comprehension, and text\ngeneration prowess of LLMs and seamlessly incorporate them into a\ncognitive-inspired modular framework that integrates with perception, memory,\nand execution. Thus building a Cooperative Embodied Language Agent CoELA, who\ncan plan, communicate, and cooperate with others to accomplish long-horizon\ntasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA\ndriven by GPT-4 can surpass strong planning-based methods and exhibit emergent\neffective communication. Though current Open LMs like LLAMA-2 still\nunderperform, we fine-tune a CoELA with data collected with our agents and show\nhow they can achieve promising performance. We also conducted a user study for\nhuman-agent interaction and discovered that CoELA communicating in natural\nlanguage can earn more trust and cooperate more effectively with humans. Our\nresearch underscores the potential of LLMs for future research in multi-agent\ncooperation. Videos can be found on the project website\nhttps://vis-www.cs.umass.edu/Co-LLM-Agents/."
  },
  {
    "title": "War and peace (waragent): Large language model-based multi-agent simulation of world wars",
    "category": "multiagent-planning",
    "link": "https://arxiv.org/abs/2311.17227",
    "authors": "Hua, Wenyue and Fan, Lizhou and Li, Lingyao and Mei, Kai and Ji, Jianchao and Ge, Yingqiang and Hemphill, Libby and Zhang, Yongfeng",
    "year": "2023",
    "abstract": "Can we avoid wars at the crossroads of history? This question has been\npursued by individuals, scholars, policymakers, and organizations throughout\nhuman history. In this research, we attempt to answer the question based on the\nrecent advances of Artificial Intelligence (AI) and Large Language Models\n(LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\nsimulate the participating countries, their decisions, and the consequences, in\nhistorical international conflicts, including the World War I (WWI), the World\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\nevaluating the simulation effectiveness, we examine the advancements and\nlimitations of cutting-edge AI systems' abilities in studying complex\ncollective human behaviors such as international conflicts under diverse\nsettings. In these simulations, the emergent interactions among agents also\noffer a novel perspective for examining the triggers and conditions that lead\nto war. Our findings offer data-driven and AI-augmented insights that can\nredefine how we approach conflict resolution and peacekeeping strategies. The\nimplications stretch beyond historical analysis, offering a blueprint for using\nAI to understand human history and possibly prevent future international\nconflicts. Code and data are available at\n\\url{https://github.com/agiresearch/WarAgent}."
  },
  {
    "title": "Unleashing the Power of Graph Learning through LLM-based Autonomous Agents",
    "category": "multiagent-planning",
    "link": "https://arxiv.org/abs/2309.04565",
    "authors": "Wei, Lanning and He, Zhiqiang and Zhao, Huan and Yao, Quanming",
    "year": "2023",
    "abstract": "Designing versatile graph learning approaches is important, considering the\ndiverse graphs and tasks existing in real-world applications. Existing methods\nhave attempted to achieve this target through automated machine learning\ntechniques, pre-training and fine-tuning strategies, and large language models.\nHowever, these methods are not versatile enough for graph learning, as they\nwork on either limited types of graphs or a single task. In this paper, we\npropose to explore versatile graph learning approaches with LLM-based agents,\nand the key insight is customizing the graph learning procedures for diverse\ngraphs and tasks. To achieve this, we develop several LLM-based agents,\nequipped with diverse profiles, tools, functions and human experience. They\ncollaborate to configure each procedure with task and data-specific settings\nstep by step towards versatile solutions, and the proposed method is dubbed\nGL-Agent. By evaluating on diverse tasks and graphs, the correct results of the\nagent and its comparable performance showcase the versatility of the proposed\nmethod, especially in complex scenarios.The low resource cost and the potential\nto use open-source LLMs highlight the efficiency of GL-Agent."
  },
  {
    "title": "Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?",
    "category": "multiagent-planning",
    "link": "https://arxiv.org/abs/2309.15943",
    "authors": "Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu",
    "year": "2023",
    "abstract": "A flurry of recent work has demonstrated that pre-trained large language\nmodels (LLMs) can be effective task planners for a variety of single-robot\ntasks. The planning performance of LLMs is significantly improved via prompting\ntechniques, such as in-context learning or re-prompting with state feedback,\nplacing new importance on the token budget for the context window. An\nunder-explored but natural next direction is to investigate LLMs as multi-robot\ntask planners. However, long-horizon, heterogeneous multi-robot planning\nintroduces new challenges of coordination while also pushing up against the\nlimits of context window length. It is therefore critical to find\ntoken-efficient LLM planning frameworks that are also able to reason about the\ncomplexities of multi-robot coordination. In this work, we compare the task\nsuccess rate and token efficiency of four multi-agent communication frameworks\n(centralized, decentralized, and two hybrid) as applied to four\ncoordination-dependent multi-agent 2D task scenarios for increasing numbers of\nagents. We find that a hybrid framework achieves better task success rates\nacross all four tasks and scales better to more agents. We further demonstrate\nthe hybrid frameworks in 3D simulations where the vision-to-text problem and\ndynamical errors are considered. See our project website\nhttps://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and\ncode."
  },
  {
    "title": "Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games",
    "category": "multiagent-planning",
    "link": "https://arxiv.org/abs/2309.17234",
    "authors": "Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\\\"o}nherr, Lea and Fritz, Mario",
    "year": "2023",
    "abstract": "There is an growing interest in using Large Language Models (LLMs) in\nmulti-agent systems to tackle interactive real-world tasks that require\neffective collaboration and assessing complex situations. Yet, we still have a\nlimited understanding of LLMs' communication and decision-making abilities in\nmulti-agent setups. The fundamental task of negotiation spans many key features\nof communication, such as cooperation, competition, and manipulation\npotentials. Thus, we propose using scorable negotiation to evaluate LLMs. We\ncreate a testbed of complex multi-agent, multi-issue, and semantically rich\nnegotiation games. To reach an agreement, agents must have strong arithmetic,\ninference, exploration, and planning capabilities while integrating them in a\ndynamic and multi-turn setup. We propose multiple metrics to rigorously\nquantify agents' performance and alignment with the assigned role. We provide\nprocedures to create new games and increase games' difficulty to have an\nevolving benchmark. Importantly, we evaluate critical safety aspects such as\nthe interaction dynamics between agents influenced by greedy and adversarial\nplayers. Our benchmark is highly challenging; GPT-3.5 and small models mostly\nfail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform."
  },
  {
    "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
    "category": "interactive-planning",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f9f54762cbb4fe4dbffdd4f792c31221-Abstract-Conference.html",
    "authors": "Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.\n\n"
  },
  {
    "title": "Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2305.17077",
    "authors": "Arora, Daman and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "There have been wide spread claims in the literature about the emergent\nreasoning capabilities of Pretrained Large Language Models. However, recent\nstudies, have found that their ability to plan remains questionable. Through\nour experiments using GPT-2, we empirically demonstrate that the performance of\na finetuned baseline remains poor because it violates pre-conditions of actions\nin the plans that it generates. To improve the planning capabilities of a\nfinetuned LLM, we train a verifier, which can classify actions as being valid\nor invalid in a particular state. By randomly sampling actions from the same\ndataset, we generate examples of invalid actions which are then used to train a\nverifier which can check for action applicability. In the presence of diverse\nsampling from a generator and a verifier which can prune invalid trajectories,\nwe show significant gains in the success rate on the Blocksworld domain.\nAdditionally, we show that finetuning the GPT-2 generator itself to create the\nverifier generalizes better than finetuning the base GPT-2. Lastly, we\ninvestigate the role of the sampling temperature which can be used to control\nthe exploration-exploitation tradeoff."
  },
  {
    "title": "Grounding large language models in interactive environments with online reinforcement learning",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2302.02662",
    "authors": "Carta, Thomas and Romac, Clement and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves",
    "year": "2023",
    "abstract": "Recent works successfully leveraged Large Language Models' (LLM) abilities to\ncapture abstract knowledge about world's physics to solve decision-making\nproblems. Yet, the alignment between LLMs' knowledge and the environment can be\nwrong and limit functional competence due to lack of grounding. In this paper,\nwe study an approach (named GLAM) to achieve this alignment through functional\ngrounding: we consider an agent using an LLM as a policy that is progressively\nupdated as the agent interacts with the environment, leveraging online\nReinforcement Learning to improve its performance to solve goals. Using an\ninteractive textual environment designed to study higher-level forms of\nfunctional grounding, and a set of spatial and navigation tasks, we study\nseveral scientific questions: 1) Can LLMs boost sample efficiency for online\nlearning of various RL tasks? 2) How can it boost different forms of\ngeneralization? 3) What is the impact of online learning? We study these\nquestions by functionally grounding several variants (size, architecture) of\nFLAN-T5."
  },
  {
    "title": "ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2308.13724",
    "authors": "Zhou, Zhehua and Song, Jiayang and Yao, Kunpeng and Shu, Zhan and Ma, Lei",
    "year": "2023",
    "abstract": "Motivated by the substantial achievements observed in Large Language Models\n(LLMs) in the field of natural language processing, recent research has\ncommenced investigations into the application of LLMs for complex, long-horizon\nsequential task planning challenges in robotics. LLMs are advantageous in\noffering the potential to enhance the generalizability as task-agnostic\nplanners and facilitate flexible interaction between human instructors and\nplanning systems. However, task plans generated by LLMs often lack feasibility\nand correctness. To address this challenge, we introduce ISR-LLM, a novel\nframework that improves LLM-based planning through an iterative self-refinement\nprocess. The framework operates through three sequential steps: preprocessing,\nplanning, and iterative self-refinement. During preprocessing, an LLM\ntranslator is employed to convert natural language input into a Planning Domain\nDefinition Language (PDDL) formulation. In the planning phase, an LLM planner\nformulates an initial plan, which is then assessed and refined in the iterative\nself-refinement step by using a validator. We examine the performance of\nISR-LLM across three distinct planning domains. The results show that ISR-LLM\nis able to achieve markedly higher success rates in task accomplishments\ncompared to state-of-the-art LLM-based planners. Moreover, it also preserves\nthe broad applicability and generalizability of working with natural language\ninstructions."
  },
  {
    "title": "Diversity of Thought Improves Reasoning Abilities of Large Language Models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2310.07088",
    "authors": "Naik, Ranjita and Chandrasekaran, Varun and Yuksekgonul, Mert and Palangi, Hamid and Nushi, Besmira",
    "year": "2023",
    "abstract": "Large language models (LLMs) are documented to struggle in settings that\nrequire complex reasoning. Nevertheless, instructing the model to break down\nthe problem into smaller reasoning steps, or ensembling various generations\nthrough modifying decoding steps boosts performance. However, these methods\nassume that the input prompt is fixed and expect the decoding strategies to\nintroduce the diversity needed for ensembling. In this work, we discuss how one\ncan create and leverage variations of the input prompt as a means of diversity\nof thought. We propose a method that automatically improves prompt diversity by\nsoliciting feedback from the LLM to ideate approaches that are apt for the\nproblem. We then ensemble the diverse prompts in our method DIVSE (DIVerse\nreasoning path Self-Ensemble) across multiple inference calls, or use diverse\napproaches within a single inference call; we call the latter IDIV-SE (In-call\nDIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming\nprior work, DIV-SE(in particular) advances state-of-the-art performance on the\nchallenging planning and graph coloring benchmarks. Our results improve the\nPareto frontier of the accuracy-cost trade-off."
  },
  {
    "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2309.16436",
    "authors": "Jha, Sumit Kumar and Jha, Susmit and Lincoln, Patrick and Bastian, Nathaniel D and Velasquez, Alvaro and Ewetz, Rickard and Neema, Sandeep",
    "year": "2023",
    "abstract": "Generative large language models (LLMs) with instruct training such as GPT-4\ncan follow human-provided instruction prompts and generate human-like responses\nto these prompts. Apart from natural language responses, they have also been\nfound to be effective at generating formal artifacts such as code, plans, and\nlogical specifications from natural language prompts. Despite their remarkably\nimproved accuracy, these models are still known to produce factually incorrect\nor contextually inappropriate results despite their syntactic coherence - a\nphenomenon often referred to as hallucination. This limitation makes it\ndifficult to use these models to synthesize formal artifacts that are used in\nsafety-critical applications. Unlike tasks such as text summarization and\nquestion-answering, bugs in code, plan, and other formal artifacts produced by\nLLMs can be catastrophic. We posit that we can use the satisfiability modulo\ntheory (SMT) solvers as deductive reasoning engines to analyze the generated\nsolutions from the LLMs, produce counterexamples when the solutions are\nincorrect, and provide that feedback to the LLMs exploiting the dialog\ncapability of instruct-trained LLMs. This interaction between inductive LLMs\nand deductive SMT solvers can iteratively steer the LLM to generate the correct\nresponse. In our experiments, we use planning over the domain of blocks as our\nsynthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,\nDavinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our\nmethod allows the user to communicate the planning problem in natural language;\neven the formulation of queries to SMT solvers is automatically generated from\nnatural language. Thus, the proposed technique can enable non-expert users to\ndescribe their problems in natural language, and the combination of LLMs and\nSMT solvers can produce provably correct solutions."
  },
  {
    "title": "Asking Before Action: Gather Information in Embodied Decision Making with Language Models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2305.15695",
    "authors": "Chen, Xiaoyu and Zhang, Shenao and Zhang, Pushi and Zhao, Li and Chen, Jianyu",
    "year": "2023",
    "abstract": "With strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
  },
  {
    "title": "Inner monologue: Embodied reasoning through planning with language models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2207.05608",
    "authors": "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",
    "year": "2022",
    "abstract": "Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world."
  },
  {
    "title": "Sayplan: Grounding large language models using 3d scene graphs for scalable task planning",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2307.06135",
    "authors": "Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko",
    "year": "2023",
    "abstract": "Large language models (LLMs) have demonstrated impressive results in\ndeveloping generalist planning agents for diverse tasks. However, grounding\nthese plans in expansive, multi-floor, and multi-room environments presents a\nsignificant challenge for robotics. We introduce SayPlan, a scalable approach\nto LLM-based, large-scale task planning for robotics using 3D scene graph\n(3DSG) representations. To ensure the scalability of our approach, we: (1)\nexploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic\nsearch' for task-relevant subgraphs from a smaller, collapsed representation of\nthe full graph; (2) reduce the planning horizon for the LLM by integrating a\nclassical path planner and (3) introduce an 'iterative replanning' pipeline\nthat refines the initial plan using feedback from a scene graph simulator,\ncorrecting infeasible actions and avoiding planning failures. We evaluate our\napproach on two large-scale environments spanning up to 3 floors and 36 rooms\nwith 140 assets and objects and show that our approach is capable of grounding\nlarge-scale, long-horizon task plans from abstract, and natural language\ninstruction for a mobile manipulator robot to execute. We provide real robot\nvideo demonstrations on our project page https://sayplan.github.io."
  },
  {
    "title": "Reflect: Summarizing robot experiences for failure explanation and correction",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2306.15724",
    "authors": "Liu, Zeyi and Bahety, Arpit and Song, Shuran",
    "year": "2023",
    "abstract": "The ability to detect and analyze failed executions automatically is crucial\nfor an explainable and robust robotic system. Recently, Large Language Models\n(LLMs) have demonstrated strong reasoning abilities on textual inputs. To\nleverage the power of LLMs for robot failure explanation, we introduce REFLECT,\na framework which queries LLM for failure reasoning based on a hierarchical\nsummary of robot past experiences generated from multisensory observations. The\nfailure explanation can further guide a language-based planner to correct the\nfailure and complete the task. To systematically evaluate the framework, we\ncreate the RoboFail dataset with a variety of tasks and failure scenarios. We\ndemonstrate that the LLM-based framework is able to generate informative\nfailure explanations that assist successful correction planning."
  },
  {
    "title": "Robots that ask for help: Uncertainty alignment for large language model planners",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2307.01928",
    "authors": "Ren, Allen Z and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and others",
    "year": "2023",
    "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities\n-- from step-by-step planning to commonsense reasoning -- that may provide\nutility for robots, but remain prone to confidently hallucinated predictions.\nIn this work, we present KnowNo, which is a framework for measuring and\naligning the uncertainty of LLM-based planners such that they know when they\ndon't know and ask for help when needed. KnowNo builds on the theory of\nconformal prediction to provide statistical guarantees on task completion while\nminimizing human help in complex multi-step planning settings. Experiments\nacross a variety of simulated and real robot setups that involve tasks with\ndifferent modes of ambiguity (e.g., from spatial to numeric uncertainties, from\nhuman preferences to Winograd schemas) show that KnowNo performs favorably over\nmodern baselines (which may involve ensembles or extensive prompt tuning) in\nterms of improving efficiency and autonomy, while providing formal assurances.\nKnowNo can be used with LLMs out of the box without model-finetuning, and\nsuggests a promising lightweight approach to modeling uncertainty that can\ncomplement and scale with the growing capabilities of foundation models.\nWebsite: https://robot-help.github.io"
  },
  {
    "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2310.08582",
    "authors": "Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping",
    "year": "2023",
    "abstract": "This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections."
  },
  {
    "title": "Guiding language model reasoning with planning tokens",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2310.05707",
    "authors": "Wang, Xinyi and Caccia, Lucas and Ostapenko, Oleksiy and Yuan, Xingdi and Sordoni, Alessandro",
    "year": "2023",
    "abstract": "Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines."
  },
  {
    "title": "DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2309.16031",
    "authors": "Kim, Gyeongmin and Kim, Taehyeon and Kannan, Shyam Sundar and Venkatesh, Vishnunandan LN and Kim, Donghan and Min, Byung-Cheol",
    "year": "2023",
    "abstract": "Mobile robots often rely on pre-existing maps for effective path planning and\nnavigation. However, when these maps are unavailable, particularly in\nunfamiliar environments, a different approach become essential. This paper\nintroduces DynaCon, a novel system designed to provide mobile robots with\ncontextual awareness and dynamic adaptability during navigation, eliminating\nthe reliance of traditional maps. DynaCon integrates real-time feedback with an\nobject server, prompt engineering, and navigation modules. By harnessing the\ncapabilities of Large Language Models (LLMs), DynaCon not only understands\npatterns within given numeric series but also excels at categorizing objects\ninto matched spaces. This facilitates dynamic path planner imbued with\ncontextual awareness. We validated the effectiveness of DynaCon through an\nexperiment where a robot successfully navigated to its goal using reasoning.\nSource code and experiment videos for this work can be found at:\nhttps://sites.google.com/view/dynacon."
  },
  {
    "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2305.10276",
    "authors": "Hu, Hanxu and Lu, Hongyuan and Zhang, Huajian and Lam, Wai and Zhang, Yue",
    "year": "2023",
    "abstract": "In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning"
  },
  {
    "title": "Planning with large language models via corrective re-prompting",
    "category": "interactive-planning",
    "link": "https://openreview.net/pdf?id=cMDMRBe1TKs",
    "authors": "Raman, Shreyas Sundara and Cohen, Vanya and Rosen, Eric and Idrees, Ifrah and Paulius, David and Tellex, Stefanie",
    "year": "2022",
    "abstract": "Extracting the common sense knowledge present in Large Language Models\n(LLMs) offers a path to designing intelligent, embodied agents. Related works\nhave queried LLMs with a wide-range of contextual information, such as goals,\nsensor observations and scene descriptions, to generate high-level action plans for\nspecific tasks; however these approaches often involve human intervention or additional machinery to enable sensor-motor interactions. In this work, we propose\na prompting-based strategy for extracting executable plans from an LLM, which\nleverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain\ncontexts, i.e., implicit preconditions must be met for an action to execute (e.g., a\ndoor must be unlocked to open it), and that the embodied agent has the ability to\ndetermine if the action is/is not executable in the current context (e.g., detect if a\nprecondition error is present). When an agent is unable to execute an action, our\napproach re-prompts the LLM with precondition error information to extract an\nexecutable corrective action to achieve the intended goal in the current context.\nWe evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to\nmethods that naively re-sample actions from the LLM. Our approach, using precondition errors, improves executability and semantic correctness of plans, while\nalso reducing the number of re-prompts required when querying actions."
  },
  {
    "title": "Integrating Common Sense and Planning with Large Language Models for Room Tidying",
    "category": "interactive-planning",
    "link": "https://openreview.net/pdf?id=vuSI9mhDaBZ",
    "authors": "Wu, Zhanxin and Ai, Bo and Hsu, David",
    "year": "2023",
    "abstract": "Do you want a personal housekeeper robot? This\nproject seeks to endow robots with the capability of tidying\nup messy rooms with brief natural language descriptions of the\nenvironment. We address three key challenges: (i) incomplete map\ninformation in the description, (ii) commonsense understanding\nof object locations, and (iii) long-horizon planning and acting\nto achieve the objective. To tackle these challenges, we leverage\nLarge Language Models’ (LLMs) understanding of typical layouts\nof human-living environments and object locations, as well as\nprogramming and control skills for action execution. Specifically,\nwe prompt ChatGPT to reconstruct complete map representations\nfrom partial descriptions, then generate a high-level action plan\nin the form of Python functions, and finally refine the plans\nwith atomic actions executable by the robot. We show that our\nframework enables effective room tidying with limited human\ninstruction guidance. On simulation and real-world maps, it is\nable to find a place missing out from human description within\nthree interactions with humans. In the simulation environment,\nit is capable of putting more than 80% household objects in\ntheir desired place. This study provides preliminary evidence that\nLLMs have common sense about the spatial layout of humanliving environments and object arrangements, and this work\nconnects this knowledge to robotics tasks."
  },
  {
    "title": "GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2310.20034",
    "authors": "Graule, Moritz A and Isler, Volkan",
    "year": "2023",
    "abstract": "A robot in a human-centric environment needs to account for the human's\nintent and future motion in its task and motion planning to ensure safe and\neffective operation. This requires symbolic reasoning about probable future\nactions and the ability to tie these actions to specific locations in the\nphysical environment. While one can train behavioral models capable of\npredicting human motion from past activities, this approach requires large\namounts of data to achieve acceptable long-horizon predictions. More\nimportantly, the resulting models are constrained to specific data formats and\nmodalities. Moreover, connecting predictions from such models to the\nenvironment at hand to ensure the applicability of these predictions is an\nunsolved problem. We present a system that utilizes a Large Language Model\n(LLM) to infer a human's next actions from a range of modalities without\nfine-tuning. A novel aspect of our system that is critical to robotics\napplications is that it links the predicted actions to specific locations in a\nsemantic map of the environment. Our method leverages the fact that LLMs,\ntrained on a vast corpus of text describing typical human behaviors, encode\nsubstantial world knowledge, including probable sequences of human actions and\nactivities. We demonstrate how these localized activity predictions can be\nincorporated in a human-aware task planner for an assistive robot to reduce the\noccurrences of undesirable human-robot interactions by 29.2% on average."
  },
  {
    "title": "Palm-e: An embodied multimodal language model",
    "category": "interactive-planning",
    "link": "https://arxiv.org/abs/2303.03378",
    "authors": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
    "year": "2023",
    "abstract": "Large language models excel at a wide range of complex tasks. However,\nenabling general inference in the real world, e.g., for robotics problems,\nraises the challenge of grounding. We propose embodied language models to\ndirectly incorporate real-world continuous sensor modalities into language\nmodels and thereby establish the link between words and percepts. Input to our\nembodied language model are multi-modal sentences that interleave visual,\ncontinuous state estimation, and textual input encodings. We train these\nencodings end-to-end, in conjunction with a pre-trained large language model,\nfor multiple embodied tasks including sequential robotic manipulation planning,\nvisual question answering, and captioning. Our evaluations show that PaLM-E, a\nsingle large embodied multimodal model, can address a variety of embodied\nreasoning tasks, from a variety of observation modalities, on multiple\nembodiments, and further, exhibits positive transfer: the model benefits from\ndiverse joint training across internet-scale language, vision, and\nvisual-language domains. Our largest model, PaLM-E-562B with 562B parameters,\nin addition to being trained on robotics tasks, is a visual-language generalist\nwith state-of-the-art performance on OK-VQA, and retains generalist language\ncapabilities with increasing scale."
  },
  {
    "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
    "category": "interactive-planning",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b5c8c1c117618267944b2617add0a766-Abstract-Conference.html",
    "authors": "Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao",
    "year": "2023",
    "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.\n\n"
  },
  {
    "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
    "category": "interactive-planning",
    "link": "https://openreview.net/pdf?id=NltzxpG0nz",
    "authors": "Zheng, Sipeng and Liu, Jiazheng and Feng, Yicheng and Lu, Zongqing",
    "year": "2023",
    "abstract": "Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact with\nthe world, which marks an initial step toward versatile robotics. However, these\nefforts tend to overlook the visual richness of open worlds, rendering the entire\ninteractive process akin to “a blindfolded text-based game.” Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their\nsurroundings and producing responses that are easy to understand. In this paper,\nwe propose Steve-Eye, an end-to-end trained large multimodal model to address\nthis limitation. Steve-Eye integrates the LLM with a visual encoder to process\nvisual-text inputs and generate multimodal feedback. We adopt a semi-automatic\nstrategy to collect an extensive dataset comprising 850K open-world instruction\npairs, enabling our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks and carry\nout experiments from a wide range of perspectives to validate our model’s capability to strategically act and plan. The project’s website and code can be found at\nhttps://sites.google.com/view/steve-eye."
  },
  {
    "title": "SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2308.12682",
    "authors": "Hazra, Rishi and Martires, Pedro Zuidberg Dos and De Raedt, Luc",
    "year": "2023",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities\ndue to their vast \"world knowledge\". Yet, obtaining plans that are both\nfeasible (grounded in affordances) and cost-effective (in plan length), remains\na challenge, despite recent progress. This contrasts with heuristic planning\nmethods that employ domain knowledge (formalized in action models such as PDDL)\nand heuristic search to generate feasible, optimal plans. Inspired by this, we\npropose to combine the power of LLMs and heuristic planning by leveraging the\nworld knowledge of LLMs and the principles of heuristic search. Our approach,\nSayCanPay, employs LLMs to generate actions (Say) guided by learnable domain\nknowledge, that evaluates actions' feasibility (Can) and long-term\nreward/payoff (Pay), and heuristic search to select the best sequence of\nactions. Our contributions are (1) a novel framing of the LLM planning problem\nin the context of heuristic planning, (2) integrating grounding and\ncost-effective elements into the generated plans, and (3) using heuristic\nsearch over actions. Our extensive evaluations show that our model surpasses\nother LLM planning approaches."
  },
  {
    "title": "PDDL planning with pretrained large language models",
    "category": "heuristics-optimization",
    "link": "https://openreview.net/forum?id=1QMMUB4zfl",
    "authors": "Silver, Tom and Hariprasad, Varun and Shuttleworth, Reece S and Kumar, Nishanth and Lozano-P{\\'e}rez, Tom{\\'a}s and Kaelbling, Leslie Pack",
    "year": "2022",
    "abstract": "We study few-shot prompting of pretrained large language models (LLMs) towards solving PDDL planning problems. We are interested in two questions: (1) To what extent can LLMs solve PDDL planning problems on their own? (2) How and to what extent can LLMs be used to guide AI planners? Recent work by Valmeekam et al. (2022) presents negative evidence for (1) in the classic blocks world domain. We confirm this finding, but expand the inquiry to 18 domains and find more mixed results with a few clear successes. For (2), we propose a simple mechanism for using good-but-imperfect LLM outputs to aid a heuristic-search planner. We also find that the LLM performance is due not only to syntactic pattern matching, but also to its commonsense understanding of English terms that appear in the PDDL.\n"
  },
  {
    "title": "Reasoning with language model is planning with world model",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2305.14992",
    "authors": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
    "year": "2023",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting."
  },
  {
    "title": "Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2306.01729",
    "authors": "Raimondo, Stefania and Pal, Christopher and Liu, Xiaotian and Vazquez, David and Palacios, Hector",
    "year": "2023",
    "abstract": "Task-oriented dialogue is difficult in part because it involves understanding\nuser intent, collecting information from the user, executing API calls, and\ngenerating helpful and fluent responses. However, for complex tasks one must\nalso correctly do all of these things over multiple steps, and in a specific\norder. While large pre-trained language models can be fine-tuned end-to-end to\ncreate multi-step task-oriented dialogue agents that generate fluent text, our\nexperiments confirm that this approach alone cannot reliably perform new\nmulti-step tasks that are unseen during training. To address these limitations,\nwe augment the dialogue contexts given to \\textmd{text2text} transformers with\nknown \\textit{valid workflow names} and \\textit{action plans}. Action plans\nconsist of sequences of actions required to accomplish a task, and are encoded\nas simple sequences of keywords (e.g. verify-identity, pull-up-account,\nreset-password, etc.). We perform extensive experiments on the Action-Based\nConversations Dataset (ABCD) with T5-small, base and large models, and show\nthat such models: a) are able to more readily generalize to unseen workflows by\nfollowing the provided plan, and b) are able to generalize to executing unseen\nactions if they are provided in the plan. In contrast, models are unable to\nfully accomplish new multi-step tasks when they are not provided action plan\ninformation, even when given new valid workflow names."
  },
  {
    "title": "On the planning abilities of large language models (a critical investigation with a proposed benchmark)",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2302.06706",
    "authors": "Valmeekam, Karthik and Sreedharan, Sarath and Marquez, Matthew and Olmo, Alberto and Kambhampati, Subbarao",
    "year": "2023",
    "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) how good LLMs are by themselves in\ngenerating and validating simple plans in commonsense planning tasks (of the\ntype that humans are generally quite good at) and (2) how good LLMs are in\nbeing a source of heuristic guidance for other agents--either AI planners or\nhuman planners--in their planning tasks. To investigate these questions in a\nsystematic rather than anecdotal manner, we start by developing a benchmark\nsuite based on the kinds of domains employed in the International Planning\nCompetition. On this benchmark, we evaluate LLMs in three modes: autonomous,\nheuristic and human-in-the-loop. Our results show that LLM's ability to\nautonomously generate executable plans is quite meager, averaging only about 3%\nsuccess rate. The heuristic and human-in-the-loop modes show slightly more\npromise. In addition to these results, we also make our benchmark and\nevaluation tools available to support investigations by research community."
  },
  {
    "title": "Navigation with large language models: Semantic guesswork as a heuristic for planning",
    "category": "heuristics-optimization",
    "link": "https://proceedings.mlr.press/v229/shah23c.html",
    "authors": "Shah, Dhruv and Equi, Michael and Osinski, Blazej and Xia, Fei and Ichter, Brian and Levine, Sergey",
    "year": "2023",
    "abstract": "Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics — e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the “semantic guesswork” produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.\n"
  },
  {
    "title": "Optimal Scene Graph Planning with Large Language Model Guidance",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2309.09182",
    "authors": "Dai, Zhirui and Asgharivaskasi, Arash and Duong, Thai and Lin, Shusen and Tzes, Maria-Elizabeth and Pappas, George and Atanasov, Nikolay",
    "year": "2023",
    "abstract": "Recent advances in metric, semantic, and topological mapping have equipped\nautonomous robots with semantic concept grounding capabilities to interpret\nnatural language tasks. This work aims to leverage these new capabilities with\nan efficient task planning algorithm for hierarchical metric-semantic models.\nWe consider a scene graph representation of the environment and utilize a large\nlanguage model (LLM) to convert a natural language task into a linear temporal\nlogic (LTL) automaton. Our main contribution is to enable optimal hierarchical\nLTL planning with LLM guidance over scene graphs. To achieve efficiency, we\nconstruct a hierarchical planning domain that captures the attributes and\nconnectivity of the scene graph and the task automaton, and provide semantic\nguidance via an LLM heuristic function. To guarantee optimality, we design an\nLTL heuristic function that is provably consistent and supplements the\npotentially inadmissible LLM guidance in multi-heuristic planning. We\ndemonstrate efficient planning of complex natural language tasks in scene\ngraphs of virtualized real environments."
  },
  {
    "title": "Alphazero-like tree-search can guide large language model decoding and training",
    "category": "heuristics-optimization",
    "link": "https://arxiv.org/abs/2309.17179",
    "authors": "Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun",
    "year": "2023",
    "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64."
  },
  {
    "title": "Gentopia: A collaborative platform for tool-augmented llms",
    "category": "tool-integration",
    "link": "https://arxiv.org/abs/2308.04030",
    "authors": "Xu, Binfeng and Liu, Xukun and Shen, Hua and Han, Zeyu and Li, Yuhan and Yue, Murong and Peng, Zhiyuan and Liu, Yuchen and Yao, Ziyu and Xu, Dongkuan",
    "year": "2023",
    "abstract": "Augmented Language Models (ALMs) empower large language models with the\nability to use tools, transforming them into intelligent agents for real-world\ninteractions. However, most existing frameworks for ALMs, to varying degrees,\nare deficient in the following critical features: flexible customization,\ncollaborative democratization, and holistic evaluation. We present gentopia, an\nALM framework enabling flexible customization of agents through simple\nconfigurations, seamlessly integrating various language models, task formats,\nprompting modules, and plugins into a unified paradigm. Furthermore, we\nestablish gentpool, a public platform enabling the registration and sharing of\nuser-customized agents. Agents registered in gentpool are composable such that\nthey can be assembled together for agent collaboration, advancing the\ndemocratization of artificial intelligence. To ensure high-quality agents,\ngentbench, an integral component of gentpool, is designed to thoroughly\nevaluate user-customized agents across diverse aspects such as safety,\nrobustness, efficiency, etc. We release gentopia on Github and will\ncontinuously move forward."
  },
  {
    "title": "Tptu: Task planning and tool usage of large language model-based ai agents",
    "category": "tool-integration",
    "link": "https://arxiv.org/abs/2308.03427",
    "authors": "Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Zeng, Xingyu and Zhao, Rui",
    "year": "2023",
    "abstract": "With recent advancements in natural language processing, Large Language\nModels (LLMs) have emerged as powerful tools for various real-world\napplications. Despite their prowess, the intrinsic generative abilities of LLMs\nmay prove insufficient for handling complex tasks which necessitate a\ncombination of task planning and the usage of external tools. In this paper, we\nfirst propose a structured framework tailored for LLM-based AI Agents and\ndiscuss the crucial capabilities necessary for tackling intricate problems.\nWithin this framework, we design two distinct types of agents (i.e., one-step\nagent and sequential agent) to execute the inference process. Subsequently, we\ninstantiate the framework using various LLMs and evaluate their Task Planning\nand Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings\nand challenges, our goal is to provide a helpful resource for researchers and\npractitioners to leverage the power of LLMs in their AI applications. Our study\nemphasizes the substantial potential of these models, while also identifying\nareas that need more investigation and improvement."
  },
  {
    "title": "Api-bank: A benchmark for tool-augmented llms",
    "category": "tool-integration",
    "link": "https://arxiv.org/abs/2304.08244",
    "authors": "Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin",
    "year": "2023",
    "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question."
  },
  {
    "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
    "category": "tool-integration",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html",
    "authors": "Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng",
    "year": "2023",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.\n\n"
  },
  {
    "title": "Tool documentation enables zero-shot tool-usage with large language models",
    "category": "tool-integration",
    "link": "https://arxiv.org/abs/2308.00675",
    "authors": "Hsieh, Cheng-Yu and Chen, Si-An and Li, Chun-Liang and Fujii, Yasuhisa and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and Pfister, Tomas",
    "year": "2023",
    "abstract": "Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models."
  },
  {
    "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
    "category": "tool-integration",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/77c33e6a367922d003ff102ffb92b658-Abstract-Conference.html",
    "authors": "Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting",
    "year": "2023",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.\n\n"
  },
  {
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "category": "tool-integration",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html",
    "authors": "Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting",
    "year": "2023",
    "abstract": "Integrating large language models (LLMs) with various tools has led to increased attention in the field. Existing approaches either involve fine-tuning the LLM, which is both computationally costly and limited to a fixed set of tools, or prompting LLMs by in-context tool demonstrations. Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named ToolkenGPT, wherein LLMs effectively learn to master tools as predicting tokens through tool embeddings for solving complex tasks. In this framework, each tool is transformed into vector embeddings and plugged into the language model head. Once the function is triggered during text generation, the LLM enters a special function mode to execute the tool calls. Our experiments show that function embeddings effectively help LLMs understand tool use and improve on several tasks, including numerical reasoning, knowledge-based question answering and embodied decision-making.\n\n"
  },
  {
    "title": "Openagi: When llm meets domain experts",
    "category": "tool-integration",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html",
    "authors": "Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng",
    "year": "2023",
    "abstract": "Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.\n\n"
  },
  {
    "title": "A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models",
    "category": "brain-inspired-planning",
    "link": "https://arxiv.org/abs/2310.00194",
    "authors": "Webb, Taylor and Mondal, Shanka Subhra and Wang, Chi and Krabach, Brian and Momennejad, Ida",
    "year": "2023",
    "abstract": "Large language models (LLMs) demonstrate impressive performance on a wide\nvariety of tasks, but they often struggle with tasks that require multi-step\nreasoning or goal-directed planning. To address this, we take inspiration from\nthe human brain, in which planning is accomplished via the recurrent\ninteraction of specialized modules in the prefrontal cortex (PFC). These\nmodules perform functions such as conflict monitoring, state prediction, state\nevaluation, task decomposition, and task coordination. We find that LLMs are\nsometimes capable of carrying out these functions in isolation, but struggle to\nautonomously coordinate them in the service of a goal. Therefore, we propose a\nblack box architecture with multiple LLM-based (GPT-4) modules. The\narchitecture improves planning through the interaction of specialized\nPFC-inspired modules that break down a larger problem into multiple brief\nautomated calls to the LLM. We evaluate the combined architecture on three\nchallenging planning tasks -- graph traversal, Tower of Hanoi, and logistics --\nfinding that it yields significant improvements over standard LLM methods\n(e.g., zero-shot prompting, in-context learning, and chain-of-thought). These\nresults demonstrate the benefit of utilizing knowledge from cognitive\nneuroscience to improve planning in LLMs."
  },
  {
    "title": "Cognitive architectures for language agents",
    "category": "brain-inspired-planning",
    "link": "https://arxiv.org/abs/2309.02427",
    "authors": "Sumers, Theodore and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L",
    "year": "2023",
    "abstract": "Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence."
  },
  {
    "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
    "category": "brain-inspired-planning",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/dc9d5dcf3e86b83e137bad367227c8ca-Abstract-Conference.html",
    "authors": "Momennejad, Ida and Hasanbeig, Hosein and Vieira, Felipe and Sharma, Hiteshi and Ness, Robert Osazuwa and Jojic, Nebojsa and Palangi, Hamid and Larson, Jonathan",
    "year": "2023",
    "abstract": "Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.\n\n"
  },
  {
    "title": "Tree-of-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning",
    "category": "brain-inspired-planning",
    "link": "https://arxiv.org/abs/2308.09658",
    "authors": "Hu, Pengbo and Qi, Ji and Li, Xingyu and Li, Hong and Wang, Xinqi and Quan, Bing and Wang, Ruiyu and Zhou, Yi",
    "year": "2023",
    "abstract": "There emerges a promising trend of using large language models (LLMs) to\ngenerate code-like plans for complex inference tasks such as visual reasoning.\nThis paradigm, known as LLM-based planning, provides flexibility in problem\nsolving and endows better interpretability. However, current research is mostly\nlimited to basic scenarios of simple questions that can be straightforward\nanswered in a few inference steps. Planning for the more challenging multi-hop\nvisual reasoning tasks remains under-explored. Specifically, under multi-hop\nreasoning situations, the trade-off between accuracy and the complexity of\nplan-searching becomes prominent. The prevailing algorithms either address the\nefficiency issue by employing the fast one-stop generation or adopt a complex\niterative generation method to improve accuracy. Both fail to balance the need\nfor efficiency and performance. Drawing inspiration from the dual system of\ncognition in the human brain, the fast and the slow think processes, we propose\na hierarchical plan-searching algorithm that integrates the one-stop reasoning\n(fast) and the Tree-of-thought (slow). Our approach succeeds in performance\nwhile significantly saving inference steps. Moreover, we repurpose the PTR and\nthe CLEVER datasets, developing a systematic framework for evaluating the\nperformance and efficiency of LLMs-based plan-search algorithms under reasoning\ntasks at different levels of difficulty. Extensive experiments demonstrate the\nsuperiority of our proposed algorithm in terms of performance and efficiency.\nThe dataset and code will be release soon."
  },
  {
    "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
    "category": "brain-inspired-planning",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/4b0eea69deea512c9e2c469187643dc2-Paper-Conference.pdf",
    "authors": "Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Ammanabrolu, Prithviraj and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang",
    "year": "2023",
    "abstract": "We introduce S WIFT S AGE , a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. S WIFT S AGE integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the S WIFT module,\nrepresenting fast and intuitive thinking, and the S AGE module, emulating deliberate\nthought processes. The S WIFT module is a small encoder-decoder LM fine-tuned\non the oracle agent’s action trajectories, while the S AGE module employs LLMs\nsuch as GPT-4 for subgoal planning and grounding. We develop a heuristic method\nto harmoniously integrate the two modules, resulting in a more efficient and\nrobust problem-solving process. In 30 tasks from the ScienceWorld benchmark,\nS WIFT S AGE significantly outperforms other methods such as SayCan, ReAct, and\nReflexion, demonstrating its effectiveness in solving complex interactive tasks."
  }
]
