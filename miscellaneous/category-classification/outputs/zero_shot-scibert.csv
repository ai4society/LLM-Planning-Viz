title,abstract,actual_categories,predicted_categories,cross_validation_fold
Llm+ p: Empowering large language models with optimal planning proficiency,"Large language models (LLMs) have demonstrated remarkable zero-shot
generalization abilities: state-of-the-art chatbots can provide plausible
answers to many common questions that arise in daily life. However, so far,
LLMs cannot reliably solve long-horizon planning problems. By contrast,
classical planners, once a problem is given in a formatted way, can use
efficient search algorithms to quickly identify correct, or even optimal,
plans. In an effort to get the best of both worlds, this paper introduces
LLM+P, the first framework that incorporates the strengths of classical
planners into LLMs. LLM+P takes in a natural language description of a planning
problem, then returns a correct (or optimal) plan for solving that problem in
natural language. LLM+P does so by first converting the language description
into a file written in the planning domain definition language (PDDL), then
leveraging classical planners to quickly find a solution, and then translating
the found solution back into natural language. Along with LLM+P, we define a
diverse set of different benchmark problems taken from common planning
scenarios. Via a comprehensive set of experiments on these benchmark problems,
we find that LLM+P is able to provide optimal solutions for most problems,
while LLMs fail to provide even feasible plans for most problems.\footnote{The
code and results are publicly available at
https://github.com/Cranial-XIX/llm-pddl.git.",language-translation,"language-translation, plan-generation",1
Translating natural language to planning goals with large-language models,"Recent large language models (LLMs) have demonstrated remarkable performance
on a variety of natural language processing (NLP) tasks, leading to intense
excitement about their applicability across various domains. Unfortunately,
recent work has also shown that LLMs are unable to perform accurate reasoning
nor solve planning problems, which may limit their usefulness for
robotics-related tasks. In this work, our central question is whether LLMs are
able to translate goals specified in natural language to a structured planning
language. If so, LLM can act as a natural interface between the planner and
human users; the translated goal can be handed to domain-independent AI
planners that are very effective at planning. Our empirical results on GPT 3.5
variants show that LLMs are much better suited towards translation rather than
planning. We find that LLMs are able to leverage commonsense knowledge and
reasoning to furnish missing details from under-specified goals (as is often
the case in natural language). However, our experiments also reveal that LLMs
can fail to generate goals in tasks that involve numerical or physical (e.g.,
spatial) reasoning, and that LLMs are sensitive to the prompts used. As such,
these models are promising for translation to structured planning languages,
but care should be taken in their use.",language-translation,"language-translation, plan-generation",1
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",language-translation,"language-translation, plan-generation",1
Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,"Long-horizon task planning is essential for the development of intelligent
assistive and service robots. In this work, we investigate the applicability of
a smaller class of large language models (LLMs), specifically GPT-2, in robotic
task planning by learning to decompose tasks into subgoal specifications for a
planner to execute sequentially. Our method grounds the input of the LLM on the
domain that is represented as a scene graph, enabling it to translate human
requests into executable robot plans, thereby learning to reason over
long-horizon tasks, as encountered in the ALFRED benchmark. We compare our
approach with classical planning and baseline methods to examine the
applicability and generalizability of LLM-based planners. Our findings suggest
that the knowledge stored in an LLM can be effectively grounded to perform
long-horizon task planning, demonstrating the promising potential for the
future application of neuro-symbolic planning methods in robotics.",language-translation,"language-translation, plan-generation",1
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text,"While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM's adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.",language-translation,"language-translation, plan-generation",1
From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought,"How does language inform our downstream thinking? In particular, how do
humans make meaning from language--and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose rational meaning construction, a computational framework
for language-informed thinking that combines neural language models with
probabilistic models for rational inference. We frame linguistic meaning as a
context-sensitive mapping from natural language into a probabilistic language
of thought (PLoT)--a general-purpose symbolic substrate for generative world
modeling. Our architecture integrates two computational tools that have not
previously come together: we model thinking with probabilistic programs, an
expressive representation for commonsense reasoning; and we model meaning
construction with large language models (LLMs), which support broad-coverage
translation from natural language utterances to code expressions in a
probabilistic programming language. We illustrate our framework through
examples covering four core domains from cognitive science: probabilistic
reasoning, logical and relational reasoning, visual and physical reasoning, and
social reasoning. In each, we show that LLMs can generate context-sensitive
translations that capture pragmatically-appropriate linguistic meanings, while
Bayesian inference with the generated programs supports coherent and robust
commonsense reasoning. We extend our framework to integrate
cognitively-motivated symbolic modules (physics simulators, graphics engines,
and planning algorithms) to provide a unified commonsense thinking interface
from language. Finally, we explore how language can drive the construction of
world models themselves. We hope this work will provide a roadmap towards
cognitive models and AI systems that synthesize the insights of both modern and
classical computational perspectives.",language-translation,"language-translation, plan-generation",1
There and back again: extracting formal domains for controllable neurosymbolic story authoring,"Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences.",language-translation,"language-translation, plan-generation",1
Neuro-Symbolic AI Approaches to Enhance Deep Neural Networks with Logical Reasoning and Knowledge Integration,"One of the challenges in Artificial Intelligence (AI) is to integrate fast, automatic, and intuitive System-1 thinking with slow, deliberate, and logical System-2 thinking. While deep
learning approaches excel at perception tasks for System-1, their reasoning capabilities for
System-2 are limited. Besides, deep learning approaches are usually data-hungry, hard to
make use of explicit knowledge, and struggling with interpretability and justification. This
dissertation presents three neuro-symbolic AI approaches that integrate neural networks
(NNs) with symbolic AI methods to address these issues.
The first approach presented in this dissertation is NeurASP, which combines NNs with
Answer Set Programming (ASP), a logic programming formalism. NeurASP provides an
effective way to integrate sub-symbolic and symbolic computation by treating NN outputs
as probability distributions over atomic facts in ASP. The explicit knowledge encoded in
ASP corrects mistakes in NN outputs and allows for better training with less data.
To avoid NeurASP’s bottleneck in symbolic computation, this dissertation presents a
Constraint Loss via Straight-Through Estimators (CL-STE). CL-STE provides a systematic
way to compile discrete logical constraints into a loss function over discretized NN outputs
and scales significantly better than state-of-the-art neuro-symbolic methods. This dissertation also presents a finding when CL-STE was applied to Transformers. Transformers can
be extended with recurrence to enhance its power for multi-step reasoning. Such Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems
while successfully addressing the symbol grounding problem.
Lastly, this dissertation addresses the limitation of pre-trained Large Language Models
(LLMs) on multi-step logical reasoning problems with a dual-process neuro-symbolic reasoning system called LLM+ASP, where an LLM (e.g., GPT-3) serves as a highly effective
few-shot semantic parser that turns natural language sentences into a logical form that can
be used as input to ASP. LLM+ASP achieves state-of-the-art performance on several textual reasoning benchmarks and can handle robot planning tasks that an LLM alone fails to
solve.",language-translation,"language-translation, plan-generation, tool-integration",1
Text2motion: From natural language instructions to feasible plans,"We propose Text2Motion, a language-based planning framework enabling robots
to solve sequential manipulation tasks that require long-horizon reasoning.
Given a natural language instruction, our framework constructs both a task- and
motion-level plan that is verified to reach inferred symbolic goals.
Text2Motion uses feasibility heuristics encoded in Q-functions of a library of
skills to guide task planning with Large Language Models. Whereas previous
language-based planners only consider the feasibility of individual skills,
Text2Motion actively resolves geometric dependencies spanning skill sequences
by performing geometric feasibility planning during its search. We evaluate our
method on a suite of problems that require long-horizon reasoning,
interpretation of abstract goals, and handling of partial affordance
perception. Our experiments show that Text2Motion can solve these challenging
problems with a success rate of 82%, while prior state-of-the-art
language-based planning methods only achieve 13%. Text2Motion thus provides
promising generalization characteristics to semantically diverse sequential
manipulation tasks with geometric dependencies between skills.",language-translation,"language-translation, plan-generation",1
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,"3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .",language-translation,"language-translation, plan-generation, tool-integration",1
From Cooking Recipes to Robot Task Trees--Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,"Task planning for robotic cooking involves generating a sequence of actions
for a robot to prepare a meal successfully. This paper introduces a novel task
tree generation pipeline producing correct planning and efficient execution for
cooking tasks. Our method first uses a large language model (LLM) to retrieve
recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a
task tree, capturing sequential and parallel dependencies among subtasks. The
pipeline then mitigates the uncertainty and unreliable features of LLM outputs
using task tree retrieval. We combine multiple LLM task tree outputs into a
graph and perform a task tree retrieval to avoid questionable nodes and
high-cost nodes to improve planning correctness and improve execution
efficiency. Our evaluation results show its superior performance compared to
previous works in task planning accuracy and efficiency.",language-translation,"language-translation, plan-generation, tool-integration",1
OceanChat: Piloting Autonomous Underwater Vehicles in Natural Language,"In the trending research of fusing Large Language Models (LLMs) and robotics,
we aim to pave the way for innovative development of AI systems that can enable
Autonomous Underwater Vehicles (AUVs) to seamlessly interact with humans in an
intuitive manner. We propose OceanChat, a system that leverages a closed-loop
LLM-guided task and motion planning framework to tackle AUV missions in the
wild. LLMs translate an abstract human command into a high-level goal, while a
task planner further grounds the goal into a task sequence with logical
constraints. To assist the AUV with understanding the task sequence, we utilize
a motion planner to incorporate real-time Lagrangian data streams received by
the AUV, thus mapping the task sequence into an executable motion plan.
Considering the highly dynamic and partially known nature of the underwater
environment, an event-triggered replanning scheme is developed to enhance the
system's robustness towards uncertainty. We also build a simulation platform
HoloEco that generates photo-realistic simulation for a wide range of AUV
applications. Experimental evaluation verifies that the proposed system can
achieve improved performance in terms of both success rate and computation
time. Project website: \url{https://sites.google.com/view/oceanchat}",language-translation,"language-translation, plan-generation",1
Human-Assisted Continual Robot Learning with Foundation Models,"Large Language Models (LLMs) have been shown to act like planners that can
decompose high-level instructions into a sequence of executable instructions.
However, current LLM-based planners are only able to operate with a fixed set
of skills. We overcome this critical limitation and present a method for using
LLM-based planners to query new skills and teach robots these skills in a data
and time-efficient manner for rigid object manipulation. Our system can re-use
newly acquired skills for future tasks, demonstrating the potential of open
world and lifelong learning. We evaluate the proposed framework on multiple
tasks in simulation and the real world. Videos are available at:
https://sites.google.com/mit.edu/halp-robot-learning.",language-translation,"brain-inspired-planning, language-translation, plan-generation",1
Optimal Scene Graph Planning with Large Language Model Guidance,"Recent advances in metric, semantic, and topological mapping have equipped
autonomous robots with semantic concept grounding capabilities to interpret
natural language tasks. This work aims to leverage these new capabilities with
an efficient task planning algorithm for hierarchical metric-semantic models.
We consider a scene graph representation of the environment and utilize a large
language model (LLM) to convert a natural language task into a linear temporal
logic (LTL) automaton. Our main contribution is to enable optimal hierarchical
LTL planning with LLM guidance over scene graphs. To achieve efficiency, we
construct a hierarchical planning domain that captures the attributes and
connectivity of the scene graph and the task automaton, and provide semantic
guidance via an LLM heuristic function. To guarantee optimality, we design an
LTL heuristic function that is provably consistent and supplements the
potentially inadmissible LLM guidance in multi-heuristic planning. We
demonstrate efficient planning of complex natural language tasks in scene
graphs of virtualized real environments.",language-translation,"brain-inspired-planning, language-translation, multiagent-planning, plan-generation",1
Vision-Language Interpreter for Robot Task Planning,"Large language models (LLMs) are accelerating the development of
language-guided robot planners. Meanwhile, symbolic planners offer the
advantage of interpretability. This paper proposes a new task that bridges
these two trends, namely, multimodal planning problem specification. The aim is
to generate a problem description (PD), a machine-readable file used by the
planners to find a plan. By generating PDs from language instruction and scene
observation, we can drive symbolic planners in a language-guided framework. We
propose a Vision-Language Interpreter (ViLaIn), a new framework that generates
PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine
generated PDs via error message feedback from the symbolic planner. Our aim is
to answer the question: How accurately can ViLaIn and the symbolic planner
generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset
called the problem description generation (ProDG) dataset. The framework is
evaluated with four new evaluation metrics. Experimental results show that
ViLaIn can generate syntactically correct problems with more than 99\% accuracy
and valid plans with more than 58\% accuracy. Our code and dataset are
available at https://github.com/omron-sinicx/ViLaIn.",language-translation,"language-translation, plan-generation",1
Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning,"Multi-object rearrangement is a crucial skill for
service robots, and commonsense reasoning is frequently needed
in this process. However, achieving commonsense arrangements
requires knowledge about objects, which is hard to transfer to
robots. Large language models (LLMs) are one potential source of
this knowledge, but they do not naively capture information about
plausible physical arrangements of the world. We propose LLMGROP, which uses prompting to extract commonsense knowledge
about semantically valid object configurations from an LLM
and instantiates them with a task and motion planner in order
to generalize to varying scene geometry. LLM-GROP allows
us to go from natural-language commands to human-aligned
object rearrangement in varied environments. Based on human
evaluations, our approach achieves the highest rating while
outperforming competitive baselines in terms of success rate
while maintaining comparable cumulative action costs. Finally,
we demonstrate a practical implementation of LLM-GROP on
a mobile manipulator in real-world scenarios. Supplementary
materials are available at: https://sites.google.com/view/llm-grop",language-translation,"language-translation, plan-generation, tool-integration",1
Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,"Despite recent success in large language model (LLM) reasoning, LLMs struggle
with hierarchical multi-step reasoning tasks like generating complex programs.
For these tasks, humans often start with a high-level algorithmic design and
implement each part gradually. We introduce Parsel, a framework enabling
automatic implementation and validation of complex algorithms with code LLMs.
With Parsel, we automatically decompose algorithmic tasks into hierarchical
natural language function descriptions and then search over combinations of
possible function implementations using tests. We show that Parsel can be used
across domains requiring hierarchical reasoning, including program synthesis
and robotic planning. We find that, using Parsel, LLMs solve more
competition-level problems in the APPS dataset, resulting in pass rates over
75\% higher than prior results from directly sampling AlphaCode and Codex,
while often using a smaller sample budget. Moreover, with automatically
generated tests, we find that Parsel can improve the state-of-the-art pass@1
performance on HumanEval from 67\% to 85\%. We also find that LLM-generated
robotic plans using Parsel are more than twice as likely to be considered
accurate than directly generated plans. Lastly, we explore how Parsel addresses
LLM limitations and discuss how Parsel may be useful for human programmers. We
release our code at https://github.com/ezelikman/parsel",language-translation,"language-translation, multiagent-planning, plan-generation, tool-integration",1
Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning,"Large Language Models (LLMs) have shown human-like reasoning abilities but
still struggle with complex logical problems. This paper introduces a novel
framework, Logic-LM, which integrates LLMs with symbolic solvers to improve
logical problem-solving. Our method first utilizes LLMs to translate a natural
language problem into a symbolic formulation. Afterward, a deterministic
symbolic solver performs inference on the formulated problem. We also introduce
a self-refinement module, which utilizes the symbolic solver's error messages
to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on
five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,
LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant
performance boost of 39.2% over using LLM alone with standard prompting and
18.4% over LLM with chain-of-thought prompting. Our findings suggest that
Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for
faithful logical reasoning. Code and data are publicly available at
https://github.com/teacherpeterpan/Logic-LLM.",language-translation,"language-translation, plan-generation",1
Creative Robot Tool Use with Large Language Models,"Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's ""hands and eyes,"" while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at https://say-can.github.io/.",language-translation,"language-translation, plan-generation",1
"Do as i can, not as i say: Grounding language in robotic affordances","Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ""hands and eyes,"" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at [this https URL](https://say-can.github.io/).
",language-translation,"language-translation, plan-generation",1
Learning Automata-Based Task Knowledge Representation from Large-Scale Generative Language Models,"Automaton-based representations of task knowledge play an important role in
control and planning for sequential decision-making problems. However,
obtaining the high-level task knowledge required to build such automata is
often difficult. Meanwhile, large-scale generative language models (GLMs) can
automatically generate relevant task knowledge. However, the textual outputs
from GLMs cannot be formally verified or used for sequential decision-making.
We propose a novel algorithm named GLM2FSA, which constructs a finite state
automaton (FSA) encoding high-level task knowledge from a brief
natural-language description of the task goal. GLM2FSA first sends queries to a
GLM to extract task knowledge in textual form, and then it builds an FSA to
represent this text-based knowledge. The proposed algorithm thus fills the gap
between natural-language task descriptions and automaton-based representations,
and the constructed FSA can be formally verified against user-defined
specifications. We accordingly propose a method to iteratively refine the
queries to the GLM based on the outcomes, e.g., counter-examples, from
verification. We demonstrate GLM2FSA's ability to build and refine
automaton-based representations of everyday tasks (e.g., crossing a road), and
also of tasks that require highly-specialized knowledge (e.g., executing secure
multi-party computation).",language-translation,"language-translation, plan-generation, tool-integration",1
Open-vocabulary queryable scene representations for real world planning,"Large language models (LLMs) have unlocked new capabilities of task planning
from human instructions. However, prior attempts to apply LLMs to real-world
robotic tasks are limited by the lack of grounding in the surrounding scene. In
this paper, we develop NLMap, an open-vocabulary and queryable scene
representation to address this problem. NLMap serves as a framework to gather
and integrate contextual information into LLM planners, allowing them to see
and query available objects in the scene before generating a
context-conditioned plan. NLMap first establishes a natural language queryable
scene representation with Visual Language models (VLMs). An LLM based object
proposal module parses instructions and proposes involved objects to query the
scene representation for object availability and location. An LLM planner then
plans with such information about the scene. NLMap allows robots to operate
without a fixed list of objects nor executable options, enabling real robot
operation unachievable by previous methods. Project website:
https://nlmap-saycan.github.io",language-translation,"brain-inspired-planning, language-translation, plan-generation",1
EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation,"Plan-and-Write is a common hierarchical approach in long-form narrative text
generation, which first creates a plan to guide the narrative writing.
Following this approach, several studies rely on simply prompting large
language models for planning, which often yields suboptimal results. In this
paper, we propose a new framework called Evaluation-guided Iterative Plan
Extraction for long-form narrative text generation (EIPE-text), which extracts
plans from the corpus of narratives and utilizes the extracted plans to
construct a better planner. EIPE-text has three stages: plan extraction,
learning, and inference. In the plan extraction stage, it iteratively extracts
and improves plans from the narrative corpus and constructs a plan corpus. We
propose a question answer (QA) based evaluation mechanism to automatically
evaluate the plans and generate detailed plan refinement instructions to guide
the iterative improvement. In the learning stage, we build a better planner by
fine-tuning with the plan corpus or in-context learning with examples in the
plan corpus. Finally, we leverage a hierarchical approach to generate long-form
narratives. We evaluate the effectiveness of EIPE-text in the domains of novels
and storytelling. Both GPT-4-based evaluations and human evaluations
demonstrate that our method can generate more coherent and relevant long-form
narratives. Our code will be released in the future.",language-translation,"language-translation, plan-generation",1
RoboVQA: Multimodal Long-Horizon Reasoning for Robotics,"We present a scalable, bottom-up and intrinsically diverse data collection
scheme that can be used for high-level reasoning with long and medium horizons
and that has 2.2x higher throughput compared to traditional narrow top-down
step-by-step collection. We collect realistic data by performing any user
requests within the entirety of 3 office buildings and using multiple robot and
human embodiments. With this data, we show that models trained on all
embodiments perform better than ones trained on the robot data only, even when
evaluated solely on robot episodes. We find that for a fixed collection budget
it is beneficial to take advantage of cheaper human collection along with robot
collection. We release a large and highly diverse (29,520 unique instructions)
dataset dubbed RoboVQA containing 829,502 (video, text) pairs for
robotics-focused visual question answering. We also demonstrate how evaluating
real robot experiments with an intervention mechanism enables performing tasks
to completion, making it deployable with human oversight even if imperfect
while also providing a single performance metric. We demonstrate a single
video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is
capable of performing a variety of grounded high-level reasoning tasks in broad
realistic settings with a cognitive intervention rate 46% lower than the
zero-shot state of the art visual language model (VLM) baseline and is able to
guide real robots through long-horizon tasks. The performance gap with
zero-shot state-of-the-art models indicates that a lot of grounded data remains
to be collected for real-world deployment, emphasizing the critical need for
scalable data collection approaches. Finally, we show that video VLMs
significantly outperform single-image VLMs with an average error rate reduction
of 19% across all VQA tasks. Data and videos available at
https://robovqa.github.io",plan-generation,"language-translation, plan-generation",1
Human-Centered Planning,"LLMs have recently made impressive inroads on tasks whose output is
structured, such as coding, robotic planning and querying databases. The vision
of creating AI-powered personal assistants also involves creating structured
outputs, such as a plan for one's day, or for an overseas trip. Here, since the
plan is executed by a human, the output doesn't have to satisfy strict
syntactic constraints. A useful assistant should also be able to incorporate
vague constraints specified by the user in natural language. This makes LLMs an
attractive option for planning.
  We consider the problem of planning one's day. We develop an LLM-based
planner (LLMPlan) extended with the ability to self-reflect on its output and a
symbolic planner (SymPlan) with the ability to translate text constraints into
a symbolic representation. Despite no formal specification of constraints, we
find that LLMPlan performs explicit constraint satisfaction akin to the
traditional symbolic planners on average (2% performance difference), while
retaining the reasoning of implicit requirements. Consequently, LLM-based
planners outperform their symbolic counterparts in user satisfaction (70.5% vs.
40.4%) during interactive evaluation with 40 users.",plan-generation,"language-translation, plan-generation",1
Plansformer: Generating symbolic plans using transformers,"Large Language Models (LLMs) have been the subject of active research,
significantly advancing the field of Natural Language Processing (NLP). From
BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural
language tasks such as question answering, summarization, and text generation.
Many ongoing efforts focus on understanding LLMs' capabilities, including their
knowledge of the world, syntax, and semantics. However, extending the textual
prowess of LLMs to symbolic reasoning has been slow and predominantly focused
on tackling problems related to the mathematical field. In this paper, we
explore the use of LLMs for automated planning - a branch of AI concerned with
the realization of action sequences (plans) to achieve a goal, typically
executed by intelligent agents, autonomous robots, and unmanned vehicles. We
introduce Plansformer; an LLM fine-tuned on planning problems and capable of
generating plans with favorable behavior in terms of correctness and length
with reduced knowledge-engineering efforts. We also demonstrate the
adaptability of Plansformer in solving different planning domains with varying
complexities, owing to the transfer learning abilities of LLMs. For one
configuration of Plansformer, we achieve ~97% valid plans, out of which ~95%
are optimal for Towers of Hanoi - a puzzle-solving domain.",plan-generation,"language-translation, multiagent-planning",1
Generalized Planning in PDDL Domains with Pretrained Large Language Models,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",plan-generation,"language-translation, plan-generation",1
Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers,"Plansformer is a novel tool that utilizes a fine-tuned
language model based on transformer architecture
to generate symbolic plans. Transformers are a
type of neural network architecture that have been
shown to be highly effective in a range of natural
language processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans
that are both fluent and feasible. Plansformer takes
the domain and problem files as input (in PDDL)
and outputs a sequence of actions that can be executed to solve the problem. We demonstrate the
effectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and
quantitative results obtained during our evaluation,
including its limitations. Plansformer has the potential to significantly improve the efficiency and
effectiveness of planning in various domains, from
logistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer
via a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at
https://youtu.be/_1rlctCGsrk.",plan-generation,"language-translation, multiagent-planning, plan-generation",1
Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners,"The emergent few-shot reasoning capabilities of Large Language Models (LLMs)
have excited the natural language and machine learning community over recent
years. Despite of numerous successful applications, the underlying mechanism of
such in-context capabilities still remains unclear. In this work, we
hypothesize that the learned \textit{semantics} of language tokens do the most
heavy lifting during the reasoning process. Different from human's symbolic
reasoning process, the semantic representations of LLMs could create strong
connections among tokens, thus composing a superficial logical chain. To test
our hypothesis, we decouple semantics from the language reasoning process and
evaluate three kinds of reasoning abilities, i.e., deduction, induction and
abduction. Our findings reveal that semantics play a vital role in LLMs'
in-context reasoning -- LLMs perform significantly better when semantics are
consistent with commonsense but struggle to solve symbolic or
counter-commonsense reasoning tasks by leveraging in-context new knowledge. The
surprising observations question whether modern LLMs have mastered the
inductive, deductive and abductive reasoning abilities as in human
intelligence, and motivate research on unveiling the magic existing within the
black-box LLMs. On the whole, our analysis provides a novel perspective on the
role of semantics in developing and evaluating language models' reasoning
abilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}.",plan-generation,"language-translation, plan-generation",2
Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,"There have been wide spread claims in the literature about the emergent
reasoning capabilities of Pretrained Large Language Models. However, recent
studies, have found that their ability to plan remains questionable. Through
our experiments using GPT-2, we empirically demonstrate that the performance of
a finetuned baseline remains poor because it violates pre-conditions of actions
in the plans that it generates. To improve the planning capabilities of a
finetuned LLM, we train a verifier, which can classify actions as being valid
or invalid in a particular state. By randomly sampling actions from the same
dataset, we generate examples of invalid actions which are then used to train a
verifier which can check for action applicability. In the presence of diverse
sampling from a generator and a verifier which can prune invalid trajectories,
we show significant gains in the success rate on the Blocksworld domain.
Additionally, we show that finetuning the GPT-2 generator itself to create the
verifier generalizes better than finetuning the base GPT-2. Lastly, we
investigate the role of the sampling temperature which can be used to control
the exploration-exploitation tradeoff.",plan-generation,"language-translation, plan-generation, tool-integration",2
Fast and Slow Planning,"The concept of Artificial Intelligence has gained a lot of attention over the
last decade. In particular, AI-based tools have been employed in several
scenarios and are, by now, pervading our everyday life. Nonetheless, most of
these systems lack many capabilities that we would naturally consider to be
included in a notion of ""intelligence"". In this work, we present an
architecture that, inspired by the cognitive theory known as Thinking Fast and
Slow by D. Kahneman, is tasked with solving planning problems in different
settings, specifically: classical and multi-agent epistemic. The system
proposed is an instance of a more general AI paradigm, referred to as SOFAI
(for Slow and Fast AI). SOFAI exploits multiple solving approaches, with
different capabilities that characterize them as either fast or slow, and a
metacognitive module to regulate them. This combination of components, which
roughly reflects the human reasoning process according to D. Kahneman, allowed
us to enhance the reasoning process that, in this case, is concerned with
planning in two different settings. The behavior of this system is then
compared to state-of-the-art solvers, showing that the newly introduced system
presents better results in terms of generality, solving a wider set of problems
with an acceptable trade-off between solving times and solution accuracy.",plan-generation,"language-translation, plan-generation",2
Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,"Long-horizon task planning is essential for the development of intelligent
assistive and service robots. In this work, we investigate the applicability of
a smaller class of large language models (LLMs), specifically GPT-2, in robotic
task planning by learning to decompose tasks into subgoal specifications for a
planner to execute sequentially. Our method grounds the input of the LLM on the
domain that is represented as a scene graph, enabling it to translate human
requests into executable robot plans, thereby learning to reason over
long-horizon tasks, as encountered in the ALFRED benchmark. We compare our
approach with classical planning and baseline methods to examine the
applicability and generalizability of LLM-based planners. Our findings suggest
that the knowledge stored in an LLM can be effectively grounded to perform
long-horizon task planning, demonstrating the promising potential for the
future application of neuro-symbolic planning methods in robotics.",plan-generation,"brain-inspired-planning, language-translation, plan-generation",2
PDDL planning with pretrained large language models,"We study few-shot prompting of pretrained large language models (LLMs) towards solving PDDL planning problems. We are interested in two questions: (1)
To what extent can LLMs solve PDDL planning problems on their own? (2) How
and to what extent can LLMs be used to guide AI planners? Recent work by
Valmeekam et al. (2022) presents negative evidence for (1) in the classic blocks
world domain. We confirm this finding, but expand the inquiry to 18 domains and
find more mixed results with a few clear successes. For (2), we propose a simple
mechanism for using good-but-imperfect LLM outputs to aid a heuristic-search
planner. We also find that the LLM performance is due not only to syntactic pattern matching, but also to its commonsense understanding of English terms that
appear in the PDDL. Code: https://tinyurl.com/llm4pddl",plan-generation,"language-translation, plan-generation",2
Reasoning with language model is planning with world model,"Large language models (LLMs) have shown remarkable reasoning capabilities,
especially when prompted to generate intermediate reasoning steps (e.g.,
Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are
easy for humans, such as generating action plans for executing tasks in a given
environment, or performing complex math, logical, and commonsense reasoning.
The deficiency stems from the key fact that LLMs lack an internal
$\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment
status, intermediate variable values) and simulate long-term outcomes of
actions. This prevents LLMs from performing deliberate planning akin to human
brains, which involves exploring alternative reasoning paths, anticipating
future states and rewards, and iteratively refining existing reasoning steps.
To overcome the limitations, we propose a new LLM reasoning framework,
$\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning
$\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning
agent, and incorporates a principled planning algorithm (based on Monto Carlo
Tree Search) for strategic exploration in the vast reasoning space. During
reasoning, the LLM (as agent) incrementally builds a reasoning tree under the
guidance of the LLM (as world model) and task-specific rewards, and obtains a
high-reward reasoning path efficiently with a proper balance between
exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of
challenging reasoning problems including plan generation, math reasoning, and
logical inference. Empirical results on these tasks demonstrate the superiority
of RAP over various strong baselines, including CoT and least-to-most prompting
with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%
relative improvement in a plan generation setting.",plan-generation,"heuristics-optimization, language-translation, plan-generation",2
Distilling Script Knowledge from Large Language Models for Constrained Language Planning,"In everyday life, humans often plan their actions by following step-by-step
instructions in the form of goal-oriented scripts. Previous work has exploited
language models (LMs) to plan for abstract goals of stereotypical activities
(e.g., ""make a cake""), but leaves more specific goals with multi-facet
constraints understudied (e.g., ""make a cake for diabetics""). In this paper, we
define the task of constrained language planning for the first time. We propose
an overgenerate-then-filter approach to improve large language models (LLMs) on
this task, and use it to distill a novel constrained language planning dataset,
CoScript, which consists of 55,000 scripts. Empirical results demonstrate that
our method significantly improves the constrained language planning ability of
LLMs, especially on constraint faithfulness. Furthermore, CoScript is
demonstrated to be quite effective in endowing smaller LMs with constrained
language planning ability.",plan-generation,"language-translation, plan-generation",2
Strategic Reasoning with Language Models,"Strategic reasoning enables agents to cooperate, communicate, and compete
with other agents in diverse situations. Existing approaches to solving
strategic games rely on extensive training, yielding strategies that do not
generalize to new scenarios or games without retraining. Large Language Models
(LLMs), with their ability to comprehend and generate complex, context-rich
language, could prove powerful as tools for strategic gameplay. This paper
introduces an approach that uses pretrained LLMs with few-shot chain-of-thought
examples to enable strategic reasoning for AI agents. Our approach uses
systematically generated demonstrations of reasoning about states, values, and
beliefs to prompt the model. Using extensive variations of simple matrix games,
we show that strategies that are derived based on systematically generated
prompts generalize almost perfectly to new game structures, alternate
objectives, and hidden information. Additionally, we demonstrate our approach
can lead to human-like negotiation strategies in realistic scenarios without
any extra training or fine-tuning. Our results highlight the ability of LLMs,
guided by systematic reasoning demonstrations, to adapt and excel in diverse
strategic scenarios.",plan-generation,"language-translation, multiagent-planning",2
CoPAL: Corrective Planning of Robot Actions with Large Language Models,"In the pursuit of fully autonomous robotic systems capable of taking over
tasks traditionally performed by humans, the complexity of open-world
environments poses a considerable challenge. Addressing this imperative, this
study contributes to the field of Large Language Models (LLMs) applied to task
and motion planning for robots. We propose a system architecture that
orchestrates a seamless interplay between multiple cognitive levels,
encompassing reasoning, planning, and motion generation. At its core lies a
novel replanning strategy that handles physically grounded, logical, and
semantic errors in the generated plans. We demonstrate the efficacy of the
proposed feedback architecture, particularly its impact on executability,
correctness, and time complexity via empirical evaluation in the context of a
simulation and two intricate real-world scenarios: blocks world, barman and
pizza preparation.",plan-generation,"brain-inspired-planning, language-translation, multiagent-planning, plan-generation",2
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers,"For effective human-robot interaction, robots need to understand, plan, and
execute complex, long-horizon tasks described by natural language. Recent
advances in large language models (LLMs) have shown promise for translating
natural language into robot action sequences for complex tasks. However,
existing approaches either translate the natural language directly into robot
trajectories or factor the inference process by decomposing language into task
sub-goals and relying on a motion planner to execute each sub-goal. When
complex environmental and temporal constraints are involved, inference over
planning tasks must be performed jointly with motion plans using traditional
task-and-motion planning (TAMP) algorithms, making factorization into subgoals
untenable. Rather than using LLMs to directly plan task sub-goals, we instead
perform few-shot translation from natural language task descriptions to an
intermediate task representation that can then be consumed by a TAMP algorithm
to jointly solve the task and motion plan. To improve translation, we
automatically detect and correct both syntactic and semantic errors via
autoregressive re-prompting, resulting in significant improvements in task
completion. We show that our approach outperforms several methods using LLMs as
planners in complex task domains. See our project website
https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.",plan-generation,"language-translation, plan-generation",2
Planning with Logical Graph-based Language Model for Instruction Generation,"Despite the superior performance of large language models to generate natural
language texts, it is hard to generate texts with correct logic according to a
given task, due to the difficulties for neural models to capture implied rules
from free-form texts. In this paper, we propose a novel graph-based language
model, Logical-GLM, to infuse logic into language models for more valid text
generation and interpretability. Specifically, we first capture information
from natural language instructions and construct logical bayes graphs that
generally describe domains. Next, we generate logical skeletons to guide
language model training, infusing domain knowledge into language models.
Finally, we alternately optimize the searching policy of graphs and language
models until convergence. The experimental results show that Logical-GLM is
both effective and efficient compared with traditional language models, despite
using smaller-scale training data and fewer parameters. Our approach can
generate instructional texts with more correct logic owing to the internalized
domain knowledge. Moreover, the usage of logical graphs reflects the inner
mechanism of the language models, which improves the interpretability of
black-box models.",plan-generation,"language-translation, plan-generation",2
A Framework to Generate Neurosymbolic PDDL-compliant Planners,"Symbolic task planning is a widely used approach to enforce robot autonomy
due to its ease of understanding and deployment in robot architectures.
However, techniques for symbolic task planning are difficult to scale in
real-world, human-robot collaboration scenarios because of the poor performance
in complex planning domains or when frequent re-planning is needed. We present
a framework, Teriyaki, specifically aimed at bridging the gap between symbolic
task planning and machine learning approaches. The rationale is training Large
Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner
compatible with the Planning Domain Definition Language (PDDL), and then
leveraging its generative capabilities to overcome a number of limitations
inherent to symbolic task planners. Potential benefits include (i) a better
scalability in so far as the planning domain complexity increases, since LLMs'
response time linearly scales with the combined length of the input and the
output, and (ii) the ability to synthesize a plan action-by-action instead of
end-to-end, making each action available for execution as soon as it is
generated instead of waiting for the whole plan to be available, which in turn
enables concurrent planning and execution. Recently, significant efforts have
been devoted by the research community to evaluate the cognitive capabilities
of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an
overall planning performance comparable to traditional planners in specific
planning domains, while leveraging LLMs capabilities to build a look-ahead
predictive planning model. Preliminary results in selected domains show that
our method can: (i) solve 95.5% of problems in a test data set of 1,000
samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic
planner; (iii) reduce average overall waiting times for a plan availability by
up to 61.4%",plan-generation,"language-translation, plan-generation",2
"On the Planning, Search, and Memorization Capabilities of Large Language Models","The rapid advancement of large language models, such as the Generative
Pre-trained Transformer (GPT) series, has had significant implications across
various disciplines. In this study, we investigate the potential of the
state-of-the-art large language model (GPT-4) for planning tasks. We explore
its effectiveness in multiple planning subfields, highlighting both its
strengths and limitations. Through a comprehensive examination, we identify
areas where large language models excel in solving planning problems and reveal
the constraints that limit their applicability. Our empirical analysis focuses
on GPT-4's performance in planning domain extraction, graph search path
planning, and adversarial planning. We then propose a way of fine-tuning a
domain-specific large language model to improve its Chain of Thought (CoT)
capabilities for the above-mentioned tasks. The results provide valuable
insights into the potential applications of large language models in the
planning domain and pave the way for future research to overcome their
limitations and expand their capabilities.",plan-generation,"language-translation, plan-generation",2
Llm-planner: Few-shot grounded planning for embodied agents with large language models,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner",plan-generation,"language-translation, plan-generation, tool-integration",2
Dynamic Planning with a LLM,"While Large Language Models (LLMs) can solve many NLP tasks in zero-shot
settings, applications involving embodied agents remain problematic. In
particular, complex plans that require multi-step reasoning become difficult
and too costly as the context window grows. Planning requires understanding the
likely effects of one's actions and identifying whether the current environment
satisfies the goal state. While symbolic planners find optimal solutions
quickly, they require a complete and accurate representation of the planning
problem, severely limiting their use in practical scenarios. In contrast,
modern LLMs cope with noisy observations and high levels of uncertainty when
reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a
neuro-symbolic framework where an LLM works hand-in-hand with a traditional
planner to solve an embodied task. Given action-descriptions, LLM-DP solves
Alfworld faster and more efficiently than a naive LLM ReAct baseline.",plan-generation,"language-translation, plan-generation",2
Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning,"Recent text-to-video (T2V) generation methods have seen significant
advancements. However, the majority of these works focus on producing short
video clips of a single event (i.e., single-scene videos). Meanwhile, recent
large language models (LLMs) have demonstrated their capability in generating
layouts and programs to control downstream visual modules. This prompts an
important question: can we leverage the knowledge embedded in these LLMs for
temporally consistent long video generation? In this paper, we propose
VideoDirectorGPT, a novel framework for consistent multi-scene video generation
that uses the knowledge of LLMs for video content planning and grounded video
generation. Specifically, given a single text prompt, we first ask our video
planner LLM (GPT-4) to expand it into a 'video plan', which includes the scene
descriptions, the entities with their respective layouts, the background for
each scene, and consistency groupings of the entities. Next, guided by this
video plan, our video generator, named Layout2Vid, has explicit control over
spatial layouts and can maintain temporal consistency of entities across
multiple scenes, while being trained only with image-level annotations. Our
experiments demonstrate that our proposed VideoDirectorGPT framework
substantially improves layout and movement control in both single- and
multi-scene video generation and can generate multi-scene videos with
consistency, while achieving competitive performance with SOTAs in open-domain
single-scene T2V generation. Detailed ablation studies, including dynamic
adjustment of layout control strength with an LLM and video generation with
user-provided images, confirm the effectiveness of each component of our
framework and its future potential.",plan-generation,"language-translation, plan-generation",2
On the planning abilities of large language models (a critical investigation with a proposed benchmark),"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) how good LLMs are by themselves in
generating and validating simple plans in commonsense planning tasks (of the
type that humans are generally quite good at) and (2) how good LLMs are in
being a source of heuristic guidance for other agents--either AI planners or
human planners--in their planning tasks. To investigate these questions in a
systematic rather than anecdotal manner, we start by developing a benchmark
suite based on the kinds of domains employed in the International Planning
Competition. On this benchmark, we evaluate LLMs in three modes: autonomous,
heuristic and human-in-the-loop. Our results show that LLM's ability to
autonomously generate executable plans is quite meager, averaging only about 3%
success rate. The heuristic and human-in-the-loop modes show slightly more
promise. In addition to these results, we also make our benchmark and
evaluation tools available to support investigations by research community.",plan-generation,"language-translation, plan-generation",2
Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,"Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.",plan-generation,"language-translation, plan-generation",2
ProgPrompt: program generation for situated robot task planning using large language models,"Task planning can require defining myriad domain knowledge about the world in
which a robot needs to act. To ameliorate that effort, large language models
(LLMs) can be used to score potential next actions during task planning, and
even generate action sequences directly, given an instruction in natural
language with no additional domain information. However, such methods either
require enumerating all possible next steps for scoring, or generate free-form
text that may contain actions not possible on a given robot in its current
context. We present a programmatic LLM prompt structure that enables plan
generation functional across situated environments, robot capabilities, and
tasks. Our key insight is to prompt the LLM with program-like specifications of
the available actions and objects in an environment, as well as with example
programs that can be executed. We make concrete recommendations about prompt
structure and generation constraints through ablation experiments, demonstrate
state of the art success rates in VirtualHome household tasks, and deploy our
method on a physical robot arm for tabletop tasks. Website at
progprompt.github.io",plan-generation,"language-translation, plan-generation",2
"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents","We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
""$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect"" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",plan-generation,"language-translation, multiagent-planning, plan-generation",2
Language models as zero-shot planners: Extracting actionable knowledge for embodied agents,"Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. ""make
breakfast""), to a chosen set of actionable steps (e.g. ""open fridge""). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner",plan-generation,"language-translation, plan-generation",2
Saynav: Grounding large language models for dynamic planning to navigation in new environments,"Semantic reasoning and dynamic planning capabilities are crucial for an
autonomous agent to perform complex navigation tasks in unknown environments.
It requires a large amount of common-sense knowledge, that humans possess, to
succeed in these tasks. We present SayNav, a new approach that leverages human
knowledge from Large Language Models (LLMs) for efficient generalization to
complex navigation tasks in unknown large-scale environments. SayNav uses a
novel grounding mechanism, that incrementally builds a 3D scene graph of the
explored environment as inputs to LLMs, for generating feasible and
contextually appropriate high-level plans for navigation. The LLM-generated
plan is then executed by a pre-trained low-level planner, that treats each
planned step as a short-distance point-goal navigation sub-task. SayNav
dynamically generates step-by-step instructions during navigation and
continuously refines future steps based on newly perceived information. We
evaluate SayNav on multi-object navigation (MultiON) task, that requires the
agent to utilize a massive amount of human knowledge to efficiently search
multiple different objects in an unknown environment. We also introduce a
benchmark dataset for MultiON task employing ProcTHOR framework that provides
large photo-realistic indoor environments with variety of objects. SayNav
achieves state-of-the-art results and even outperforms an oracle based baseline
with strong ground-truth assumptions by more than 8% in terms of success rate,
highlighting its ability to generate dynamic plans for successfully locating
objects in large-scale new environments. The code, benchmark dataset and
demonstration videos are accessible at
https://www.sri.com/ics/computer-vision/saynav.",plan-generation,"heuristics-optimization, interactive-planning, language-translation, model-construction, plan-generation, tool-integration",2
Task and motion planning with large language models for object rearrangement,"Multi-object rearrangement is a crucial skill for service robots, and
commonsense reasoning is frequently needed in this process. However, achieving
commonsense arrangements requires knowledge about objects, which is hard to
transfer to robots. Large language models (LLMs) are one potential source of
this knowledge, but they do not naively capture information about plausible
physical arrangements of the world. We propose LLM-GROP, which uses prompting
to extract commonsense knowledge about semantically valid object configurations
from an LLM and instantiates them with a task and motion planner in order to
generalize to varying scene geometry. LLM-GROP allows us to go from
natural-language commands to human-aligned object rearrangement in varied
environments. Based on human evaluations, our approach achieves the highest
rating while outperforming competitive baselines in terms of success rate while
maintaining comparable cumulative action costs. Finally, we demonstrate a
practical implementation of LLM-GROP on a mobile manipulator in real-world
scenarios. Supplementary materials are available at:
https://sites.google.com/view/llm-grop",plan-generation,"brain-inspired-planning, language-translation, plan-generation",2
SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"In this work, we introduce SMART-LLM, an innovative framework designed for
embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task
Planning using Large Language Models (LLMs), harnesses the power of LLMs to
convert high-level task instructions provided as input into a multi-robot task
plan. It accomplishes this by executing a series of stages, including task
decomposition, coalition formation, and task allocation, all guided by
programmatic LLM prompts within the few-shot prompting paradigm. We create a
benchmark dataset designed for validating the multi-robot task planning
problem, encompassing four distinct categories of high-level instructions that
vary in task complexity. Our evaluation experiments span both simulation and
real-world scenarios, demonstrating that the proposed model can achieve
promising results for generating multi-robot task plans. The experimental
videos, code, and datasets from the work can be found at
https://sites.google.com/view/smart-llm/.",plan-generation,"language-translation, plan-generation",2
Multimodal Procedural Planning via Dual Text-Image Prompting,"Embodied agents have achieved prominent performance in following human
instructions to complete tasks. However, the potential of providing
instructions informed by texts and images to assist humans in completing tasks
remains underexplored. To uncover this capability, we present the multimodal
procedural planning (MPP) task, in which models are given a high-level goal and
generate plans of paired text-image steps, providing more complementary and
informative guidance than unimodal plans. The key challenges of MPP are to
ensure the informativeness, temporal coherence,and accuracy of plans across
modalities. To tackle this, we propose Text-Image Prompting (TIP), a
dual-modality prompting method that jointly leverages zero-shot reasoning
ability in large language models (LLMs) and compelling text-to-image generation
ability from diffusion-based models. TIP improves the interaction in the dual
modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs
to guide the textual-grounded image plan generation and leveraging the
descriptions of image plans to ground the textual plan reversely. To address
the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed
for MPP. Our results show compelling human preferences and automatic scores
against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms
of informativeness, temporal coherence, and plan accuracy. Our code and data:
https://github.com/YujieLu10/MPP.",plan-generation,"language-translation, plan-generation, tool-integration",2
"Plan, Eliminate, and Track--Language Models are Good Teachers for Embodied Agents","Pre-trained large language models (LLMs) capture procedural knowledge about
the world. Recent work has leveraged LLM's ability to generate abstract plans
to simplify challenging control tasks, either by action scoring, or action
modeling (fine-tuning). However, the transformer architecture inherits several
constraints that make it difficult for the LLM to directly serve as the agent:
e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,
and incompatibility with non-text environments. To maintain compatibility with
a low-level trainable actor, we propose to instead use the knowledge in LLMs to
simplify the control problem, rather than solving it. We propose the Plan,
Eliminate, and Track (PET) framework. The Plan module translates a task
description into a list of high-level sub-tasks. The Eliminate module masks out
irrelevant objects and receptacles from the observation for the current
sub-task. Finally, the Track module determines whether the agent has
accomplished each sub-task. On the AlfWorld instruction following benchmark,
the PET framework leads to a significant 15% improvement over SOTA for
generalization to human goal specifications.",plan-generation,"language-translation, multiagent-planning, plan-generation",2
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help,"This paper addresses planning problems for mobile robots. We consider
missions that require accomplishing multiple high-level sub-tasks, expressed in
natural language (NL), in a temporal and logical order. To formally define the
mission, we treat these sub-tasks as atomic predicates in a Linear Temporal
Logic (LTL) formula. We refer to this task specification framework as LTL-NL.
Our goal is to design plans, defined as sequences of robot actions,
accomplishing LTL-NL tasks. This action planning problem cannot be solved
directly by existing LTL planners because of the NL nature of atomic
predicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolic
planner that relies on a novel integration of (i) existing symbolic planners
generating high-level task plans determining the order at which the NL
sub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)
to design sequences of robot actions based on these task plans; and (iii)
conformal prediction acting as a formal interface between (i) and (ii) and
managing uncertainties due to LLM imperfections. We show, both theoretically
and empirically, that HERACLEs can achieve user-defined mission success rates.
Finally, we provide comparative experiments demonstrating that HERACLEs
outperforms LLM-based planners that require the mission to be defined solely
using NL. Additionally, we present examples demonstrating that our approach
enhances user-friendliness compared to conventional symbolic approaches.",plan-generation,"language-translation, plan-generation",2
Guiding language model reasoning with planning tokens,"Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
(CoT) reasoning. However, most of the existing approaches to enhance this
ability rely heavily on data-driven methods, while neglecting the structural
aspects of the model's reasoning capacity. To encourage a more structural
generation of CoT steps, we propose a hierarchical generation scheme: we let
the LM generate a planning token at the start of each reasoning step,
intuitively serving as a high-level plan of the current step, and add their
embeddings to the model parameters. Our approach requires a negligible increase
in trainable parameters (0.001%) and can be applied through either full
fine-tuning or a more parameter-efficient scheme. We demonstrate our method's
effectiveness by applying it to three different LLMs, showing notable accuracy
improvements across three math word problem datasets and one multihop QA
dataset with respect to standard fine-tuning baselines.",plan-generation,"language-translation, plan-generation",2
DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs,"Mobile robots often rely on pre-existing maps for effective path planning and
navigation. However, when these maps are unavailable, particularly in
unfamiliar environments, a different approach become essential. This paper
introduces DynaCon, a novel system designed to provide mobile robots with
contextual awareness and dynamic adaptability during navigation, eliminating
the reliance of traditional maps. DynaCon integrates real-time feedback with an
object server, prompt engineering, and navigation modules. By harnessing the
capabilities of Large Language Models (LLMs), DynaCon not only understands
patterns within given numeric series but also excels at categorizing objects
into matched spaces. This facilitates dynamic path planner imbued with
contextual awareness. We validated the effectiveness of DynaCon through an
experiment where a robot successfully navigated to its goal using reasoning.
Source code and experiment videos for this work can be found at:
https://sites.google.com/view/dynacon.",plan-generation,"language-translation, multiagent-planning, plan-generation",3
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models,"In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning",plan-generation,"brain-inspired-planning, language-translation, plan-generation",3
Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning,"For robots to perform a wide variety of tasks, they require a 3D
representation of the world that is semantically rich, yet compact and
efficient for task-driven perception and planning. Recent approaches have
attempted to leverage features from large vision-language models to encode
semantics in 3D representations. However, these approaches tend to produce maps
with per-point feature vectors, which do not scale well in larger environments,
nor do they contain semantic spatial relationships between entities in the
environment, which are useful for downstream planning. In this work, we propose
ConceptGraphs, an open-vocabulary graph-structured representation for 3D
scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing
their output to 3D by multi-view association. The resulting representations
generalize to novel semantic classes, without the need to collect large 3D
datasets or finetune models. We demonstrate the utility of this representation
through a number of downstream planning tasks that are specified through
abstract (language) prompts and require complex reasoning over spatial and
semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer
video: https://youtu.be/mRhNkQwRYnc )",plan-generation,"language-translation, plan-generation",3
Bootstrap your own skills: Learning to solve new tasks with large language model guidance,"We propose BOSS, an approach that automatically learns to solve new
long-horizon, complex, and meaningful tasks by growing a learned skill library
with minimal supervision. Prior work in reinforcement learning require expert
supervision, in the form of demonstrations or rich reward functions, to learn
long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)
learns to accomplish new tasks by performing ""skill bootstrapping,"" where an
agent with a set of primitive skills interacts with the environment to practice
new skills without receiving reward feedback for tasks outside of the initial
skill set. This bootstrapping phase is guided by large language models (LLMs)
that inform the agent of meaningful skills to chain together. Through this
process, BOSS builds a wide range of complex and useful behaviors from a basic
set of primitive skills. We demonstrate through experiments in realistic
household environments that agents trained with our LLM-guided bootstrapping
procedure outperform those trained with naive bootstrapping as well as prior
unsupervised skill acquisition methods on zero-shot execution of unseen,
long-horizon tasks in new environments. Website at clvrai.com/boss.",plan-generation,"language-translation, plan-generation",3
Housekeep: Tidying virtual households using commonsense reasoning,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",plan-generation,"language-translation, plan-generation",3
Reasoning on graphs: Faithful and interpretable large language model reasoning,"Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.",plan-generation,"language-translation, plan-generation",3
Understanding the Capabilities of Large Language Models for Automated Planning,"Automated planning is concerned with developing efficient algorithms to
generate plans or sequences of actions to achieve a specific goal in a given
environment. Emerging Large Language Models (LLMs) can answer questions, write
high-quality programming code, and predict protein folding, showcasing their
versatility in solving various tasks beyond language-based problems. In this
paper, we aim to explore how LLMs can also be used for automated planning. To
do so, we seek to answer four key questions. Firstly, we want to understand the
extent to which LLMs can be used for plan generation. Secondly, we aim to
identify which pre-training data is most effective in facilitating plan
generation. Thirdly, we investigate whether fine-tuning or prompting is a more
effective approach for plan generation. Finally, we explore whether LLMs are
capable of plan generalization. By answering these questions, the study seeks
to shed light on the capabilities of LLMs in solving complex planning problems
and provide insights into the most effective approaches for using LLMs in this
context.",plan-generation,"language-translation, plan-generation",3
Grounded decoding: Guiding text generation with grounded models for robot control,"Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project's website can be found at
grounded-decoding.github.io.",plan-generation,"language-translation, plan-generation",3
Evaluation of Pretrained Large Language Models in Embodied Planning Tasks,"Modern pretrained large language models (LLMs) are increasingly being used in zero-shot or few-shot learning modes. Recent years have seen increased interest in applying such models to embodied artificial intelligence and robotics tasks. When given in a natural language, the agent needs to build a plan based on this prompt. The best solutions use LLMs through APIs or models that are not publicly available, making it difficult to reproduce the results. In this paper, we use publicly available LLMs to build a plan for an embodied agent and evaluate them in three modes of operation: 1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3) the step-by-step autoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples of different tasks. Through extensive experiments, we have shown that the subtask evaluation mode, in most cases, outperforms others with a task-specific prompt, whereas the step-by-step autoregressive plan generation posts better performance in the mixed prompt setting.

",plan-generation,"language-translation, plan-generation",3
Neuro-symbolic causal language planning with commonsense prompting,"Procedural planning aims to implement complex high-level goals by
decomposition into sequential simpler low-level steps. Although procedural
planning is a basic skill set for humans in daily life, it remains a challenge
for large language models (LLMs) that lack a deep understanding of the
cause-effect relations in procedures. Previous methods require manual exemplars
to acquire procedural planning knowledge from LLMs in the zero-shot setting.
However, such elicited pre-trained knowledge in LLMs induces spurious
correlations between goals and steps, which impair the model generalization to
unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural
PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with
commonsense-infused prompting. To mitigate spurious goal-step correlations, we
use symbolic program executors on the latent procedural representations to
formalize prompts from commonsense knowledge bases as a causal intervention
toward the Structural Causal Model. Both automatic and human evaluations on
WikiHow and RobotHow show the superiority of PLAN on procedural planning
without further training or manual exemplars.",plan-generation,"language-translation, plan-generation",3
Human-Assisted Continual Robot Learning with Foundation Models,"Large Language Models (LLMs) have been shown to act like planners that can
decompose high-level instructions into a sequence of executable instructions.
However, current LLM-based planners are only able to operate with a fixed set
of skills. We overcome this critical limitation and present a method for using
LLM-based planners to query new skills and teach robots these skills in a data
and time-efficient manner for rigid object manipulation. Our system can re-use
newly acquired skills for future tasks, demonstrating the potential of open
world and lifelong learning. We evaluate the proposed framework on multiple
tasks in simulation and the real world. Videos are available at:
https://sites.google.com/mit.edu/halp-robot-learning.",plan-generation,"language-translation, plan-generation",3
Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,"Despite recent success in large language model (LLM) reasoning, LLMs struggle
with hierarchical multi-step reasoning tasks like generating complex programs.
For these tasks, humans often start with a high-level algorithmic design and
implement each part gradually. We introduce Parsel, a framework enabling
automatic implementation and validation of complex algorithms with code LLMs.
With Parsel, we automatically decompose algorithmic tasks into hierarchical
natural language function descriptions and then search over combinations of
possible function implementations using tests. We show that Parsel can be used
across domains requiring hierarchical reasoning, including program synthesis
and robotic planning. We find that, using Parsel, LLMs solve more
competition-level problems in the APPS dataset, resulting in pass rates over
75\% higher than prior results from directly sampling AlphaCode and Codex,
while often using a smaller sample budget. Moreover, with automatically
generated tests, we find that Parsel can improve the state-of-the-art pass@1
performance on HumanEval from 67\% to 85\%. We also find that LLM-generated
robotic plans using Parsel are more than twice as likely to be considered
accurate than directly generated plans. Lastly, we explore how Parsel addresses
LLM limitations and discuss how Parsel may be useful for human programmers. We
release our code at https://github.com/ezelikman/parsel",plan-generation,"language-translation, plan-generation",3
Graph of thoughts: Solving elaborate problems with large language models,"We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information (""LLM thoughts"") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by >31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.",plan-generation,"language-translation, plan-generation",3
Voxposer: Composable 3d value maps for robotic manipulation with language models,"Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a vision-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Videos and code at https://voxposer.github.io",plan-generation,"language-translation, plan-generation",3
Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks,"Large Language Models (LLMs) are highly capable of performing
planning for long-horizon robotics tasks, yet existing methods require access to
a pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating).
However, LLM planning does not address how to design or learn those behaviors,
which remains challenging particularly in long-horizon settings. Furthermore,
for many tasks of interest, the robot needs to be able to adjust its behavior in a
fine-grained manner, requiring the agent to be capable of modifying low-level
control actions. Can we instead use the internet-scale knowledge from LLMs for
high-level policies, guiding reinforcement learning (RL) policies to efficiently solve
robotic control tasks online without requiring a pre-determined set of skills? In this
paper, we propose Plan-Seq-Learn (PSL): a modular approach that uses motion
planning to bridge the gap between abstract language and learned low-level control
for solving long-horizon robotics tasks from scratch. We demonstrate that PSL is
capable of solving 20+ challenging single and multi-stage robotics tasks on four
benchmarks at success rates of over 80% from raw visual input, out-performing
language-based, classical, and end-to-end approaches. Video results and code at
mihdalal.github.io/planseqlearn.",plan-generation,"brain-inspired-planning, language-translation, plan-generation",3
Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models,"Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
""Let's think step by step"" as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",plan-generation,"language-translation, plan-generation",3
Tree of thoughts: Deliberate problem solving with large language models,"Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.",plan-generation,"language-translation, multiagent-planning, plan-generation",3
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change),"Generating plans of action, and reasoning about change have long been
considered a core competence of intelligent agents. It is thus no surprise that
evaluating the planning and reasoning capabilities of large language models
(LLMs) has become a hot topic of research. Most claims about LLM planning
capabilities are however based on common sense tasks-where it becomes hard to
tell whether LLMs are planning or merely retrieving from their vast world
knowledge. There is a strong need for systematic and extensible planning
benchmarks with sufficient diversity to evaluate whether LLMs have innate
planning capabilities. Motivated by this, we propose PlanBench, an extensible
benchmark suite based on the kinds of domains used in the automated planning
community, especially in the International Planning Competition, to test the
capabilities of LLMs in planning or reasoning about actions and change.
PlanBench provides sufficient diversity in both the task domains and the
specific planning capabilities. Our studies also show that on many critical
capabilities-including plan generation-LLM performance falls quite short, even
with the SOTA models. PlanBench can thus function as a useful marker of
progress of LLMs in planning and reasoning.",plan-generation,"language-translation, plan-generation",3
Can Large Language Models Really Improve by Self-critiquing Their Own Plans?,"There have been widespread claims about Large Language Models (LLMs) being
able to successfully verify or self-critique their candidate solutions in
reasoning problems in an iterative mode. Intrigued by those claims, in this
paper we set out to investigate the verification/self-critiquing abilities of
large language models in the context of planning. We evaluate a planning system
that employs LLMs for both plan generation and verification. We assess the
verifier LLM's performance against ground-truth verification, the impact of
self-critiquing on plan generation, and the influence of varying feedback
levels on system performance. Using GPT-4, a state-of-the-art LLM, for both
generation and verification, our findings reveal that self-critiquing appears
to diminish plan generation performance, especially when compared to systems
with external, sound verifiers and the LLM verifiers in that system produce a
notable number of false positives, compromising the system's reliability.
Additionally, the nature of feedback, whether binary or detailed, showed
minimal impact on plan generation. Collectively, our results cast doubt on the
effectiveness of LLMs in a self-critiquing, iterative framework for planning
tasks.",plan-generation,"language-translation, plan-generation",3
Generating executable action plans with environmentally-aware language models,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable,
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that agents are better able to execute. Our approach involves integrating
environmental objects and object relations as additional inputs into LLM action
plan generation to provide the system with an awareness of its surroundings,
resulting in plans where each generated action is mapped to objects present in
the scene. We also design a novel scoring function that, along with generating
the action steps and associating them with objects, helps the system
disambiguate among object instances and take into account their states. We
evaluated our approach using the VirtualHome simulator and the ActivityPrograms
knowledge base and found that action plans generated from our system had a 310%
improvement in executability and a 147% improvement in correctness over prior
work. The complete code and a demo of our method is publicly available at
https://github.com/hri-ironlab/scene_aware_language_planner.",plan-generation,"language-translation, plan-generation",3
Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling,"Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.",model-construction,"language-translation, plan-generation",3
Reasoning with language model is planning with world model,"Large language models (LLMs) have shown remarkable reasoning capabilities,
especially when prompted to generate intermediate reasoning steps (e.g.,
Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are
easy for humans, such as generating action plans for executing tasks in a given
environment, or performing complex math, logical, and commonsense reasoning.
The deficiency stems from the key fact that LLMs lack an internal
$\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment
status, intermediate variable values) and simulate long-term outcomes of
actions. This prevents LLMs from performing deliberate planning akin to human
brains, which involves exploring alternative reasoning paths, anticipating
future states and rewards, and iteratively refining existing reasoning steps.
To overcome the limitations, we propose a new LLM reasoning framework,
$\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning
$\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning
agent, and incorporates a principled planning algorithm (based on Monto Carlo
Tree Search) for strategic exploration in the vast reasoning space. During
reasoning, the LLM (as agent) incrementally builds a reasoning tree under the
guidance of the LLM (as world model) and task-specific rewards, and obtains a
high-reward reasoning path efficiently with a proper balance between
exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of
challenging reasoning problems including plan generation, math reasoning, and
logical inference. Empirical results on these tasks demonstrate the superiority
of RAP over various strong baselines, including CoT and least-to-most prompting
with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%
relative improvement in a plan generation setting.",model-construction,"language-translation, plan-generation",3
Large language models as zero-shot human models for human-robot interaction,"Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.",model-construction,"language-translation, plan-generation",3
From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought,"How does language inform our downstream thinking? In particular, how do
humans make meaning from language--and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose rational meaning construction, a computational framework
for language-informed thinking that combines neural language models with
probabilistic models for rational inference. We frame linguistic meaning as a
context-sensitive mapping from natural language into a probabilistic language
of thought (PLoT)--a general-purpose symbolic substrate for generative world
modeling. Our architecture integrates two computational tools that have not
previously come together: we model thinking with probabilistic programs, an
expressive representation for commonsense reasoning; and we model meaning
construction with large language models (LLMs), which support broad-coverage
translation from natural language utterances to code expressions in a
probabilistic programming language. We illustrate our framework through
examples covering four core domains from cognitive science: probabilistic
reasoning, logical and relational reasoning, visual and physical reasoning, and
social reasoning. In each, we show that LLMs can generate context-sensitive
translations that capture pragmatically-appropriate linguistic meanings, while
Bayesian inference with the generated programs supports coherent and robust
commonsense reasoning. We extend our framework to integrate
cognitively-motivated symbolic modules (physics simulators, graphics engines,
and planning algorithms) to provide a unified commonsense thinking interface
from language. Finally, we explore how language can drive the construction of
world models themselves. We hope this work will provide a roadmap towards
cognitive models and AI systems that synthesize the insights of both modern and
classical computational perspectives.",model-construction,"language-translation, plan-generation",3
There and back again: extracting formal domains for controllable neurosymbolic story authoring,"Story generators using language models offer the automatic production of highly fluent narrative content, but they are hard to control and understand, seizing creative tasks that many authors wish to perform themselves. On the other hand, planning-based story generators are highly controllable and easily understood but require story domains that must be laboriously crafted; further, they lack the capacity for fluent language generation. In this paper, we explore hybrid approaches that aim to bridge the gap between language models and narrative planners. First, we demonstrate that language models can be used to author narrative planning domains from natural language stories with minimal human intervention. Second, we explore the reverse, demonstrating that we can use logical story domains and plans to produce stories that respect the narrative commitments of the planner. In doing so, we aim to build a foundation for human-centric authoring tools that facilitate novel creative experiences.
",model-construction,"language-translation, plan-generation, tool-integration",3
Exploiting Language Models as a Source of Knowledge for Cognitive Agents,"Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.",model-construction,"language-translation, plan-generation",3
Exploring the Limitations of using Large Language Models to Fix Planning Tasks,"Large language models (LLMs) have revolutionized natural
language processing (NLP), enabling human-like text generation, question answering, and translation. Despite claims
of emergent reasoning capabilities, it has been demonstrated
their lack of planning skills in tasks such as plan generation,
plan reuse or replanning. In this work, we present ongoing
efforts on exploring the limitations of LLMs in another task
requiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks.",model-construction,"language-translation, plan-generation",3
Embodied task planning with large language models,"Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.",model-construction,"brain-inspired-planning, language-translation, plan-generation",3
Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks,"We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.",model-construction,"language-translation, plan-generation",4
Roco: Dialectic multi-robot collaboration with large language models,"We propose a novel approach to multi-robot collaboration that harnesses the
power of pre-trained large language models (LLMs) for both high-level
communication and low-level path planning. Robots are equipped with LLMs to
discuss and collectively reason task strategies. They then generate sub-task
plans and task space waypoint paths, which are used by a multi-arm motion
planner to accelerate trajectory planning. We also provide feedback from the
environment, such as collision checking, and prompt the LLM agents to improve
their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a
6-task benchmark covering a wide range of multi-robot collaboration scenarios,
accompanied by a text-only dataset for agent representation and reasoning. We
experimentally demonstrate the effectiveness of our approach -- it achieves
high success rates across all tasks in RoCoBench and adapts to variations in
task semantics. Our dialog setup offers high interpretability and flexibility
-- in real world experiments, we show RoCo easily incorporates
human-in-the-loop, where a user can communicate and collaborate with a robot
agent to complete tasks together. See project website
https://project-roco.github.io for videos and code.",model-construction,"language-translation, plan-generation",4
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach,"Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.",model-construction,"language-translation, plan-generation, tool-integration",4
Large Language Models as Commonsense Knowledge for Large-Scale Task Planning,"Large-scale task planning is a major challenge. Recent work exploits large language
models (LLMs) directly as a policy and shows surprisingly interesting results. This
paper shows that LLMs provide a commonsense model of the world in addition
to a policy that acts on it. The world model and the policy can be combined in
a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task
planning. In our new LLM-MCTS algorithm, the LLM-induced world model
provides a commonsense prior belief for MCTS to achieve effective reasoning; the
LLM-induced policy acts as a heuristic to guide the search, vastly improving search
efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone
and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex,
novel tasks. Further experiments and analyses on multiple tasks—multiplication,
travel planning, object rearrangement—suggest minimum description length (MDL)
as a general guiding principle: if the description length of the world model is
substantially smaller than that of the policy, using LLM as a world model for
model-based planning is likely better than using LLM solely as a policy.",model-construction,"language-translation, plan-generation",4
Statler: State-maintaining language models for embodied reasoning,"There has been a significant research interest in employing large language
models to empower intelligent robots with complex reasoning. Existing work
focuses on harnessing their abilities to reason about the histories of their
actions and observations. In this paper, we explore a new dimension in which
large language models may benefit robotics planning. In particular, we propose
Statler, a framework in which large language models are prompted to maintain an
estimate of the world state, which are often unobservable, and track its
transition as new actions are taken. Our framework then conditions each action
on the estimate of the current world state. Despite being conceptually simple,
our Statler framework significantly outperforms strong competing methods (e.g.,
Code-as-Policies) on several robot planning tasks. Additionally, it has the
potential advantage of scaling up to more challenging long-horizon planning
tasks.",model-construction,"brain-inspired-planning, language-translation, plan-generation",4
Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning,"Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",model-construction,"language-translation, plan-generation",4
Voxposer: Composable 3d value maps for robotic manipulation with language models,"Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a vision-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Videos and code at https://voxposer.github.io",model-construction,"language-translation, plan-generation",4
Creative Robot Tool Use with Large Language Models,"Tool use is a hallmark of advanced intelligence, exemplified in both animal
behavior and robotic capabilities. This paper investigates the feasibility of
imbuing robots with the ability to creatively use tools in tasks that involve
implicit physical constraints and long-term planning. Leveraging Large Language
Models (LLMs), we develop RoboTool, a system that accepts natural language
instructions and outputs executable code for controlling robots in both
simulated and real-world environments. RoboTool incorporates four pivotal
components: (i) an ""Analyzer"" that interprets natural language to discern key
task-related concepts, (ii) a ""Planner"" that generates comprehensive strategies
based on the language input and key concepts, (iii) a ""Calculator"" that
computes parameters for each skill, and (iv) a ""Coder"" that translates these
plans into executable Python code. Our results show that RoboTool can not only
comprehend explicit or implicit physical constraints and environmental factors
but also demonstrate creative tool use. Unlike traditional Task and Motion
Planning (TAMP) methods that rely on explicit optimization, our LLM-based
system offers a more flexible, efficient, and user-friendly solution for
complex robotics tasks. Through extensive experiments, we validate that
RoboTool is proficient in handling tasks that would otherwise be infeasible
without the creative use of tools, thereby expanding the capabilities of
robotic systems. Demos are available on our project page:
https://creative-robotool.github.io/.",model-construction,"language-translation, plan-generation",4
"Do as i can, not as i say: Grounding language in robotic affordances","Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's ""hands and eyes,"" while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at https://say-can.github.io/.",model-construction,"brain-inspired-planning, language-translation, plan-generation",4
Building cooperative embodied agents modularly with large language models,"In this work, we address challenging multi-agent cooperation problems with
decentralized control, raw sensory observations, costly communication, and
multi-objective tasks instantiated in various embodied environments. While
previous research either presupposes a cost-free communication channel or
relies on a centralized controller with shared observations, we harness the
commonsense knowledge, reasoning ability, language comprehension, and text
generation prowess of LLMs and seamlessly incorporate them into a
cognitive-inspired modular framework that integrates with perception, memory,
and execution. Thus building a Cooperative Embodied Language Agent CoELA, who
can plan, communicate, and cooperate with others to accomplish long-horizon
tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA
driven by GPT-4 can surpass strong planning-based methods and exhibit emergent
effective communication. Though current Open LMs like LLAMA-2 still
underperform, we fine-tune a CoELA with data collected with our agents and show
how they can achieve promising performance. We also conducted a user study for
human-agent interaction and discovered that CoELA communicating in natural
language can earn more trust and cooperate more effectively with humans. Our
research underscores the potential of LLMs for future research in multi-agent
cooperation. Videos can be found on the project website
https://vis-www.cs.umass.edu/Co-LLM-Agents/.",multiagent-planning,"language-translation, multiagent-planning, plan-generation",4
War and peace (waragent): Large language model-based multi-agent simulation of world wars,"Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.",multiagent-planning,"language-translation, plan-generation",4
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents,"Designing versatile graph learning approaches is important, considering the
diverse graphs and tasks existing in real-world applications. Existing methods
have attempted to achieve this target through automated machine learning
techniques, pre-training and fine-tuning strategies, and large language models.
However, these methods are not versatile enough for graph learning, as they
work on either limited types of graphs or a single task. In this paper, we
propose to explore versatile graph learning approaches with LLM-based agents,
and the key insight is customizing the graph learning procedures for diverse
graphs and tasks. To achieve this, we develop several LLM-based agents,
equipped with diverse profiles, tools, functions and human experience. They
collaborate to configure each procedure with task and data-specific settings
step by step towards versatile solutions, and the proposed method is dubbed
GL-Agent. By evaluating on diverse tasks and graphs, the correct results of the
agent and its comparable performance showcase the versatility of the proposed
method, especially in complex scenarios.The low resource cost and the potential
to use open-source LLMs highlight the efficiency of GL-Agent.",multiagent-planning,"interactive-planning, language-translation, plan-generation",4
Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?,"A flurry of recent work has demonstrated that pre-trained large language
models (LLMs) can be effective task planners for a variety of single-robot
tasks. The planning performance of LLMs is significantly improved via prompting
techniques, such as in-context learning or re-prompting with state feedback,
placing new importance on the token budget for the context window. An
under-explored but natural next direction is to investigate LLMs as multi-robot
task planners. However, long-horizon, heterogeneous multi-robot planning
introduces new challenges of coordination while also pushing up against the
limits of context window length. It is therefore critical to find
token-efficient LLM planning frameworks that are also able to reason about the
complexities of multi-robot coordination. In this work, we compare the task
success rate and token efficiency of four multi-agent communication frameworks
(centralized, decentralized, and two hybrid) as applied to four
coordination-dependent multi-agent 2D task scenarios for increasing numbers of
agents. We find that a hybrid framework achieves better task success rates
across all four tasks and scales better to more agents. We further demonstrate
the hybrid frameworks in 3D simulations where the vision-to-text problem and
dynamical errors are considered. See our project website
https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and
code.",multiagent-planning,"language-translation, multiagent-planning, plan-generation",4
Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games,"There is an growing interest in using Large Language Models (LLMs) in
multi-agent systems to tackle interactive real-world tasks that require
effective collaboration and assessing complex situations. Yet, we still have a
limited understanding of LLMs' communication and decision-making abilities in
multi-agent setups. The fundamental task of negotiation spans many key features
of communication, such as cooperation, competition, and manipulation
potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We
create a testbed of complex multi-agent, multi-issue, and semantically rich
negotiation games. To reach an agreement, agents must have strong arithmetic,
inference, exploration, and planning capabilities while integrating them in a
dynamic and multi-turn setup. We propose multiple metrics to rigorously
quantify agents' performance and alignment with the assigned role. We provide
procedures to create new games and increase games' difficulty to have an
evolving benchmark. Importantly, we evaluate critical safety aspects such as
the interaction dynamics between agents influenced by greedy and adversarial
players. Our benchmark is highly challenging; GPT-3.5 and small models mostly
fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.",multiagent-planning,"language-translation, multiagent-planning, plan-generation",4
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,"There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.

",interactive-planning,"language-translation, plan-generation",4
Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,"There have been wide spread claims in the literature about the emergent
reasoning capabilities of Pretrained Large Language Models. However, recent
studies, have found that their ability to plan remains questionable. Through
our experiments using GPT-2, we empirically demonstrate that the performance of
a finetuned baseline remains poor because it violates pre-conditions of actions
in the plans that it generates. To improve the planning capabilities of a
finetuned LLM, we train a verifier, which can classify actions as being valid
or invalid in a particular state. By randomly sampling actions from the same
dataset, we generate examples of invalid actions which are then used to train a
verifier which can check for action applicability. In the presence of diverse
sampling from a generator and a verifier which can prune invalid trajectories,
we show significant gains in the success rate on the Blocksworld domain.
Additionally, we show that finetuning the GPT-2 generator itself to create the
verifier generalizes better than finetuning the base GPT-2. Lastly, we
investigate the role of the sampling temperature which can be used to control
the exploration-exploitation tradeoff.",interactive-planning,"brain-inspired-planning, language-translation, multiagent-planning, plan-generation",4
Grounding large language models in interactive environments with online reinforcement learning,"Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.",interactive-planning,"language-translation, plan-generation",4
ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,"Motivated by the substantial achievements observed in Large Language Models
(LLMs) in the field of natural language processing, recent research has
commenced investigations into the application of LLMs for complex, long-horizon
sequential task planning challenges in robotics. LLMs are advantageous in
offering the potential to enhance the generalizability as task-agnostic
planners and facilitate flexible interaction between human instructors and
planning systems. However, task plans generated by LLMs often lack feasibility
and correctness. To address this challenge, we introduce ISR-LLM, a novel
framework that improves LLM-based planning through an iterative self-refinement
process. The framework operates through three sequential steps: preprocessing,
planning, and iterative self-refinement. During preprocessing, an LLM
translator is employed to convert natural language input into a Planning Domain
Definition Language (PDDL) formulation. In the planning phase, an LLM planner
formulates an initial plan, which is then assessed and refined in the iterative
self-refinement step by using a validator. We examine the performance of
ISR-LLM across three distinct planning domains. The results show that ISR-LLM
is able to achieve markedly higher success rates in task accomplishments
compared to state-of-the-art LLM-based planners. Moreover, it also preserves
the broad applicability and generalizability of working with natural language
instructions.",interactive-planning,"language-translation, plan-generation",4
Diversity of Thought Improves Reasoning Abilities of Large Language Models,"Large language models (LLMs) are documented to struggle in settings that
require complex reasoning. Nevertheless, instructing the model to break down
the problem into smaller reasoning steps, or ensembling various generations
through modifying decoding steps boosts performance. However, these methods
assume that the input prompt is fixed and expect the decoding strategies to
introduce the diversity needed for ensembling. In this work, we discuss how one
can create and leverage variations of the input prompt as a means of diversity
of thought. We propose a method that automatically improves prompt diversity by
soliciting feedback from the LLM to ideate approaches that are apt for the
problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse
reasoning path Self-Ensemble) across multiple inference calls, or use diverse
approaches within a single inference call; we call the latter IDIV-SE (In-call
DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming
prior work, DIV-SE(in particular) advances state-of-the-art performance on the
challenging planning and graph coloring benchmarks. Our results improve the
Pareto frontier of the accuracy-cost trade-off.",interactive-planning,"language-translation, multiagent-planning, plan-generation",4
Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving,"Generative large language models (LLMs) with instruct training such as GPT-4
can follow human-provided instruction prompts and generate human-like responses
to these prompts. Apart from natural language responses, they have also been
found to be effective at generating formal artifacts such as code, plans, and
logical specifications from natural language prompts. Despite their remarkably
improved accuracy, these models are still known to produce factually incorrect
or contextually inappropriate results despite their syntactic coherence - a
phenomenon often referred to as hallucination. This limitation makes it
difficult to use these models to synthesize formal artifacts that are used in
safety-critical applications. Unlike tasks such as text summarization and
question-answering, bugs in code, plan, and other formal artifacts produced by
LLMs can be catastrophic. We posit that we can use the satisfiability modulo
theory (SMT) solvers as deductive reasoning engines to analyze the generated
solutions from the LLMs, produce counterexamples when the solutions are
incorrect, and provide that feedback to the LLMs exploiting the dialog
capability of instruct-trained LLMs. This interaction between inductive LLMs
and deductive SMT solvers can iteratively steer the LLM to generate the correct
response. In our experiments, we use planning over the domain of blocks as our
synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,
Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our
method allows the user to communicate the planning problem in natural language;
even the formulation of queries to SMT solvers is automatically generated from
natural language. Thus, the proposed technique can enable non-expert users to
describe their problems in natural language, and the combination of LLMs and
SMT solvers can produce provably correct solutions.",interactive-planning,"language-translation, plan-generation",4
Asking Before Action: Gather Information in Embodied Decision Making with Language Models,"With strong capabilities of reasoning and a broad understanding of the world,
Large Language Models (LLMs) have demonstrated immense potential in building
versatile embodied decision-making agents capable of executing a wide array of
tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM
agents encounter challenges in efficiently gathering essential information,
leading to suboptimal performance. Conversely, human individuals often seek
additional information from their peers prior to taking action, harnessing
external knowledge to avoid unnecessary trial and error. Drawing inspiration
from this behavior, we propose \textit{Asking Before Acting} (ABA), a method
that empowers the agent to proactively inquire with external sources for
pertinent information using natural language during their interactions within
the environment. In this way, the agent is able to enhance its efficiency and
performance by circumventing potentially laborious steps and combating the
difficulties associated with exploration in unfamiliar environments and
vagueness of the instructions. We conduct extensive experiments involving a
spectrum of environments including text-based household everyday tasks, robot
arm manipulation tasks, and real world open domain image based embodied tasks.
The experiments involve various models from Vicuna to GPT-4. The results
demonstrate that, even with modest prompts modifications, ABA exhibits
substantial advantages on both performance and efficiency over baseline LLM
agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates
learning the rationale for asking and allows for additional enhancements
especially in tasks that baselines struggle to solve.",interactive-planning,"language-translation, plan-generation",4
Inner monologue: Embodied reasoning through planning with language models,"Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.",interactive-planning,"language-translation, plan-generation",4
Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,"Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.",interactive-planning,"language-translation, plan-generation",4
Reflect: Summarizing robot experiences for failure explanation and correction,"The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.",interactive-planning,"language-translation, plan-generation",4
Robots that ask for help: Uncertainty alignment for large language model planners,"Large language models (LLMs) exhibit a wide range of promising capabilities
-- from step-by-step planning to commonsense reasoning -- that may provide
utility for robots, but remain prone to confidently hallucinated predictions.
In this work, we present KnowNo, which is a framework for measuring and
aligning the uncertainty of LLM-based planners such that they know when they
don't know and ask for help when needed. KnowNo builds on the theory of
conformal prediction to provide statistical guarantees on task completion while
minimizing human help in complex multi-step planning settings. Experiments
across a variety of simulated and real robot setups that involve tasks with
different modes of ambiguity (e.g., from spatial to numeric uncertainties, from
human preferences to Winograd schemas) show that KnowNo performs favorably over
modern baselines (which may involve ensembles or extensive prompt tuning) in
terms of improving efficiency and autonomy, while providing formal assurances.
KnowNo can be used with LLMs out of the box without model-finetuning, and
suggests a promising lightweight approach to modeling uncertainty that can
complement and scale with the growing capabilities of foundation models.
Website: https://robot-help.github.io",interactive-planning,"language-translation, plan-generation",4
Tree-Planner: Efficient Close-loop Task Planning with Large Language Models,"This paper studies close-loop task planning, which refers to the process of
generating a sequence of skills (a plan) to accomplish a specific goal while
adapting the plan based on real-time observations. Recently, prompting Large
Language Models (LLMs) to generate actions iteratively has become a prevalent
paradigm due to its superior performance and user-friendliness. However, this
paradigm is plagued by two inefficiencies: high token consumption and redundant
error correction, both of which hinder its scalability for large-scale testing
and applications. To address these issues, we propose Tree-Planner, which
reframes task planning with LLMs into three distinct phases: plan sampling,
action tree construction, and grounded deciding. Tree-Planner starts by using
an LLM to sample a set of potential plans before execution, followed by the
aggregation of them to form an action tree. Finally, the LLM performs a
top-down decision-making process on the tree, taking into account real-time
environmental information. Experiments show that Tree-Planner achieves
state-of-the-art performance while maintaining high efficiency. By decomposing
LLM queries into a single plan-sampling call and multiple grounded-deciding
calls, a considerable part of the prompt are less likely to be repeatedly
consumed. As a result, token consumption is reduced by 92.2% compared to the
previously best-performing model. Additionally, by enabling backtracking on the
action tree as needed, the correction process becomes more flexible, leading to
a 40.5% decrease in error corrections.",interactive-planning,"brain-inspired-planning, language-translation, plan-generation",4
Guiding language model reasoning with planning tokens,"Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
(CoT) reasoning. However, most of the existing approaches to enhance this
ability rely heavily on data-driven methods, while neglecting the structural
aspects of the model's reasoning capacity. To encourage a more structural
generation of CoT steps, we propose a hierarchical generation scheme: we let
the LM generate a planning token at the start of each reasoning step,
intuitively serving as a high-level plan of the current step, and add their
embeddings to the model parameters. Our approach requires a negligible increase
in trainable parameters (0.001%) and can be applied through either full
fine-tuning or a more parameter-efficient scheme. We demonstrate our method's
effectiveness by applying it to three different LLMs, showing notable accuracy
improvements across three math word problem datasets and one multihop QA
dataset with respect to standard fine-tuning baselines.",interactive-planning,"language-translation, plan-generation",4
DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs,"Mobile robots often rely on pre-existing maps for effective path planning and
navigation. However, when these maps are unavailable, particularly in
unfamiliar environments, a different approach become essential. This paper
introduces DynaCon, a novel system designed to provide mobile robots with
contextual awareness and dynamic adaptability during navigation, eliminating
the reliance of traditional maps. DynaCon integrates real-time feedback with an
object server, prompt engineering, and navigation modules. By harnessing the
capabilities of Large Language Models (LLMs), DynaCon not only understands
patterns within given numeric series but also excels at categorizing objects
into matched spaces. This facilitates dynamic path planner imbued with
contextual awareness. We validated the effectiveness of DynaCon through an
experiment where a robot successfully navigated to its goal using reasoning.
Source code and experiment videos for this work can be found at:
https://sites.google.com/view/dynacon.",interactive-planning,"language-translation, plan-generation",4
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models,"In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning",interactive-planning,"language-translation, plan-generation",5
Planning with large language models via corrective re-prompting,"Extracting the common sense knowledge present in Large Language Models
(LLMs) offers a path to designing intelligent, embodied agents. Related works
have queried LLMs with a wide-range of contextual information, such as goals,
sensor observations and scene descriptions, to generate high-level action plans for
specific tasks; however these approaches often involve human intervention or additional machinery to enable sensor-motor interactions. In this work, we propose
a prompting-based strategy for extracting executable plans from an LLM, which
leverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain
contexts, i.e., implicit preconditions must be met for an action to execute (e.g., a
door must be unlocked to open it), and that the embodied agent has the ability to
determine if the action is/is not executable in the current context (e.g., detect if a
precondition error is present). When an agent is unable to execute an action, our
approach re-prompts the LLM with precondition error information to extract an
executable corrective action to achieve the intended goal in the current context.
We evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to
methods that naively re-sample actions from the LLM. Our approach, using precondition errors, improves executability and semantic correctness of plans, while
also reducing the number of re-prompts required when querying actions.",interactive-planning,"language-translation, multiagent-planning, plan-generation",5
Integrating Common Sense and Planning with Large Language Models for Room Tidying,"Do you want a personal housekeeper robot? This
project seeks to endow robots with the capability of tidying
up messy rooms with brief natural language descriptions of the
environment. We address three key challenges: (i) incomplete map
information in the description, (ii) commonsense understanding
of object locations, and (iii) long-horizon planning and acting
to achieve the objective. To tackle these challenges, we leverage
Large Language Models’ (LLMs) understanding of typical layouts
of human-living environments and object locations, as well as
programming and control skills for action execution. Specifically,
we prompt ChatGPT to reconstruct complete map representations
from partial descriptions, then generate a high-level action plan
in the form of Python functions, and finally refine the plans
with atomic actions executable by the robot. We show that our
framework enables effective room tidying with limited human
instruction guidance. On simulation and real-world maps, it is
able to find a place missing out from human description within
three interactions with humans. In the simulation environment,
it is capable of putting more than 80% household objects in
their desired place. This study provides preliminary evidence that
LLMs have common sense about the spatial layout of humanliving environments and object arrangements, and this work
connects this knowledge to robotics tasks.",interactive-planning,"language-translation, plan-generation",5
GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning,"A robot in a human-centric environment needs to account for the human's
intent and future motion in its task and motion planning to ensure safe and
effective operation. This requires symbolic reasoning about probable future
actions and the ability to tie these actions to specific locations in the
physical environment. While one can train behavioral models capable of
predicting human motion from past activities, this approach requires large
amounts of data to achieve acceptable long-horizon predictions. More
importantly, the resulting models are constrained to specific data formats and
modalities. Moreover, connecting predictions from such models to the
environment at hand to ensure the applicability of these predictions is an
unsolved problem. We present a system that utilizes a Large Language Model
(LLM) to infer a human's next actions from a range of modalities without
fine-tuning. A novel aspect of our system that is critical to robotics
applications is that it links the predicted actions to specific locations in a
semantic map of the environment. Our method leverages the fact that LLMs,
trained on a vast corpus of text describing typical human behaviors, encode
substantial world knowledge, including probable sequences of human actions and
activities. We demonstrate how these localized activity predictions can be
incorporated in a human-aware task planner for an assistive robot to reduce the
occurrences of undesirable human-robot interactions by 29.2% on average.",interactive-planning,"language-translation, plan-generation",5
Palm-e: An embodied multimodal language model,"Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.",interactive-planning,"language-translation, plan-generation",5
AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.

",interactive-planning,"language-translation, multiagent-planning, plan-generation",5
Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact with
the world, which marks an initial step toward versatile robotics. However, these
efforts tend to overlook the visual richness of open worlds, rendering the entire
interactive process akin to “a blindfolded text-based game.” Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their
surroundings and producing responses that are easy to understand. In this paper,
we propose Steve-Eye, an end-to-end trained large multimodal model to address
this limitation. Steve-Eye integrates the LLM with a visual encoder to process
visual-text inputs and generate multimodal feedback. We adopt a semi-automatic
strategy to collect an extensive dataset comprising 850K open-world instruction
pairs, enabling our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks and carry
out experiments from a wide range of perspectives to validate our model’s capability to strategically act and plan. The project’s website and code can be found at
https://sites.google.com/view/steve-eye.",interactive-planning,"language-translation, plan-generation",5
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",heuristics-optimization,"language-translation, plan-generation",5
PDDL planning with pretrained large language models,"We study few-shot prompting of pretrained large language models (LLMs) towards solving PDDL planning problems. We are interested in two questions: (1) To what extent can LLMs solve PDDL planning problems on their own? (2) How and to what extent can LLMs be used to guide AI planners? Recent work by Valmeekam et al. (2022) presents negative evidence for (1) in the classic blocks world domain. We confirm this finding, but expand the inquiry to 18 domains and find more mixed results with a few clear successes. For (2), we propose a simple mechanism for using good-but-imperfect LLM outputs to aid a heuristic-search planner. We also find that the LLM performance is due not only to syntactic pattern matching, but also to its commonsense understanding of English terms that appear in the PDDL.
",heuristics-optimization,"brain-inspired-planning, language-translation, plan-generation",5
Reasoning with language model is planning with world model,"Large language models (LLMs) have shown remarkable reasoning capabilities,
especially when prompted to generate intermediate reasoning steps (e.g.,
Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are
easy for humans, such as generating action plans for executing tasks in a given
environment, or performing complex math, logical, and commonsense reasoning.
The deficiency stems from the key fact that LLMs lack an internal
$\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment
status, intermediate variable values) and simulate long-term outcomes of
actions. This prevents LLMs from performing deliberate planning akin to human
brains, which involves exploring alternative reasoning paths, anticipating
future states and rewards, and iteratively refining existing reasoning steps.
To overcome the limitations, we propose a new LLM reasoning framework,
$\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning
$\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning
agent, and incorporates a principled planning algorithm (based on Monto Carlo
Tree Search) for strategic exploration in the vast reasoning space. During
reasoning, the LLM (as agent) incrementally builds a reasoning tree under the
guidance of the LLM (as world model) and task-specific rewards, and obtains a
high-reward reasoning path efficiently with a proper balance between
exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of
challenging reasoning problems including plan generation, math reasoning, and
logical inference. Empirical results on these tasks demonstrate the superiority
of RAP over various strong baselines, including CoT and least-to-most prompting
with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%
relative improvement in a plan generation setting.",heuristics-optimization,"language-translation, plan-generation",5
Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans,"Task-oriented dialogue is difficult in part because it involves understanding
user intent, collecting information from the user, executing API calls, and
generating helpful and fluent responses. However, for complex tasks one must
also correctly do all of these things over multiple steps, and in a specific
order. While large pre-trained language models can be fine-tuned end-to-end to
create multi-step task-oriented dialogue agents that generate fluent text, our
experiments confirm that this approach alone cannot reliably perform new
multi-step tasks that are unseen during training. To address these limitations,
we augment the dialogue contexts given to \textmd{text2text} transformers with
known \textit{valid workflow names} and \textit{action plans}. Action plans
consist of sequences of actions required to accomplish a task, and are encoded
as simple sequences of keywords (e.g. verify-identity, pull-up-account,
reset-password, etc.). We perform extensive experiments on the Action-Based
Conversations Dataset (ABCD) with T5-small, base and large models, and show
that such models: a) are able to more readily generalize to unseen workflows by
following the provided plan, and b) are able to generalize to executing unseen
actions if they are provided in the plan. In contrast, models are unable to
fully accomplish new multi-step tasks when they are not provided action plan
information, even when given new valid workflow names.",heuristics-optimization,"language-translation, multiagent-planning, plan-generation",5
On the planning abilities of large language models (a critical investigation with a proposed benchmark),"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) how good LLMs are by themselves in
generating and validating simple plans in commonsense planning tasks (of the
type that humans are generally quite good at) and (2) how good LLMs are in
being a source of heuristic guidance for other agents--either AI planners or
human planners--in their planning tasks. To investigate these questions in a
systematic rather than anecdotal manner, we start by developing a benchmark
suite based on the kinds of domains employed in the International Planning
Competition. On this benchmark, we evaluate LLMs in three modes: autonomous,
heuristic and human-in-the-loop. Our results show that LLM's ability to
autonomously generate executable plans is quite meager, averaging only about 3%
success rate. The heuristic and human-in-the-loop modes show slightly more
promise. In addition to these results, we also make our benchmark and
evaluation tools available to support investigations by research community.",heuristics-optimization,"language-translation, plan-generation",5
Navigation with large language models: Semantic guesswork as a heuristic for planning,"Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics — e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the “semantic guesswork” produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
",heuristics-optimization,"language-translation, plan-generation",5
Optimal Scene Graph Planning with Large Language Model Guidance,"Recent advances in metric, semantic, and topological mapping have equipped
autonomous robots with semantic concept grounding capabilities to interpret
natural language tasks. This work aims to leverage these new capabilities with
an efficient task planning algorithm for hierarchical metric-semantic models.
We consider a scene graph representation of the environment and utilize a large
language model (LLM) to convert a natural language task into a linear temporal
logic (LTL) automaton. Our main contribution is to enable optimal hierarchical
LTL planning with LLM guidance over scene graphs. To achieve efficiency, we
construct a hierarchical planning domain that captures the attributes and
connectivity of the scene graph and the task automaton, and provide semantic
guidance via an LLM heuristic function. To guarantee optimality, we design an
LTL heuristic function that is provably consistent and supplements the
potentially inadmissible LLM guidance in multi-heuristic planning. We
demonstrate efficient planning of complex natural language tasks in scene
graphs of virtualized real environments.",heuristics-optimization,"language-translation, plan-generation",5
Alphazero-like tree-search can guide large language model decoding and training,"Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim
to augment the reasoning capabilities of LLMs by using tree-search algorithms
to guide multi-step reasoning. These methods rely on prompting a pre-trained
model to serve as a value function and focus on problems with low search depth.
As a result, these methods will not work in domains where the pre-trained LLM
does not have enough knowledge to serve as an effective value function or in
domains that require long-horizon planning. To address these limitations, we
present an AlphaZero-like tree-search learning framework for LLMs (termed
TS-LLM), systematically illustrating how tree-search with a learned value
function can guide LLM decoding. TS-LLM distinguishes itself in two key ways.
(1) Leveraging a learned value function and AlphaZero-like algorithms, our
approach can be generally adaptable to a wide range of tasks, language models
of any size, and tasks of varying search depths. (2) Our approach can guide
LLMs during both inference and training, iteratively improving the LLM.
Empirical results across reasoning, planning, alignment, and decision-making
tasks show that TS-LLM outperforms existing approaches and can handle trees
with a depth of 64.",heuristics-optimization,"language-translation, plan-generation",5
Gentopia: A collaborative platform for tool-augmented llms,"Augmented Language Models (ALMs) empower large language models with the
ability to use tools, transforming them into intelligent agents for real-world
interactions. However, most existing frameworks for ALMs, to varying degrees,
are deficient in the following critical features: flexible customization,
collaborative democratization, and holistic evaluation. We present gentopia, an
ALM framework enabling flexible customization of agents through simple
configurations, seamlessly integrating various language models, task formats,
prompting modules, and plugins into a unified paradigm. Furthermore, we
establish gentpool, a public platform enabling the registration and sharing of
user-customized agents. Agents registered in gentpool are composable such that
they can be assembled together for agent collaboration, advancing the
democratization of artificial intelligence. To ensure high-quality agents,
gentbench, an integral component of gentpool, is designed to thoroughly
evaluate user-customized agents across diverse aspects such as safety,
robustness, efficiency, etc. We release gentopia on Github and will
continuously move forward.",tool-integration,"language-translation, plan-generation",5
Tptu: Task planning and tool usage of large language model-based ai agents,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",tool-integration,"language-translation, plan-generation",5
Api-bank: A benchmark for tool-augmented llms,"Recent research has demonstrated that Large Language Models (LLMs) can
enhance their capabilities by utilizing external tools. However, three pivotal
questions remain unanswered: (1) How effective are current LLMs in utilizing
tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What
obstacles need to be overcome to leverage tools? To address these questions, we
introduce API-Bank, a groundbreaking benchmark, specifically designed for
tool-augmented LLMs. For the first question, we develop a runnable evaluation
system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753
API calls to assess the existing LLMs' capabilities in planning, retrieving,
and calling APIs. For the second question, we construct a comprehensive
training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000
distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM
initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits
improved tool utilization compared to GPT-3, while GPT-4 excels in planning.
However, there is still significant potential for further improvement.
Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26
pts and approaches the effectiveness of GPT-3.5. Through error analysis, we
highlight the key challenges for future research in this field to answer the
third question.",tool-integration,"language-translation, plan-generation",5
Chameleon: Plug-and-play compositional reasoning with large language models,"Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.

",tool-integration,"language-translation, multiagent-planning, plan-generation",5
Tool documentation enables zero-shot tool-usage with large language models,"Today, large language models (LLMs) are taught to use new tools by providing
a few demonstrations of the tool's usage. Unfortunately, demonstrations are
hard to acquire, and can result in undesirable biased usage if the wrong
demonstration is chosen. Even in the rare scenario that demonstrations are
readily available, there is no principled selection protocol to determine how
many and which ones to provide. As tasks grow more complex, the selection
search grows combinatorially and invariably becomes intractable. Our work
provides an alternative to demonstrations: tool documentation. We advocate the
use of tool documentation, descriptions for the individual tool usage, over
demonstrations. We substantiate our claim through three main empirical findings
on 6 tasks across both vision and language modalities. First, on existing
benchmarks, zero-shot prompts with only tool documentation are sufficient for
eliciting proper tool usage, achieving performance on par with few-shot
prompts. Second, on a newly collected realistic tool-use dataset with hundreds
of available tool APIs, we show that tool documentation is significantly more
valuable than demonstrations, with zero-shot documentation significantly
outperforming few-shot without documentation. Third, we highlight the benefits
of tool documentations by tackling image generation and video tracking using
just-released unseen state-of-the-art models as tools. Finally, we highlight
the possibility of using tool documentation to automatically enable new
applications: by using nothing more than the documentation of GroundingDino,
Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the
just-released Grounded-SAM and Track Anything models.",tool-integration,"language-translation, plan-generation",5
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.

",tool-integration,"language-translation, plan-generation",5
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings,"Integrating large language models (LLMs) with various tools has led to increased attention in the field. Existing approaches either involve fine-tuning the LLM, which is both computationally costly and limited to a fixed set of tools, or prompting LLMs by in-context tool demonstrations. Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named ToolkenGPT, wherein LLMs effectively learn to master tools as predicting tokens through tool embeddings for solving complex tasks. In this framework, each tool is transformed into vector embeddings and plugged into the language model head. Once the function is triggered during text generation, the LLM enters a special function mode to execute the tool calls. Our experiments show that function embeddings effectively help LLMs understand tool use and improve on several tasks, including numerical reasoning, knowledge-based question answering and embodied decision-making.

",tool-integration,"language-translation, plan-generation",5
Openagi: When llm meets domain experts,"Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.

",tool-integration,"language-translation, plan-generation",5
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models,"Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on three
challenging planning tasks -- graph traversal, Tower of Hanoi, and logistics --
finding that it yields significant improvements over standard LLM methods
(e.g., zero-shot prompting, in-context learning, and chain-of-thought). These
results demonstrate the benefit of utilizing knowledge from cognitive
neuroscience to improve planning in LLMs.",brain-inspired-planning,"language-translation, plan-generation",5
Cognitive architectures for language agents,"Recent efforts have augmented large language models (LLMs) with external
resources (e.g., the Internet) or internal control flows (e.g., prompt
chaining) for tasks requiring grounding or reasoning, leading to a new class of
language agents. While these agents have achieved substantial empirical
success, we lack a systematic framework to organize existing agents and plan
future developments. In this paper, we draw on the rich history of cognitive
science and symbolic artificial intelligence to propose Cognitive Architectures
for Language Agents (CoALA). CoALA describes a language agent with modular
memory components, a structured action space to interact with internal memory
and external environments, and a generalized decision-making process to choose
actions. We use CoALA to retrospectively survey and organize a large body of
recent work, and prospectively identify actionable directions towards more
capable agents. Taken together, CoALA contextualizes today's language agents
within the broader history of AI and outlines a path towards language-based
general intelligence.",brain-inspired-planning,"language-translation, plan-generation",5
Evaluating Cognitive Maps and Planning in Large Language Models with CogEval,"Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.

",brain-inspired-planning,"language-translation, plan-generation",5
Tree-of-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning,"There emerges a promising trend of using large language models (LLMs) to
generate code-like plans for complex inference tasks such as visual reasoning.
This paradigm, known as LLM-based planning, provides flexibility in problem
solving and endows better interpretability. However, current research is mostly
limited to basic scenarios of simple questions that can be straightforward
answered in a few inference steps. Planning for the more challenging multi-hop
visual reasoning tasks remains under-explored. Specifically, under multi-hop
reasoning situations, the trade-off between accuracy and the complexity of
plan-searching becomes prominent. The prevailing algorithms either address the
efficiency issue by employing the fast one-stop generation or adopt a complex
iterative generation method to improve accuracy. Both fail to balance the need
for efficiency and performance. Drawing inspiration from the dual system of
cognition in the human brain, the fast and the slow think processes, we propose
a hierarchical plan-searching algorithm that integrates the one-stop reasoning
(fast) and the Tree-of-thought (slow). Our approach succeeds in performance
while significantly saving inference steps. Moreover, we repurpose the PTR and
the CLEVER datasets, developing a systematic framework for evaluating the
performance and efficiency of LLMs-based plan-search algorithms under reasoning
tasks at different levels of difficulty. Extensive experiments demonstrate the
superiority of our proposed algorithm in terms of performance and efficiency.
The dataset and code will be release soon.",brain-inspired-planning,"language-translation, plan-generation",5
SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,"We introduce S WIFT S AGE , a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. S WIFT S AGE integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the S WIFT module,
representing fast and intuitive thinking, and the S AGE module, emulating deliberate
thought processes. The S WIFT module is a small encoder-decoder LM fine-tuned
on the oracle agent’s action trajectories, while the S AGE module employs LLMs
such as GPT-4 for subgoal planning and grounding. We develop a heuristic method
to harmoniously integrate the two modules, resulting in a more efficient and
robust problem-solving process. In 30 tasks from the ScienceWorld benchmark,
S WIFT S AGE significantly outperforms other methods such as SayCan, ReAct, and
Reflexion, demonstrating its effectiveness in solving complex interactive tasks.",brain-inspired-planning,"language-translation, plan-generation",5
